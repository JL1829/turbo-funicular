<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras | Johnny’s Machine Learning Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A tutorial of how to use Ensemble Method to stack multiple classifier." />
<meta property="og:description" content="A tutorial of how to use Ensemble Method to stack multiple classifier." />
<link rel="canonical" href="https://johdev.com/ensemble/2020/01/18/Ensemble_Keras.html" />
<meta property="og:url" content="https://johdev.com/ensemble/2020/01/18/Ensemble_Keras.html" />
<meta property="og:site_name" content="Johnny’s Machine Learning Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-18T00:00:00-06:00" />
<script type="application/ld+json">
{"description":"A tutorial of how to use Ensemble Method to stack multiple classifier.","mainEntityOfPage":{"@type":"WebPage","@id":"https://johdev.com/ensemble/2020/01/18/Ensemble_Keras.html"},"url":"https://johdev.com/ensemble/2020/01/18/Ensemble_Keras.html","@type":"BlogPosting","headline":"How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras","dateModified":"2020-01-18T00:00:00-06:00","datePublished":"2020-01-18T00:00:00-06:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://johdev.com/feed.xml" title="Johnny's Machine Learning Blog" /><link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Johnny&#39;s Machine Learning Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/about/">About Me</a><a class="page-link" href="/search/">Search</a><a class="page-link" href="/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras</h1><p class="page-description">A tutorial of how to use Ensemble Method to stack multiple classifier.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-18T00:00:00-06:00" itemprop="datePublished">
        Jan 18, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      23 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/categories/#Ensemble">Ensemble</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h1"><a href="#how-to-develop-a-stacking-ensemble-for-deep-learning-neural-networks-in-python-with-keras">How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras</a>
<ul>
<li class="toc-entry toc-h2"><a href="#tutorial-overview">Tutorial Overview</a>
<ul>
<li class="toc-entry toc-h3"><a href="#stacked-generalization-ensemble">Stacked Generalization Ensemble</a></li>
<li class="toc-entry toc-h3"><a href="#multi-class-classfication-problem">Multi-Class Classfication Problem</a></li>
<li class="toc-entry toc-h3"><a href="#multilayer-perceptron-model">Multilayer Perceptron Model</a></li>
<li class="toc-entry toc-h3"><a href="#train-and-save-sub-models">Train and Save Sub-Models</a></li>
<li class="toc-entry toc-h3"><a href="#separate-stacking-model">Separate Stacking Model</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#integrated-stacking-model">Integrated Stacking Model</a></li>
<li class="toc-entry toc-h2"><a href="#extensions">Extensions</a></li>
<li class="toc-entry toc-h2"><a href="#further-reading">Further Reading</a></li>
</ul>
</li>
</ul><h1 id="how-to-develop-a-stacking-ensemble-for-deep-learning-neural-networks-in-python-with-keras">
<a class="anchor" href="#how-to-develop-a-stacking-ensemble-for-deep-learning-neural-networks-in-python-with-keras" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras</h1>

<p>Model averaging is an <a href="https://machinelearningmastery.com/ensemble-methods-for-deep-learning-neural-networks/">ensemble technique</a> where multiple sub-models contribute equally to a combined prediction</p>

<p>Model averaging can be improved by weighting the contributions of each sub-model to the combined prediction by the expected performance of the submodel. This can be extended further by training an entirely new model to learn how to best combine the contributions from each submodel. This approach is called stacked generalization, or stacking for short, and can result in better predictive performance than any single contributing model.</p>

<p>In this tutorial, you will discover how to develop a stacked generalization ensemble for deep learning neural networks.</p>

<p>After completing this tutorial, you will know:</p>

<ul>
  <li>
    <p>Stacked generalization is an ensemble method where a new model learns how to best combine the predictions from multiple  existing models.</p>
  </li>
  <li>
    <p>How to develop a stacking model using neural networks as a submodel and a scikit-learn classifier as the meta-learner.</p>
  </li>
  <li>
    <p>How to develop a stacking model where neural network sub-models are embedded in a larger stacking ensemble model for training and prediction.</p>
  </li>
</ul>

<h2 id="tutorial-overview">
<a class="anchor" href="#tutorial-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Tutorial Overview</h2>

<p>This tutorial is divided into six parts, they are:</p>

<ol>
  <li>Stacked Generalization ensemble</li>
  <li>Multi-Class Classification Problem</li>
  <li>Multilayer Perceptron Model</li>
  <li>Train and Save Sub-model</li>
  <li>Separate Stacking Model</li>
  <li>Integrated Stacking Model</li>
</ol>

<h3 id="stacked-generalization-ensemble">
<a class="anchor" href="#stacked-generalization-ensemble" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stacked Generalization Ensemble</h3>

<p>A model averaging ensemble combines the predictions from multiple trained models.</p>

<p>A limitation of this approach is that each model contributes the same amount to the ensemble prediction, regardless of how well the model performed. A variation of this approach, called a weighted average ensemble, weighs the contribution of each ensemble member by the trust or expected performance of the model on a holdout dataset. This allows well-performing models to contribute more and less-well-performing models to contribute less. The weighted average ensemble provides an improvement over the model average ensemble.</p>

<p>A further generalization of this approach is replacing the linear weighted sum (e.g. linear regression) model used to combine the predictions of the sub-models with any learning algorithm. This approach is called stacked generalization, or stacking for short.</p>

<p>In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction.</p>

<p>It may be helpful to think of the stacking procedure as having two levels: level 0 and level 1</p>

<ul>
  <li>
<strong>Level 0</strong>: The level 0 data is the training dataset inputs and level 0 models learn to make predictions from this data</li>
  <li>
<strong>Level 1</strong>: The level 1 data takes the output of the level 0 model s as input and the single level 1 model, or meta-learner, learns to make predictions from this data</li>
</ul>

<p>Unlike a weighted average ensemble, a stacked generalization ensemble can use the set of predictions as a context and conditionally decide to weigh the input predictions differently, potentially resulting in better performance.</p>

<p>Interestingly, although stacking is described as an ensemble learning method with two or more level 0 models, it can be used in the case where there is only a single level 0 model. In this case, the level 1, or meta-learner, model learns to correct the predictions from the level 0 model.</p>

<p>It is important that the meta-learner is trained on a separate dataset to the examples used to train the level 0 models to avoid overfitting.</p>

<p>A simple way that this can be achieved is by splitting the training dataset into a train and validation set. The level 0 models are then trained on the train set. The level 1 model is then trained using the validation set, where the raw inputs are first fed through the level 0 models to get predictions that are used as inputs to the level 1 model.</p>

<p>A limitation of the hold-out validation set approach to training a stacking model is that level 0 and level 1 models are not trained on the full dataset.</p>

<p>A more sophisticated approach to training a stacked model involves using k-fold cross-validation to develop the training dataset for the meta-learner model. Each level 0 model is trained using k-fold cross-validation (or even leave-one-out cross-validation for maximum effect); the models are then discarded, but the predictions are retained. This means for each model, there are predictions made by a version of the model that was not trained on those examples, e.g. like having holdout examples, but in this case for the entire training dataset.</p>

<p>The predictions are then used as inputs to train the meta-learner. Level 0 models are then trained on the entire training dataset and together with the meta-learner, the stacked model can be used to make predictions on new data.</p>

<p>In practice, it is common to use different algorithms to prepare each of the level 0 models, to provide a diverse set of predictions.</p>

<p>It is also common to use a simple linear model to combine the predictions. Because use of a linear model is common, stacking is more recently referred to as “model blending” or simply “blending,” especially in machine learning competitions.</p>

<p>A stacked generalization ensemble can be developed for regression and classification problems. In the case of classification problems, better results have been seen when using the prediction of class probabilities as input to the meta-learner instead of class labels.</p>

<p>Now that we are familiar with stacked generalization, we can work through a case study of developing a stacked deep learning model.</p>

<h3 id="multi-class-classfication-problem">
<a class="anchor" href="#multi-class-classfication-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-Class Classfication Problem</h3>

<p>We will use a small multi-class classification problem as the basis to demonstrate the stacking ensemble.</p>

<p>The Scikit-learn class provides the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">make_blobs() function</a></p>

<p>The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the <a href="https://machinelearningmastery.com/how-to-generate-random-numbers-in-python/">pseudorandom number generator</a> to ensue that we always get the same data points</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>

<span class="c1"># generate 2d classification dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># scatter plot, dots colored by class value
</span><span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="p">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">y</span><span class="o">=</span><span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">y</span><span class="p">))</span>
</code></pre></div></div>

<p>The results are the input and output elements of a dataset that we can model.</p>

<p>In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">colors</span> <span class="o">=</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'red'</span><span class="p">,</span> <span class="mi">1</span><span class="p">:</span><span class="s">'blue'</span><span class="p">,</span> <span class="mi">2</span><span class="p">:</span><span class="s">'green'</span><span class="p">}</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="p">.</span><span class="n">subplots</span><span class="p">()</span>
<span class="n">grouded</span> <span class="o">=</span> <span class="n">df</span><span class="p">.</span><span class="n">groupby</span><span class="p">(</span><span class="s">'label'</span><span class="p">)</span>
<span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="n">grouded</span><span class="p">:</span>
    <span class="n">group</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ax</span><span class="o">=</span><span class="n">ax</span><span class="p">,</span> <span class="n">kind</span><span class="o">=</span><span class="s">'scatter'</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">key</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">colors</span><span class="p">[</span><span class="n">key</span><span class="p">])</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/output_4_0.svg" alt="plot"></p>

<p>Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points.</p>

<p>This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different “good enough” candidate solutions, resulting in a high variance.</p>

<h3 id="multilayer-perceptron-model">
<a class="anchor" href="#multilayer-perceptron-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multilayer Perceptron Model</h3>

<p>Before we define a model, we need to contrive a problem that is appropriate for the stacking ensemble.</p>

<p>In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model.</p>

<p>We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model.</p>

<p>The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values before we split the rows into the train and test datasets. We can do this using the Keras <code class="language-plaintext highlighter-rouge">to_categorical()</code> function.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use PlaidML as backend intead of default TensorFlow, 
# so that can utilize the power of MacBook Pro's AMD GPU
</span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="p">.</span><span class="n">environ</span><span class="p">[</span><span class="s">"KERAS_BACKEND"</span><span class="p">]</span> <span class="o">=</span> <span class="s">"plaidml.keras.backend"</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import the modules
</span><span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Using plaidml.keras.backend backend.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate 2d clasification dataset
</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

<span class="c1"># apply one-hot encoding
</span><span class="n">y</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

<span class="c1"># split train test set
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"The shape of X train set is </span><span class="si">{</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">, the shape of X test set is </span><span class="si">{</span><span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">."</span><span class="p">)</span> 
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The shape of X train set is (100, 2), the shape of X test set is (1000, 2).
</code></pre></div></div>

<p>Next, we can define and combine the model.</p>

<p>The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function.</p>

<p>Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient <a href="https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/">Adam flavor of stochastic gradient desent</a></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:plaidml:Opening device "metal_amd_radeon_pro_560x.0"
</code></pre></div></div>

<p>The model is fit for 500 training epochs and we will evaluate the model each epoch on the test set, using the test set as validation set</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit the model
</span><span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">),</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Epoch 150/150
100/100 [==============================] - 0s 1ms/step - loss: 0.4755 - acc: 0.7900 - val_loss: 0.5328 - val_acc: 0.7800
</code></pre></div></div>

<p>At the end of the run, we will evaluate the performance of the model on the train and test sets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># evaluate the model
</span><span class="n">_</span><span class="p">,</span> <span class="n">train_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>100/100 [==============================] - 0s 450us/step
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># evaluate the model
</span><span class="n">_</span><span class="p">,</span> <span class="n">test_acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1000/1000 [==============================] - 0s 112us/step
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="s">f"The train accuracy is </span><span class="si">{</span><span class="n">train_acc</span><span class="si">}</span><span class="s">, and the test accuracy is </span><span class="si">{</span><span class="n">test_acc</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The train accuracy is 0.79, and the test accuracy is 0.78.
</code></pre></div></div>

<p>Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># learning curves of model accuracy
</span><span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'train'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">plot</span><span class="p">(</span><span class="n">history</span><span class="p">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s">'test'</span><span class="p">)</span>
<span class="n">plt</span><span class="p">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="p">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/images/output_19_0.svg" alt="plot_2"></p>

<p>Running the example first prints the shape of each dataset for confirmation, then the performance of the final model on the train and test datasets.</p>

<p>Your specific results will vary (by design!) given the high variance nature of the model.</p>

<p>In this case, we can see that the model achieved about 78% accuracy on the training dataset, which we know is optimistic, and about 73.9% on the test dataset, which we would expect to be more realistic.</p>

<p>We can now look at using instances of this model as part of a stacking ensemble.</p>

<h3 id="train-and-save-sub-models">
<a class="anchor" href="#train-and-save-sub-models" aria-hidden="true"><span class="octicon octicon-link"></span></a>Train and Save Sub-Models</h3>

<p>To keep this example simple, we will use multiple instances of the same model as level-0 or sub-models in the stacking ensemble.</p>

<p>We will also use a holdout validation dataset to train the level-1 or meta-learner in the ensemble.</p>

<p>A more advanced example may use different types of MLP models (deeper, wider, etc.) as sub-models and train the meta-learner using <a href="https://machinelearningmastery.com/k-fold-cross-validation/">k-fold cross-validation</a></p>

<p>In this section, we will train multiple sub-models and <a href="https://machinelearningmastery.com/save-load-keras-deep-learning-models/">save them to file for later use</a> in our stacking ensembles.</p>

<p>The first step is to create a function that will define and fit an MLP model on the training dataset.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit model on dataset
</span><span class="k">def</span> <span class="nf">fit_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">):</span>
    <span class="c1"># define the model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">))</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
    
    <span class="c1"># fit the model
</span>    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">150</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<p>Next, we can create a sub-directory to store the models.</p>

<p>Note, if the directory already exists, you may have to delete it when re-running this code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">os</span> <span class="kn">import</span> <span class="n">makedirs</span>
<span class="n">makedirs</span><span class="p">(</span><span class="s">'models'</span><span class="p">)</span>
</code></pre></div></div>

<p>Finally, we can create multiple instances of the MLP and save each to the “models/” subdirectory with a unique filename.</p>

<p>In this case, we will create five sub-models, but you can experiment with a different number of models and see how it impacts model performance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit and save models:
</span><span class="n">n_members</span> <span class="o">=</span> <span class="mi">5</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_members</span><span class="p">):</span>
    <span class="c1"># fit model
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">fit_model</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

    <span class="c1"># save model
</span>    <span class="n">filename</span> <span class="o">=</span> <span class="s">'models/model_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.h5'</span>
    <span class="n">model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">f"[INFO]&gt;&gt;Save </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>INFO:plaidml:Opening device "metal_amd_radeon_pro_560x.0"
[INFO]&gt;&gt;Save models/model_1.h5.
[INFO]&gt;&gt;Save models/model_2.h5.
[INFO]&gt;&gt;Save models/model_3.h5.
[INFO]&gt;&gt;Save models/model_4.h5.
[INFO]&gt;&gt;Save models/model_5.h5.
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="err">!</span><span class="n">ls</span> <span class="o">-</span><span class="n">l</span> <span class="n">models</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>total 280
-rw-r--r--  1 johnnylu  staff  27936 Jan 16 12:40 model_1.h5
-rw-r--r--  1 johnnylu  staff  27936 Jan 16 12:40 model_2.h5
-rw-r--r--  1 johnnylu  staff  27936 Jan 16 12:40 model_3.h5
-rw-r--r--  1 johnnylu  staff  27936 Jan 16 12:41 model_4.h5
-rw-r--r--  1 johnnylu  staff  27936 Jan 16 12:41 model_5.h5
</code></pre></div></div>

<h3 id="separate-stacking-model">
<a class="anchor" href="#separate-stacking-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Separate Stacking Model</h3>

<p>We can now train a meta-learner that will best combine the predictions from the sub-models and ideally perform better than any single sub-model.</p>

<p>The first step is to load the saved models.</p>

<p>We can use the <code class="language-plaintext highlighter-rouge">load_model()</code> Keras function and create a Python list of loaded models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">n_members</span> <span class="o">=</span> <span class="mi">5</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>

<span class="c1"># load models from file
</span><span class="k">def</span> <span class="nf">load_all_model</span><span class="p">(</span><span class="n">n_models</span><span class="p">):</span>
    <span class="n">all_models</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_models</span><span class="p">):</span>
        <span class="c1"># define filename for this ensemble
</span>        <span class="n">filename</span> <span class="o">=</span> <span class="s">'models/model_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s">'.h5'</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">load_model</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>

        <span class="c1"># add to list of members
</span>        <span class="n">all_models</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
        <span class="k">print</span><span class="p">(</span><span class="s">f"[INFO]&gt;&gt;loaded </span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">all_models</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># load all models: 
</span><span class="n">members</span> <span class="o">=</span> <span class="n">load_all_model</span><span class="p">(</span><span class="n">n_members</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">members</span><span class="p">)</span><span class="si">}</span><span class="s"> models."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[INFO]&gt;&gt;loaded models/model_1.h5.
[INFO]&gt;&gt;loaded models/model_2.h5.
[INFO]&gt;&gt;loaded models/model_3.h5.
[INFO]&gt;&gt;loaded models/model_4.h5.
[INFO]&gt;&gt;loaded models/model_5.h5.
Loaded 5 models.
</code></pre></div></div>

<p>It would be useful to know how well the single models perform on the test dataset as we would expect a stacking model to perform better.</p>

<p>We can easily evaluate each single model on the training dataset and establish a baseline of performance.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># evaluate standalone models on test dataset
</span><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">members</span><span class="p">:</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">f"Model Test Set Accuracy: </span><span class="si">{</span><span class="n">acc</span><span class="si">}</span><span class="s">."</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>1000/1000 [==============================] - 0s 402us/step
Model Test Set Accuracy: 0.78.
1000/1000 [==============================] - 0s 98us/step
Model Test Set Accuracy: 0.724.
1000/1000 [==============================] - 0s 100us/step
Model Test Set Accuracy: 0.755.
1000/1000 [==============================] - 0s 103us/step
Model Test Set Accuracy: 0.778.
1000/1000 [==============================] - 0s 101us/step
Model Test Set Accuracy: 0.752.
</code></pre></div></div>

<p>Next, we can train our meta-learner. This requires two steps:</p>

<ul>
  <li>Prepare a training dataset for the meta-learner.</li>
  <li>Use the prepared training set to fit a meta-learner model</li>
</ul>

<p>We will prepare a training dataset for the meta-learner by providing examples from the test set to each of the submodels and collecting the predictions. In this case, each model will output three predictions for each example for the probabilities that a given example belongs to each of the three classes. Therefore, the 1,000 examples in the test set will result in five arrays with the shape <code class="language-plaintext highlighter-rouge">[1000, 3]</code>.</p>

<p>We can combine these arrays into a three-dimensional array with the shape <code class="language-plaintext highlighter-rouge">[1000, 5, 3]</code> by using the <a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html">dstack() numpy function</a>that will stack each new set of predictions</p>

<p>As input for new model, we will require 1,000 examples with some number of features. Given that we have 5 models and each model makes three predictions per example, then we would have 15 (3 x 5) features for each example provided to the submodels. We can transform the <code class="language-plaintext highlighter-rouge">[1000, 5, 3]</code> shaped predictions from the sub-models into a <code class="language-plaintext highlighter-rouge">[1000, 15]</code> shaped array to be used to train a meta-learner using the <a href="https://machinelearningmastery.com/index-slice-reshape-numpy-arrays-machine-learning-python/">reshape() numpy function</a> and flattening the final two dimensions. The <code class="language-plaintext highlighter-rouge">stacked_dataset()</code> function implements this steps</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># check the sub-model prediction output shape
</span><span class="n">test</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[[0.87745166 0.00568948 0.11685885]
 [0.00420502 0.08186033 0.91393465]
 [0.0560521  0.27464437 0.66930354]
 ...
 [0.22192474 0.524442   0.25363323]
 [0.8633721  0.05750959 0.0791183 ]
 [0.66364163 0.11609959 0.2202588 ]]


(1000, 3)


[[1. 0. 0.]
 [0. 0. 1.]
 [0. 0. 1.]
 ...
 [0. 1. 0.]
 [1. 0. 0.]
 [0. 0. 1.]]
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># numpy dstack example
</span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">dstack</span>

<span class="n">a</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"The array a is </span><span class="si">{</span><span class="n">a</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">((</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"The array b is </span><span class="si">{</span><span class="n">b</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"</span><span class="se">\n</span><span class="s">"</span><span class="p">)</span>

<span class="n">c</span> <span class="o">=</span> <span class="n">dstack</span><span class="p">((</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"dstack: </span><span class="si">{</span><span class="n">c</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">f"Shape of dstack: </span><span class="si">{</span><span class="n">c</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>The array a is [1 2 3]


The array b is [4 5 6]


dstack: [[[1 4]
  [2 5]
  [3 6]]]
Shape of dstack: (1, 3, 2)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create stacked model input dataset as output from the ensemble
</span><span class="k">def</span> <span class="nf">stacked_dataset</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">inputX</span><span class="p">):</span>
    <span class="n">stackX</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">members</span><span class="p">:</span>
        <span class="c1"># make prediction
</span>        <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">inputX</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># stack predictions into [rows, members, probabilities]
</span>        <span class="k">if</span> <span class="n">stackX</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">stackX</span> <span class="o">=</span> <span class="n">yhat</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">stackX</span> <span class="o">=</span> <span class="n">dstack</span><span class="p">((</span><span class="n">stackX</span><span class="p">,</span> <span class="n">yhat</span><span class="p">))</span>
        <span class="c1"># flatten predictions to [rows, members x probabilities]
</span>    <span class="n">stackX</span> <span class="o">=</span> <span class="n">stackX</span><span class="p">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">stackX</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">stackX</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">stackX</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
    <span class="k">return</span> <span class="n">stackX</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stackX</span> <span class="o">=</span> <span class="n">stacked_dataset</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stackX</span><span class="p">.</span><span class="n">shape</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(1000, 15)
</code></pre></div></div>

<p>Once prepared, we can use this input dataset along with the output, or <code class="language-plaintext highlighter-rouge">y</code> part, of the test set to train a new meta-learner</p>

<p>In this case, we will train a simple logistic regression algorithm from the scikit-learn library</p>

<p><a href="https://machinelearningmastery.com/logistic-regression-for-machine-learning/">Logistic Regression</a> only supports binary classification, although the implementation of logistic regression in scikit-learn in the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">LogisticRegression class</a> support multi-class classification (more than two classes) using a one-vs-rest scheme. The function <code class="language-plaintext highlighter-rouge">fit_stacked_model()</code> below will prepare the training dataset for meta-learner by calling the <code class="language-plaintext highlighter-rouge">stacked_dataset()</code> function, then fit a logistic regression model that is then returned.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import Logistic Regression class
</span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="c1"># fit a model based on the outputs from the ensemble members
</span><span class="k">def</span> <span class="nf">fit_stacked_model</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">inputX</span><span class="p">,</span> <span class="n">inputy</span><span class="p">):</span>
    <span class="c1"># create dataset using ensemble
</span>    <span class="n">stackedX</span> <span class="o">=</span> <span class="n">stacked_dataset</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">inputX</span><span class="p">)</span>
    <span class="c1"># fit standalone mode
</span>    <span class="n">model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">stackedX</span><span class="p">,</span> <span class="n">inputy</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">model</span>

</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make a prediction with the stacked model
</span><span class="k">def</span> <span class="nf">stacked_prediction</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">inputX</span><span class="p">):</span>
    <span class="c1"># create dataset using ensemble
</span>    <span class="n">stackedX</span> <span class="o">=</span> <span class="n">stacked_dataset</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">inputX</span><span class="p">)</span>
    <span class="c1"># make a prediction
</span>    <span class="n">yhat</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">stackedX</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">yhat</span>
</code></pre></div></div>

<p>We can call this function and pass in the list of loaded models and the training dataset</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="c1"># reset the X, y and X_test, y_test variable
</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># split into train and test
</span><span class="n">n_train</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:</span><span class="n">n_train</span><span class="p">,</span> <span class="p">:],</span> <span class="n">X</span><span class="p">[</span><span class="n">n_train</span><span class="p">:,</span> <span class="p">:]</span>
<span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:</span><span class="n">n_train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">n_train</span><span class="p">:]</span>
<span class="k">print</span><span class="p">(</span><span class="n">X_train</span><span class="p">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X_test</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(100, 2) (1000, 2)
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="c1"># load all models
</span><span class="n">n_members</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">members</span> <span class="o">=</span> <span class="n">load_all_model</span><span class="p">(</span><span class="n">n_members</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Loaded %d models'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">members</span><span class="p">))</span>
<span class="c1"># evaluate standalone models on test dataset
</span><span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">members</span><span class="p">:</span>
	<span class="n">testy_enc</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">y_test</span><span class="p">)</span>
	<span class="n">_</span><span class="p">,</span> <span class="n">acc</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">testy_enc</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	<span class="k">print</span><span class="p">(</span><span class="s">'Model Accuracy: %.3f'</span> <span class="o">%</span> <span class="n">acc</span><span class="p">)</span>
<span class="c1"># fit stacked model using the ensemble
</span><span class="n">model</span> <span class="o">=</span> <span class="n">fit_stacked_model</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="c1"># evaluate model on test set
</span><span class="n">yhat</span> <span class="o">=</span> <span class="n">stacked_prediction</span><span class="p">(</span><span class="n">members</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Stacked Test Accuracy: %.3f'</span> <span class="o">%</span> <span class="n">acc</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[INFO]&gt;&gt;loaded models/model_1.h5.
[INFO]&gt;&gt;loaded models/model_2.h5.
[INFO]&gt;&gt;loaded models/model_3.h5.
[INFO]&gt;&gt;loaded models/model_4.h5.
[INFO]&gt;&gt;loaded models/model_5.h5.
Loaded 5 models
Model Accuracy: 0.780
Model Accuracy: 0.724
Model Accuracy: 0.755
Model Accuracy: 0.778
Model Accuracy: 0.752
Stacked Test Accuracy: 0.826
</code></pre></div></div>

<h2 id="integrated-stacking-model">
<a class="anchor" href="#integrated-stacking-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Integrated Stacking Model</h2>

<p>When using neural networks as sub-models, it may be desirable to use a neural network as a meta-learner.</p>

<p>Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model.</p>

<p>The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable.</p>

<p>This can be achieved using the <a href="https://machinelearningmastery.com/keras-functional-api-deep-learning/">Keras functional interface</a> for developing models.</p>

<p>After the models are loaded as a list, a larger stacking ensemble model can be defined where each of the loaded models is used as a separate input-head to the model. This requires that all of the layers in each of the loaded models be marked as not trainable so the weights cannot be updated when the new larger model is being trained. Keras also requires that each layer has a unique name, therefore the names of each layer in each of the loaded models will have to be updated to indicate to which ensemble member they belong.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import modules
</span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_blobs</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">load_model</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.utils</span> <span class="kn">import</span> <span class="n">plot_model</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Input</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">keras.layers.merge</span> <span class="kn">import</span> <span class="n">concatenate</span>
<span class="kn">from</span> <span class="nn">numpy</span> <span class="kn">import</span> <span class="n">argmax</span>
</code></pre></div></div>

<p>Once the sub-models have been prepared, we can define the stacking ensemble model.</p>

<p>The input layer for each of the sub-models will be used as separete input head to this new model. This means that <code class="language-plaintext highlighter-rouge">k</code> copies of any input data will have to be provided to the model, where <code class="language-plaintext highlighter-rouge">k</code> is the number of input models, in this case, <code class="language-plaintext highlighter-rouge">k = 5</code>.</p>

<p>The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 15-elememts vector will be created from the 3 class-probabilities predicted by each of the <code class="language-plaintext highlighter-rouge">5</code> models.</p>

<p>We will then define a hidden layer to interpret this <code class="language-plaintext highlighter-rouge">input</code> to the meta-learner and an output layer that will make its own probabilistic prediction. The <code class="language-plaintext highlighter-rouge">define_stacked_model()</code> function below implements this and will return a stacked generalization neural network model given a list of trained sub-models.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># define stacked model from multiple member input models
</span><span class="k">def</span> <span class="nf">define_stacked_model</span><span class="p">(</span><span class="n">members</span><span class="p">):</span>
    <span class="c1"># update all layers in all models to not be trainable
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">members</span><span class="p">)):</span>
        <span class="n">model</span> <span class="o">=</span> <span class="n">members</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
            <span class="c1"># make not trainable
</span>            <span class="n">layer</span><span class="p">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="c1"># rename to avoid 'unique layer name' issue
</span>            <span class="n">layer</span><span class="p">.</span><span class="n">name</span> <span class="o">=</span> <span class="s">'ensemble_'</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="s">'_'</span> <span class="o">+</span> <span class="n">layer</span><span class="p">.</span><span class="n">name</span>
    <span class="c1"># define multi-headed input
</span>    <span class="n">ensemble_visible</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="nb">input</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">members</span><span class="p">]</span>
    <span class="c1"># concatenate merge output from each model
</span>    <span class="n">ensemble_outputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">model</span><span class="p">.</span><span class="n">output</span> <span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="n">members</span><span class="p">]</span>
    <span class="n">merge</span> <span class="o">=</span> <span class="n">concatenate</span><span class="p">(</span><span class="n">ensemble_outputs</span><span class="p">)</span>
    <span class="n">hidden</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">)(</span><span class="n">merge</span><span class="p">)</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'softmax'</span><span class="p">)(</span><span class="n">hidden</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="n">ensemble_visible</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">output</span><span class="p">)</span>
    <span class="c1"># plot graph of ensemble
</span>    <span class="n">plot_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">show_shapes</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">to_file</span><span class="o">=</span><span class="s">'model_graph.png'</span><span class="p">)</span>
    <span class="c1"># complie
</span>    <span class="n">model</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'categorical_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">model</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stacked_model</span> <span class="o">=</span> <span class="n">define_stacked_model</span><span class="p">(</span><span class="n">members</span><span class="p">)</span>
</code></pre></div></div>

<p>A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together.</p>

<p>Once the model is defined, it can be fit. We can fit it directly on the holdout test dataset.</p>

<p>Because the sub-models are not trainable, their weights wil not be updated during training and only the weights of the new hidden and output layer will be updated. The <code class="language-plaintext highlighter-rouge">fit_stacked_model()</code> function below will fit the stacking neural network model on for <code class="language-plaintext highlighter-rouge">300</code> epochs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># fit a stacked model
</span><span class="k">def</span> <span class="nf">fit_stacked_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputX</span><span class="p">,</span> <span class="n">inputy</span><span class="p">):</span>
    <span class="c1"># prepare input data
</span>    <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputX</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nb">input</span><span class="p">))]</span>
    <span class="c1"># encode y variable
</span>    <span class="n">inputy_enc</span> <span class="o">=</span> <span class="n">to_categorical</span><span class="p">(</span><span class="n">inputy</span><span class="p">)</span>
    <span class="c1"># fit the mode
</span>    <span class="n">model</span><span class="p">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">inputy_enc</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>Once fit, we can use the new stacked model to make a prediction on new data.</p>

<p>This is as simple as calling the <code class="language-plaintext highlighter-rouge">predict()</code> function on the model. One minor change is that we require <code class="language-plaintext highlighter-rouge">k</code> copies of the input data in a list to be provided to the model for each of the <code class="language-plaintext highlighter-rouge">k</code> sub-models. the <code class="language-plaintext highlighter-rouge">predict_stacked_model()</code> function below simplifies this process of making a prediction with the stacking model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># make a prediction with a stacked model
</span><span class="k">def</span> <span class="nf">predict_stacked_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputX</span><span class="p">):</span>
    <span class="c1"># prepare input data
</span>    <span class="n">X</span> <span class="o">=</span> <span class="p">[</span><span class="n">inputX</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="nb">input</span><span class="p">))]</span>
    <span class="c1"># make prediction
</span>    <span class="k">return</span> <span class="n">model</span><span class="p">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    
</code></pre></div></div>

<p>We can call this function to make a prediction for the test dataset and report the accuracy</p>

<p>We would expect the performance of the neural network learner to be better than any individual submodel and perhaps competitive with the linear meta-learner used in the previous section.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># generate 2d classification dataset
</span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_blobs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">1100</span><span class="p">,</span> <span class="n">centers</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cluster_std</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="c1"># split into train and test
</span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="c1"># load all models
</span><span class="n">n_members</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">members</span> <span class="o">=</span> <span class="n">load_all_model</span><span class="p">(</span><span class="n">n_members</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Loaded %d models'</span> <span class="o">%</span> <span class="nb">len</span><span class="p">(</span><span class="n">members</span><span class="p">))</span>
<span class="c1"># define ensemble model
</span><span class="n">stacked_model</span> <span class="o">=</span> <span class="n">define_stacked_model</span><span class="p">(</span><span class="n">members</span><span class="p">)</span>
<span class="c1"># fit stacked model on test dataset
</span><span class="n">fit_stacked_model</span><span class="p">(</span><span class="n">stacked_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="c1"># make predictions and evaluate
</span><span class="n">yhat</span> <span class="o">=</span> <span class="n">predict_stacked_model</span><span class="p">(</span><span class="n">stacked_model</span><span class="p">,</span> <span class="n">X_test</span><span class="p">)</span>
<span class="n">yhat</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">yhat</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">acc</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">yhat</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Stacked Test Accuracy: %.3f'</span> <span class="o">%</span> <span class="n">acc</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[INFO]&gt;&gt;loaded models/model_1.h5.
[INFO]&gt;&gt;loaded models/model_2.h5.
[INFO]&gt;&gt;loaded models/model_3.h5.
[INFO]&gt;&gt;loaded models/model_4.h5.
[INFO]&gt;&gt;loaded models/model_5.h5.
Loaded 5 models
550/550 [==============================] - 0s 448us/step
Stacked Test Accuracy: 0.820
</code></pre></div></div>

<p>Running the example first loads the five sub-models.</p>

<p>A larger stacking ensemble neural network is defined and fit on the test dataset, then the new model is used to make a prediction on the test dataset. We can see that in this case, the model achieved an higher accuracy. out-performing the linear model from the previous section.</p>

<h2 id="extensions">
<a class="anchor" href="#extensions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Extensions</h2>

<p>This section lists some ideas for extending the tutorial that you may wish to explor</p>

<ul>
  <li>
<strong>Alternate Meta-Learner</strong>. Update the example to use an alternate meta-learner classifier model to the logistic regression model</li>
  <li>
<strong>Single Level 0 Models</strong>. Update the example to use a single level-0 model and compare the results.</li>
  <li>** Vary Level0 Models**. Develop a study that demostrates the relationship between test classification accuracy and the number of sub-models used in the stacked ensemble.</li>
  <li>
<strong>Cross-Validation Stacking Ensemble</strong>. Update the example to use k-fold cross-validation to prepare the training dataset for the meta-learner model.</li>
  <li>
<strong>Use Raw Input in Meta-Learner</strong>. Update the example so that the meta-learner algorithms take the raw input data for the sample as well as the output from the sub-models and compare performance.</li>
</ul>

<h2 id="further-reading">
<a class="anchor" href="#further-reading" aria-hidden="true"><span class="octicon octicon-link"></span></a>Further Reading</h2>

<p><strong>Books</strong></p>

<ul>
  <li>Section 8.8 Model Averaging and Stacking, <a href="https://www.amazon.com/Elements-Statistical-Learning-Prediction-Statistics/dp/0387848576/ref=as_li_ss_tl?ie=UTF8&amp;qid=1538688826&amp;sr=8-1&amp;keywords=elements+of+statistical+learning&amp;linkCode=sl1&amp;tag=inspiredalgor-20&amp;linkId=4562f94edf5ba260262968e8e2e02d18&amp;language=en_US">The Elements of Statistical Learning: Data Mining, Inference and Prediction</a>, 2nd Edition, 2016</li>
  <li>Section 7.5 Combining multiple models, <a href="https://www.amazon.com/Data-Mining-Practical-Techniques-Paperback/dp/B00ME3KRJU/ref=as_li_ss_tl?ie=UTF8&amp;qid=1538689436&amp;sr=8-2&amp;keywords=Data+Mining+-+Practical+Machine+Learning+Tools+and+Techniques+(2nd+edition)&amp;linkCode=sl1&amp;tag=inspiredalgor-20&amp;linkId=c94e485b89957e267cde4c376f0b290b&amp;language=en_US">Data Mining: Practical Machine Learning Tools and Techniques</a>, 2nd Editions, 2005</li>
  <li>Section 9.8.2 Stacked Generalization, <a href="http://home.elka.pw.edu.pl/~ptrojane/books/Bishop%20-%20Neural%20Networks%20for%20Pattern%20Recognition.pdf">Neural Networks for Pattern Recognition</a>, 1995</li>
</ul>

<p><strong>Papers</strong></p>

<ul>
  <li>
<a href="https://www.sciencedirect.com/science/article/pii/S0893608005800231">Stacked Generalization</a>, 1992</li>
  <li>
<a href="https://www.jair.org/index.php/jair/article/view/10228">Issues in Stacked Generalization</a>, 1999</li>
</ul>

<p><strong>API</strong></p>

<ul>
  <li><a href="https://keras.io/getting-started/sequential-model-guide/">Getting started with the Keras Sequential model</a></li>
  <li><a href="https://keras.io/layers/core/">Keras Core Layers API</a></li>
  <li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html">numpy.argmax API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html">sklearn.datasets.make_blobs() API</a></li>
  <li><a href="https://docs.scipy.org/doc/numpy/reference/generated/numpy.dstack.html">numpy.dstack API</a></li>
  <li><a href="http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html">sklearn.linear_model.LogisticRegression API</a></li>
</ul>


  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="JL1829/turbo-funicular"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/ensemble/2020/01/18/Ensemble_Keras.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/JL1829" title="JL1829"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/JALC1829" title="JALC1829"><svg class="svg-icon grey"><use xlink:href="/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
