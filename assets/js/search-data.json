{
  
    
        "post0": {
            "title": "Data Science Project imporvement, using LightGBM to gain more accuracy and no need to One-Hot Encoding",
            "content": "Overview . We have done the last Data Science Project with U.S. Adult Income Dataset with around 86% model accuracy, which can go into the top level accuracy in Kaggle competition. we have done the following steps: . Understand the business problem. | EDA(Exploratory Data Analysis): Look through and investigate the overall dataset, visualize it with matplotlib and finding any missing value and outliers. | Data cleaning: impute the missing and outliers value. | Baseline model: Dummy classifier gave us 75% accuracy as baseline, meaning that anything below 75% accuracy, the model do nothing better than flipping a coin, and above this value, the model have some skill to classify the labels. | Model evaluate and fine turn: we have evaluate Support Vector Machine; RamdonForestClassifier; BaggingClassifier; GradientBoostingClassifier and Neural Network, the best performance model is GradientBoostingClassifier which providing 86% of accuracy. | Today we are going to use another light weight, powerful, and fast algorithem: lightGBM, open source at 2017, and now maintainced by Microsoft . As the EDA was already done by the last blog, we will just skip it and move directly into today&#39;s topic. . Get the imports done and read the dataset . import numpy as np import pandas as pd import matplotlib.pyplot as plt import lightgbm as lgb import warnings warnings.filterwarnings(&#39;ignore&#39;) . df = pd.read_csv(&#39;datasets/adult.csv&#39;, na_values=&quot;?&quot;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null int64 1 workclass 46043 non-null object 2 fnlwgt 48842 non-null int64 3 education 48842 non-null object 4 educational-num 48842 non-null int64 5 marital-status 48842 non-null object 6 occupation 46033 non-null object 7 relationship 48842 non-null object 8 race 48842 non-null object 9 gender 48842 non-null object 10 capital-gain 48842 non-null int64 11 capital-loss 48842 non-null int64 12 hours-per-week 48842 non-null int64 13 native-country 47985 non-null object 14 income 48842 non-null object dtypes: int64(6), object(9) memory usage: 5.6+ MB . Data Cleaning part: Scaled the numerical column, and Label encoding the binary categorical column . The target column income need to be encoded as 1 and 0 | As well as the gender column | . import seaborn as sns from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() num_col = df.select_dtypes(exclude=[&#39;object&#39;, &#39;datetime&#39;]).columns df[num_col] = scaler.fit_transform(df[num_col]) . le = LabelEncoder() df[&#39;gender&#39;] = le.fit_transform(df[&#39;gender&#39;]) df[&#39;income&#39;] = le.fit_transform(df[&#39;income&#39;]) . df . age workclass fnlwgt education educational-num marital-status occupation relationship race gender capital-gain capital-loss hours-per-week native-country income . 0 0.109589 | Private | 0.145129 | 11th | 0.400000 | Never-married | Machine-op-inspct | Own-child | Black | 1 | 0.000000 | 0.0 | 0.397959 | United-States | 0 | . 1 0.287671 | Private | 0.052451 | HS-grad | 0.533333 | Married-civ-spouse | Farming-fishing | Husband | White | 1 | 0.000000 | 0.0 | 0.500000 | United-States | 0 | . 2 0.150685 | Local-gov | 0.219649 | Assoc-acdm | 0.733333 | Married-civ-spouse | Protective-serv | Husband | White | 1 | 0.000000 | 0.0 | 0.397959 | United-States | 1 | . 3 0.369863 | Private | 0.100153 | Some-college | 0.600000 | Married-civ-spouse | Machine-op-inspct | Husband | Black | 1 | 0.076881 | 0.0 | 0.397959 | United-States | 1 | . 4 0.013699 | NaN | 0.061708 | Some-college | 0.600000 | Never-married | NaN | Own-child | White | 0 | 0.000000 | 0.0 | 0.295918 | United-States | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 48837 0.136986 | Private | 0.165763 | Assoc-acdm | 0.733333 | Married-civ-spouse | Tech-support | Wife | White | 0 | 0.000000 | 0.0 | 0.377551 | United-States | 0 | . 48838 0.315068 | Private | 0.096129 | HS-grad | 0.533333 | Married-civ-spouse | Machine-op-inspct | Husband | White | 1 | 0.000000 | 0.0 | 0.397959 | United-States | 1 | . 48839 0.561644 | Private | 0.094462 | HS-grad | 0.533333 | Widowed | Adm-clerical | Unmarried | White | 0 | 0.000000 | 0.0 | 0.397959 | United-States | 0 | . 48840 0.068493 | Private | 0.128004 | HS-grad | 0.533333 | Never-married | Adm-clerical | Own-child | White | 1 | 0.000000 | 0.0 | 0.193878 | United-States | 0 | . 48841 0.479452 | Self-emp-inc | 0.186482 | HS-grad | 0.533333 | Married-civ-spouse | Exec-managerial | Wife | White | 0 | 0.150242 | 0.0 | 0.397959 | United-States | 1 | . 48842 rows × 15 columns . Data Cleaning Part: Impute the missing value . The missing value are all fall into categorical features . df.isnull().sum() . age 0 workclass 2799 fnlwgt 0 education 0 educational-num 0 marital-status 0 occupation 2809 relationship 0 race 0 gender 0 capital-gain 0 capital-loss 0 hours-per-week 0 native-country 857 income 0 dtype: int64 . Impute the missing value with the most frequent value . df = df.fillna(df.mode().iloc[0]) . df.isnull().sum() . age 0 workclass 0 fnlwgt 0 education 0 educational-num 0 marital-status 0 occupation 0 relationship 0 race 0 gender 0 capital-gain 0 capital-loss 0 hours-per-week 0 native-country 0 income 0 dtype: int64 . Data Cleaning Part: Convert the object Data type into category Data type . LightGBM can handle the category feature by itself, but before that, we need to convert the object dtype to category dtype, so that LightGBM can handle it. . for column in df.columns: if df[column].dtype == &#39;object&#39;: df[column] = df[column].astype(&#39;category&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null float64 1 workclass 48842 non-null category 2 fnlwgt 48842 non-null float64 3 education 48842 non-null category 4 educational-num 48842 non-null float64 5 marital-status 48842 non-null category 6 occupation 48842 non-null category 7 relationship 48842 non-null category 8 race 48842 non-null category 9 gender 48842 non-null int64 10 capital-gain 48842 non-null float64 11 capital-loss 48842 non-null float64 12 hours-per-week 48842 non-null float64 13 native-country 48842 non-null category 14 income 48842 non-null int64 dtypes: category(7), float64(6), int64(2) memory usage: 3.3 MB . Modeling Part . X = df.drop(&#39;income&#39;, axis=1) y = df[&#39;income&#39;] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y) . clf = lgb.LGBMClassifier(objective=&#39;binary&#39;, silent=False, colsample_bytree=0.9, subsample=0.9, learning_rate=0.05) . fit_params = { &#39;early_stopping_rounds&#39;: 10, &#39;eval_metric&#39;: &#39;accuracy&#39;, &#39;eval_set&#39;: [(X_test, y_test)], &#39;eval_names&#39;: [&#39;valid&#39;], &#39;verbose&#39;: 100, &#39;feature_name&#39;: &#39;auto&#39;, # actually this is default &#39;categorical_feature&#39;: &#39;auto&#39; # actually this is default } . clf.fit(X_train, y_train, **fit_params) . Training until validation scores don&#39;t improve for 10 rounds [100] valid&#39;s binary_logloss: 0.2779 Did not meet early stopping. Best iteration is: [100] valid&#39;s binary_logloss: 0.2779 . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=0.9, importance_type=&#39;split&#39;, learning_rate=0.05, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=&#39;binary&#39;, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False, subsample=0.9, subsample_for_bin=200000, subsample_freq=0) . print(f&quot;The Model Accuracy: {(clf.score(X_test, y_test)*100):.2f}%&quot;) . The Model Accuracy: 87.53% . Accuracy imporvement . Compare to last blog, the best performing model: GradientBoostingClassifier have achieved around 86% of Accuracy, here using LightGBM, without One-Hot Encoding the categorical feature, it have around 1% of Accuracy improving. . %matplotlib inline feat_imp = pd.Series(clf.feature_importances_, index=X.columns) feat_imp.nlargest(30).plot(kind=&#39;barh&#39;, figsize=(8,10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x127c9ac10&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Feature Importance . LightGBM has built-in Feature Importance examination, it shows clearly in the plot, the age and capital-gain feature is the most important features that impact the income target. .",
            "url": "https://jl1829.github.io/turbo-funicular/lightgbm/2020/04/15/Data-Science-Project-imporvement,-using-LightGBM-to-gain-more-accuracy.html",
            "relUrl": "/lightgbm/2020/04/15/Data-Science-Project-imporvement,-using-LightGBM-to-gain-more-accuracy.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "End-to-End Data Science Project with Adult Income Dataset",
            "content": "Overview . Recently I got a freelance Data Science Job from a LinkedIn member. It&#39;s typical binary classification task with applicant basic information from an online commerical startup. The business onwer consult me about building a simpel Machine Learning model to provide an automated account opening checking system, and gave me a small dataset(around 60,000 instances). I would like to share some of my experiences of this Data Science project, of course I won&#39;t be able to share the excat dataset and it&#39;s detail , but I found out the U.S. Adult Income Dataset could be one candidate to share the similarity, so I pick it as a sharing sample. . About U.S. Adult Income Dataset . Abstract: Based on every observation&#39;s attributes, predict whether a instance aka a person, income exceeds US$50,000 annually. Also known as &quot;Census Income&quot; dataset, it&#39;s originally donated by Ronny Kohavi and Barry Becker at 1996. It&#39;s Classification Task, with categorical and numerical features, some of the instance have missing values, the missing value was denoted as &quot;?&quot; . Total have 14 columns of features, the last column: income is the classification label: &gt;50k, &lt;=50k, other features as below: . age: numerical | workclass: categorical | fnlwgt: numerical | education: categorical | education-num: numerical | marital-status: categorical | occupation: categorical | relationship: categorical | race: categorical | sex: categorical | capital-gain: numerical | capital-loss: numerical | hours-per-week: numerical | native-country: categorical | . And one more things need to take note is the dataset is imbalanced, there are two classes values: &gt;50k and &lt;=50k. . &gt;50k: miniority class, around 25% | &lt;=50k: majority class, around 75% | . This dataset is openly accessable, either visit: UCI(University of California, Irvine) Machine Learning Repository, or Kaggle Adult Dataset. . Typical Data Science workflow . Before we get our hand dirty, let&#39;s understand our workflow so that we can follow the steps, normally when we receive a Data Science job, no matter is from which sector, it could be Financial, consumer, computer network, manufacturing, we need to have a proper understanding of the problem we are going to solve. An article from Towardsdatascience have a very good explaination: . . For most of the project, it will fall into the flows: . Understand the Business Problem: . In here, the problem is to predict whether a person&#39;s income is exceeded US$50,000.00 based on the features, in commercial, it could be predict a person will place order on particular products based on his/her browsing behavior | . | EDA(Exploratory Data Analysis): . Not all the data science project dataset is perfectly clean like the professior gave you in academic, real word dataset that the customer gave you will be very &quot;dirty&quot;, it contain lot&#39;s of outlier, missing value, or others intentionly wrong filling. We need to identify it and procss to next steps | . | Data Cleaning: . Following previous step, once we have identified the outlier, missing values, we will &quot;clean&quot; it via statistical method or others method. | . | Feature Engineering: . Well, this is one of the most time and brain power consuming steps, it mean we need to figure out each feature&#39;s corrlation between the label, selecting, or extracting the most relevent feature to feed into the model, it&#39;s very important step that help us to fight with the Curse of Dimensionality and the under-fit/over-fit problem. | . | Baseline Model Result: . Some of the Data Scientist will miss out this part, most of the time they will just simple feed the &quot;cleaned&quot; dataset into the model and get a simply 75% accuracy, well, in Adult dataset, I can say I can throw any dummy classification model it will get 75% accuracy, as it&#39;s imbalanced with 75% majority class, in order to understand how well the model is working, we must have some baseline data, so this is the part we figure the baseline. | . | Model Evaluate and Fine Turn: . How good is the model, can we improve more? here we fine turn the hyperparameter to make it closer to production level. | . | Iteration: . Present the work to customer, put the model into production, get feedback, imporve. | . | . Exploratory Data Analysis (EDA) . Let&#39;s get into Adult Dataset . Move to Dataset Kaggle Site to download it. Or we can use scikit-learn fetch_openml to fetch it. . # import import numpy as np import pandas as pd from collections import Counter # load dataset adult_df = pd.read_csv(&#39;/storage/adult.csv&#39;, na_values=&#39;?&#39;) # overview of the dataset print(adult_df.info()) print(&quot; n&quot;) print(adult_df.head()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null int64 1 workclass 46043 non-null object 2 fnlwgt 48842 non-null int64 3 education 48842 non-null object 4 educational-num 48842 non-null int64 5 marital-status 48842 non-null object 6 occupation 46033 non-null object 7 relationship 48842 non-null object 8 race 48842 non-null object 9 gender 48842 non-null object 10 capital-gain 48842 non-null int64 11 capital-loss 48842 non-null int64 12 hours-per-week 48842 non-null int64 13 native-country 47985 non-null object 14 income 48842 non-null object dtypes: int64(6), object(9) memory usage: 5.6+ MB None age workclass fnlwgt education educational-num marital-status 0 25 Private 226802 11th 7 Never-married 1 38 Private 89814 HS-grad 9 Married-civ-spouse 2 28 Local-gov 336951 Assoc-acdm 12 Married-civ-spouse 3 44 Private 160323 Some-college 10 Married-civ-spouse 4 18 NaN 103497 Some-college 10 Never-married occupation relationship race gender capital-gain capital-loss 0 Machine-op-inspct Own-child Black Male 0 0 1 Farming-fishing Husband White Male 0 0 2 Protective-serv Husband White Male 0 0 3 Machine-op-inspct Husband Black Male 7688 0 4 NaN Own-child White Female 0 0 hours-per-week native-country income 0 40 United-States &lt;=50K 1 50 United-States &lt;=50K 2 40 United-States &gt;50K 3 40 United-States &gt;50K 4 30 United-States &lt;=50K . # check the missing value print(&quot;Checking dataframe missing values: n&quot;) for column in adult_df.columns: if adult_df[column].isnull().sum() != 0: missingValue = adult_df[column].isnull().sum() percentage = missingValue / len(adult_df[column]) * 100 dtype = adult_df[column].dtype print(f&quot;The column: &#39;{column}&#39; with Data Type: &#39;{dtype}&#39; has missing value: {missingValue}, percentage: {percentage:.2f}%&quot;) # memory cleaning del missingValue del percentage del dtype . Checking dataframe missing values: The column: &#39;workclass&#39; with Data Type: &#39;object&#39; has missing value: 2799, percentage: 5.73% The column: &#39;occupation&#39; with Data Type: &#39;object&#39; has missing value: 2809, percentage: 5.75% The column: &#39;native-country&#39; with Data Type: &#39;object&#39; has missing value: 857, percentage: 1.75% . Well, that not too much, about how to handle the missing value, is a balance game, either throw it away, if there&#39;s not too much impact to the model performance, or impute it with some strategy, such as most-frquent since they are all categorical value, or mean, median etc if they are numerical value. In this case, as the missing value fall into the categorical features, we will use the pandas DataFrame mode() method to fill the missing value . # check the label class and it&#39;s distribution percentage label = adult_df.values[:, -1] counter = Counter(label) for key, value in counter.items(): percentage = value / len(label) * 100 print(f&quot;Class: {key}, Count = {value}, Percentage = {percentage:.1f}%.&quot;) . Class: &lt;=50K, Count = 37155, Percentage = 76.1%. Class: &gt;50K, Count = 11687, Percentage = 23.9%. . numerical_subset = adult_df.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]) . print(numerical_subset) . age fnlwgt educational-num capital-gain capital-loss 0 25 226802 7 0 0 1 38 89814 9 0 0 2 28 336951 12 0 0 3 44 160323 10 7688 0 4 18 103497 10 0 0 ... ... ... ... ... ... 48837 27 257302 12 0 0 48838 40 154374 9 0 0 48839 58 151910 9 0 0 48840 22 201490 9 0 0 48841 52 287927 9 15024 0 hours-per-week 0 40 1 50 2 40 3 40 4 30 ... ... 48837 38 48838 40 48839 40 48840 20 48841 40 [48842 rows x 6 columns] . import matplotlib.pyplot as plt numerical_subset.hist(bins=20, figsize=(20, 15)) plt.show() . Data Cleaning . After we have explored the overview of the dataset, we are going to dig out any anomaly in the dataset and clean it out. . # use DataFrame `mode()` method adult_df = adult_df.fillna(adult_df.mode().iloc[0]) print(adult_df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null int64 1 workclass 48842 non-null object 2 fnlwgt 48842 non-null int64 3 education 48842 non-null object 4 educational-num 48842 non-null int64 5 marital-status 48842 non-null object 6 occupation 48842 non-null object 7 relationship 48842 non-null object 8 race 48842 non-null object 9 gender 48842 non-null object 10 capital-gain 48842 non-null int64 11 capital-loss 48842 non-null int64 12 hours-per-week 48842 non-null int64 13 native-country 48842 non-null object 14 income 48842 non-null object dtypes: int64(6), object(9) memory usage: 5.6+ MB None . The Non-Null Count shows there&#39;s no more missing value from the dataset, it filled by the most_frequent value . # checking the outliers print(numerical_subset.describe()) . age fnlwgt educational-num capital-gain count 48842.000000 4.884200e+04 48842.000000 48842.000000 mean 38.643585 1.896641e+05 10.078089 1079.067626 std 13.710510 1.056040e+05 2.570973 7452.019058 min 17.000000 1.228500e+04 1.000000 0.000000 25% 28.000000 1.175505e+05 9.000000 0.000000 50% 37.000000 1.781445e+05 10.000000 0.000000 75% 48.000000 2.376420e+05 12.000000 0.000000 max 90.000000 1.490400e+06 16.000000 99999.000000 capital-loss hours-per-week count 48842.000000 48842.000000 mean 87.502314 40.422382 std 403.004552 12.391444 min 0.000000 1.000000 25% 0.000000 40.000000 50% 0.000000 40.000000 75% 0.000000 45.000000 max 4356.000000 99.000000 . Well, from the column capital-gain, the maximum value is 99999 which seems like little bit werid, we can check the dataset description, the capital-gain means the additional income from capital market, such as stocks, securities, 99999 indicated somebody wrongly imput or it repersented as None additional income, here we can try to replace it with the mean value, and we will replace the 99 hours in the hours-per-week column also . # check the quantity of this outlier print(f&quot;There&#39;s {adult_df[adult_df[&#39;capital-gain&#39;] == 99999].shape[0]} outlier in the capital-gain column&quot;) print(f&quot;There&#39;s {adult_df[adult_df[&#39;hours-per-week&#39;] == 99].shape[0]} outlier in the hours-per-week column&quot;) . There&#39;s 244 outlier in the capital-gain column There&#39;s 137 outlier in the hours-per-week column . # replace it with mean value adult_df[&#39;capital-gain&#39;].replace(99999, np.mean(adult_df[&#39;capital-gain&#39;].values), inplace=True) adult_df[&#39;hours-per-week&#39;].replace(99, np.mean(adult_df[&#39;hours-per-week&#39;].values), inplace=True) print(adult_df.describe()) . age fnlwgt educational-num capital-gain count 48842.000000 4.884200e+04 48842.000000 48842.000000 mean 38.643585 1.896641e+05 10.078089 584.893278 std 13.710510 1.056040e+05 2.570973 2530.549506 min 17.000000 1.228500e+04 1.000000 0.000000 25% 28.000000 1.175505e+05 9.000000 0.000000 50% 37.000000 1.781445e+05 10.000000 0.000000 75% 48.000000 2.376420e+05 12.000000 0.000000 max 90.000000 1.490400e+06 16.000000 41310.000000 capital-loss hours-per-week count 48842.000000 48842.000000 mean 87.502314 40.258074 std 403.004552 11.995662 min 0.000000 1.000000 25% 0.000000 40.000000 50% 0.000000 40.000000 75% 0.000000 45.000000 max 4356.000000 98.000000 . After the data exploration and cleaning, we save the cleaned DataFrame to adult_cleaned.csv file . adult_df.to_csv(&#39;/storage/adult_cleaned.csv&#39;, index=False) . Baseline Model Result . We will evaluate candidate models using repeated stratified k-fold cross-validation . The k-fold cross-validation procedure provides a good general estimate of model performance that is not too optimistically biased, at least compared to a single train-test split. We will use k=10, meaning each fold will contain about 45,222/10, or about 4,522 examples. . Stratified means that each fold will contain the same mixture of examples by class, that is about 75% to 25% for the majority and minority classes respectively. Repeated means that the evaluation process will be performed multiple times to help avoid fluke results and better capture the variance of the chosen model. We will use three repeats. . This means a single model will be fit and evaluated 10 * 3 or 30 times and the mean and standard deviation of these runs will be reported. . This can be achieved using the RepeatedStratifiedKFold scikit-learn class. . We will predict a class label for each example and measure model performance using classification accuracy. . The evaluate_model() function below will take the loaded dataset and a defined model and will evaluate it using repeated stratified k-fold cross-validation, then return a list of accuracy scores that can later be summarized. . import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold def load_dataset(filename): df = pd.read_csv(filename) X, y = df.iloc[:, :-1], df.iloc[:, -1] cate_index = X.select_dtypes(include=[&#39;object&#39;]).columns num_index = X.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns y = LabelEncoder().fit_transform(y) return X, y, cate_index, num_index def evaluate_model(X, y, model): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42) scores = cross_val_score(model, X, y, scoring=&#39;accuracy&#39;, cv=cv, n_jobs=-1) return scores . from sklearn.dummy import DummyClassifier X, y, cate_index, num_index = load_dataset(&#39;storage/adult_cleaned.csv&#39;) model = DummyClassifier(strategy=&#39;most_frequent&#39;) . scores = evaluate_model(X, y, model) print(scores) . [0.76069601 0.76069601 0.76085176 0.76085176 0.76085176 0.76064701 0.76064701 0.76064701 0.76064701 0.76064701 0.76069601 0.76069601 0.76085176 0.76085176 0.76085176 0.76064701 0.76064701 0.76064701 0.76064701 0.76064701 0.76069601 0.76069601 0.76085176 0.76085176 0.76085176 0.76064701 0.76064701 0.76064701 0.76064701 0.76064701] . print(f&quot;The Dummy Classifier mean accuracy: {(np.mean(scores)*100):.2f}%, with Standard Deviation: {np.std(scores):.2f}&quot;) . The Dummy Classifier mean accuracy: 76.07%, with Standard Deviation: 0.00 . print(f&quot;The type of dataset: {type(X)}.&quot;) print(f&quot;The shape of the dataset: Row: {X.shape[0]}, with {X.shape[1]} fetures&quot;) print(f&quot;The type of the target label: {type(y)}&quot;) print(f&quot;The shape of the target label is: {y.shape[0]} dimensional vector.&quot;) . The type of dataset: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;. The shape of the dataset: Row: 48842, with 14 fetures The type of the target label: &lt;class &#39;numpy.ndarray&#39;&gt; The shape of the target label is: 48842 dimensional vector. . Now that we have a test harness and a baseline in performance. In this case, we can see that the baseline algorithm achieves an accuracy of about 76.07%. This score provides a lower limit on model skill; any model that achieves an average accuracy above about 76.07% has skill, whereas models that achieve a score below this value do not have skill on this dataset. Now we can begin to evaluate some models on this dataset . Model Evaluate and Fine Turn . Evaluate Machine Learning Algorithms . Let’s start by evaluating a mixture of machine learning models on the dataset. . It can be a good idea to spot check a suite of different nonlinear algorithms on a dataset to quickly flush out what works well and deserves further attention, and what doesn’t. . We will evaluate the following machine learning models on the adult dataset: . Decision Tree (CART) | Support Vector Machine (SVM) | Bagged Decision Trees (BAG) | Random Forest (RF) | Gradient Boosting Machine (GBM) | . We will use mostly default model hyperparameters, with the exception of the number of trees in the ensemble algorithms, which we will set to a reasonable default of 100. . We will define each model in turn and add them to a list so that we can evaluate them sequentially. The generate_models() function below defines the list of models for evaluation, as well as a list of model short names for plotting the results later. . import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.ensemble import GradientBoostingClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.neural_network import MLPClassifier from sklearn.svm import SVC from sklearn.ensemble import BaggingClassifier def load_dataset(filename): df = pd.read_csv(filename) X, y = df.iloc[:, :-1], df.iloc[:, -1] cate_index = X.select_dtypes(include=[&#39;object&#39;]).columns num_index = X.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns y = LabelEncoder().fit_transform(y) return X, y, cate_index, num_index X, y, cate_index, num_index = load_dataset(&#39;/storage/adult_cleaned.csv&#39;) print(type(X)) print(X.shape) print(type(y)) print(y.shape) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; (48842, 14) &lt;class &#39;numpy.ndarray&#39;&gt; (48842,) . def generate_models(): models, names = [], [] names.append(&#39;CART&#39;) models.append(DecisionTreeClassifier()) names.append(&#39;SVM&#39;) models.append(SVC(gamma=&#39;scale&#39;)) names.append(&#39;BAG&#39;) models.append(BaggingClassifier(n_estimators=100)) names.append(&#39;RF&#39;) models.append(RandomForestClassifier(n_estimators=100)) names.append(&#39;GBM&#39;) models.append(GradientBoostingClassifier(n_estimators=100)) names.append(&#39;Neural Network&#39;) models.append(MLPClassifier(early_stopping=True)) return models, names models, names = generate_models() . As now the X array still in pandas DataFrame with categorical values, here we need to &quot;encoding&quot; the categorical values into numerical values, OneHotEncoder with Scikit-Learn Pipeline are quite handy . steps = [(&#39;Categorical&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), cate_index), (&#39;Numerical&#39;, MinMaxScaler(), num_index)] from sklearn.compose import ColumnTransformer transformer = ColumnTransformer(steps, verbose=True) X = transformer.fit_transform(X) print(type(X)) print(X.shape) . [ColumnTransformer] ... (1 of 2) Processing Categorical, total= 0.1s [ColumnTransformer] ..... (2 of 2) Processing Numerical, total= 0.0s &lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt; (48842, 105) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (34189, 105) (14653, 105) (34189,) (14653,) . # filter unwanted warning import warnings warnings.filterwarnings(&#39;ignore&#39;) . # evaluate each model in default parameters for i in range(len(models)): print(f&quot;&quot;&quot; ******************************** Now evaluating {names[i]} model ******************************** n&quot;&quot;&quot;) scores = evaluate_model(X_train, y_train, models[i]) print(f&quot;The {names[i]} model average accuracy is: {(np.mean(scores)*100):.2f}%, with Standard Deviation: {(np.std(scores)*100):.2f}.&quot;) . ******************************** Now evaluating CART model ******************************** The CART model average accuracy is: 81.20%, with Standard Deviation: 0.70. ******************************** Now evaluating SVM model ******************************** The SVM model average accuracy is: 84.61%, with Standard Deviation: 0.61. ******************************** Now evaluating BAG model ******************************** The BAG model average accuracy is: 85.24%, with Standard Deviation: 0.54. ******************************** Now evaluating RF model ******************************** The RF model average accuracy is: 84.99%, with Standard Deviation: 0.58. ******************************** Now evaluating GBM model ******************************** The GBM model average accuracy is: 86.31%, with Standard Deviation: 0.47. ******************************** Now evaluating Neural Network model ******************************** The Neural Network model average accuracy is: 85.00%, with Standard Deviation: 0.58. . In this case, we can see that all of the chosen algorithms are skillful, achieving a classification accuracy above 76.07%. We can see that the ensemble decision tree algorithms perform the best with perhaps stochastic gradient boosting performing the best with a classification accuracy of about 86.3%. . This accuracy is using the default Hyperperameter, we can pick two top performance algorithms to use scikit-learn GridSearch() to fine turn the Hyperperameter to see whether it can get better performance. . The best two performance algorithms: . BaggingClassfier(n_estimators=100) | GradientBoostingClassfier(n_estimators=100) | . We can try to fine turn this two model. . # fine turn BaggingClassifier from sklearn.model_selection import GridSearchCV BAGgrid = {&#39;n_estimators&#39;: [100, 200]} cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42) BAGclf = BaggingClassifier() BAGgrid_search = GridSearchCV(estimator=BAGclf, param_grid=BAGgrid, n_jobs=-1, cv=cv, scoring=&#39;accuracy&#39;, error_score=0) BAGgrid_result = BAGgrid_search.fit(X_train, y_train) . print(BAGgrid_result.best_score_) print(BAGgrid_result.best_params_) . 0.8518822471349845 {&#39;n_estimators&#39;: 200} . # fine turn GradientBoostingClassifier GBMgrid = {&#39;n_estimators&#39;: [100, 200]} GBMclf = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500, min_samples_leaf=50, max_depth=8, max_features=&#39;sqrt&#39;, subsample=0.8, random_state=42) GBMgrid_search = GridSearchCV(estimator=GBMclf, param_grid=GBMgrid, n_jobs=-1, cv=cv, scoring=&#39;accuracy&#39;, error_score=0) GBMgrid_result = GBMgrid_search.fit(X_train, y_train) . print(GBMgrid_result.best_score_) print(GBMgrid_result.best_params_) . 0.8669747643525415 {&#39;n_estimators&#39;: 200} . Well, seems like if the n_estimators is equal to 200, the GradientBoostingClassifier performance incrase to 86.69%, then we can update our hyperparameter for GradientBoostingClassfier and train it according to our Training Subset, now we have the winner, is GradientBoostingClassifier algorithm. . Actually using GridSearchCV is quite computational expensive, I would suggest to use Cloud Notebook Envirnoment, such Google Colab, AWS, or Google Cloud or Gradient, or Kaggle, both of them provide quite power CPU and tons of memory, and most important, they provide free GPU in certain amount of time . Final: Train the best model, save it, and deliver to customer. . We pick GradientBoostingClassifier as our final model, we will train it with Training Subset, and see how it goes in Testing Subset, then finally we will save it and deliver to our customer. . model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, min_samples_split=500, min_samples_leaf=50, max_depth=8, max_features=&#39;sqrt&#39;, subsample=0.8, random_state=42) model.fit(X_train, y_train) . GradientBoostingClassifier(ccp_alpha=0.0, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=8, max_features=&#39;sqrt&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=50, min_samples_split=500, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, presort=&#39;deprecated&#39;, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . # evaluate the testing subset TestScore = model.score(X_test, y_test) print(f&quot;The model test set accuracy is: {(TestScore*100):.1f}%.&quot;) . The model test set accuracy is: 87.4%. . # perform the Classification Report from sklearn.metrics import classification_report predicted = model.predict(X_test) print(classification_report(y_test, predicted)) . precision recall f1-score support 0 0.90 0.94 0.92 11147 1 0.79 0.65 0.71 3506 accuracy 0.87 14653 macro avg 0.84 0.80 0.82 14653 weighted avg 0.87 0.87 0.87 14653 . # to save the model import joblib joblib.dump(model, &#39;storage/final_model.sav&#39;) . [&#39;storage/final_model.sav&#39;] . Further Reading . Paper . Scaling Up The Accuracy of Naive-bayes Classifiers: A Decision-tree Hybrid, 1996 | . APIs . pandas.DataFrame.select_dtypes API | sklearn.model_selection.RepeatedStratifiedKFold API | sklearn.dummy.DummyClassifier API | pandas.DataFrame.mode() API | .",
            "url": "https://jl1829.github.io/turbo-funicular/machine%20learning/2020/03/24/End-to-End-Data-Science-Project-with-Adult-Income-Dataset.html",
            "relUrl": "/machine%20learning/2020/03/24/End-to-End-Data-Science-Project-with-Adult-Income-Dataset.html",
            "date": " • Mar 24, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The map(), filter(), reduce(), zip() and Lambda() funcion in Python",
            "content": "About . Today I would like to share some straightforward example of these important built-in functions in Python: . map | filter | reduce | zip | lambda | . They are some convenient function for us to deal with sequences, benefited by the world of functional programming, we can apply this functions into every single elements of a sequances, in order to reduce the time of explicit Loop. What&#39;s more, all this functions are pure function, both have return value, we can use this return value to repersent our expression. . So in layman term, why to use them is it cam much simplify our code, to execute those loop and iteration mission in a simple, elegance and efficient way. . map() . The map() function is going to return an iterator that applies function to every item of iterable, yielding the results. Such as list we can check whether an object have iterable attribute by using hasattr such as: . &gt; &gt;&gt; a = [1, 2, 3, 4] &gt;&gt;&gt; hasattr(a, &#39;__iter__&#39;) &gt;&gt;&gt; True . map() function simple syntex:map(func, iterable)* parameter: func is an function that map() pass to the every elements in the iterable object, the iterable is an object that has __iter__ attribute, so every elements can execute the func | return value: a map object | . Sounds like complicated? let&#39;s see an example: Assume we have a list that contain 1 - 5 digits, we want to every number add 1, before map() function, most likely we will do this: . numbers = [1, 2, 3, 4, 5] for i in range(0, len(numbers)): numbers[i] += 1 print(numbers) . [2, 3, 4, 5, 6] . Or, in another way: . numbers = [1, 2, 3, 4, 5] # create empty list result = [] for n in numbers: result.append(n+1) print(result) . [2, 3, 4, 5, 6] . Obviously, no matter in which way, we all need to handle loop, so, we can try to use map() function: . def add_one(n): return n+1 numbers = [1, 2, 3, 4, 5] result = map(add_one, numbers) print(result) print(type(result)) print(list(result)) . &lt;map object at 0x10a3c7150&gt; &lt;class &#39;map&#39;&gt; [2, 3, 4, 5, 6] . I believe you already notice the beauty of map(), we have achieved our purpose by not using loop, meanwhile, the code written in an elegent and simplicity way. So the map() function will return a map object if we planning to use this object in the future, this object type will help us to save the memory utilization, we use the getsizeof() function from sys to see the memory utilization of each object, map object and list . from sys import getsizeof print(f&#39;The size of map object in memory is {getsizeof(result)} bytes&#39;) print(f&#39;Convert it into list: {getsizeof(list(result))} bytes&#39;) . The size of map object in memory is 64 bytes Convert it into list: 72 bytes . The requirement of object to passed in map() function is iterable so as long as the object has attribute of __iter__ it works, not only list, but also tuple, such as: . numbers = (1, 2, 3, 4, 5) print(f&quot;Is tuple numbers iterable? Answer: {hasattr(numbers, &#39;__iter__&#39;)}&quot;) result = map(add_one, numbers) print(result) print(type(result)) print(tuple(result)) . Is tuple numbers iterable? Answer: True &lt;map object at 0x109bb9410&gt; &lt;class &#39;map&#39;&gt; (2, 3, 4, 5, 6) . Have you notice in order to achieved this, we need to create a function called add_one(n)? and it just simply return n+1, possible to reduce this? to make the code more simply and elegent? Yes, we can use lambda . numbers = (1, 2, 3, 4, 5) result = map(lambda x: x + 1, numbers) print(tuple(result)) . (2, 3, 4, 5, 6) . Three lines of code, simple, elegent, Pythonic . Beside using defined function or lambda function to execute the iterable, we also can utilize Python bulit-in function, bulit-in type to execute the iterable, such this case: . # list of strings words = [&#39;Singapore&#39;, &#39;Guangzhou&#39;, &#39;Tokyo&#39;] # convert every elements in the array into List converted = list(map(list, words)) print(converted) print(f&quot;The type of converted: {type(converted)}&quot;) print(f&quot;The lenght of converted: {len(converted)}&quot;) . [[&#39;S&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39;a&#39;, &#39;p&#39;, &#39;o&#39;, &#39;r&#39;, &#39;e&#39;], [&#39;G&#39;, &#39;u&#39;, &#39;a&#39;, &#39;n&#39;, &#39;g&#39;, &#39;z&#39;, &#39;h&#39;, &#39;o&#39;, &#39;u&#39;], [&#39;T&#39;, &#39;o&#39;, &#39;k&#39;, &#39;y&#39;, &#39;o&#39;]] The type of converted: &lt;class &#39;list&#39;&gt; The lenght of converted: 3 . words is a list that contain string type of elements, we can use map() and Python bulit-in list to convert every elements in words into List, but do take note, every elements must have __iter__ attribute, otherwise, it will raise TypeError, such as int type: . numbers = [3, &#39;23&#39;, 42] print(list(map(list, numbers))) . TypeError Traceback (most recent call last) &lt;ipython-input-14-58446c133a68&gt; in &lt;module&gt; 1 numbers = [3, &#39;23&#39;, 42] -&gt; 2 print(list(map(list, numbers))) TypeError: &#39;int&#39; object is not iterable . We can see: TypeError, int object is not iterable, we can avoid it by this way: . numbers = [3, &#39;23&#39;, 42] print(list(map(float, numbers))) . [3.0, 23.0, 42.0] . filter() . filter() function is using a function to &quot;filter&quot; the sequence, the function is going to examinate every elements in the sequence is True or False . filter() syntex: filter(func, iterable) | Parameter: func test iterable sequances&#39; elements is True or False, iterable is the iterable sequances that been filter | Return value: an iterable sequance that every elements is True to the filter function func | . Layman term: filter() is to filter a set of data based on the given conditions . Example: . # filter vowel def func(variable): letters = [&#39;a&#39;, &#39;e&#39;, &#39;i&#39;, &#39;o&#39;, &#39;u&#39;] if (variable.lower() in letters): return True else: return False # given sequance sequance = [&#39;I&#39;, &#39;l&#39;, &#39;o&#39;, &#39;v&#39;, &#39;e&#39;, &#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] filtered = list(filter(func, sequance)) print(f&quot;The vowel in the sequance is {filtered}&quot;) . The vowel in the sequance is [&#39;I&#39;, &#39;o&#39;, &#39;e&#39;, &#39;o&#39;] . Above we create a method to pull the vowel from a given sequance, the given sequance is List, so it have &#39;__iter__&#39;, and apply it to filter() to pull out the vowel. . Here we have another example: . # positive or negitive number def positive(num): if num &gt; 0: return True else: return False # odd or even number def even_number(num): if num % 2 == 0: return True else: return False numbers = [1, -3, 5, -20, 0, 9, 12] positive_number = list(filter(positive, numbers)) even_number = list(filter(even_number, numbers)) print(f&quot;The positive number is: {positive_number}.&quot;) print(f&quot;The even number is {even_number}.&quot;) . The positive number is: [1, 5, 9, 12]. The even number is [-20, 0, 12]. . So, how to use filter() function is quite simple: . define a method that can filter out True or False | apply it to iterable object | Integrate it into your bigger code block | Now let&#39;s see how to use lambda together: . numbers = [0, 1, 2, -3, 5, -8, 42] # odd number odd_number = filter(lambda x: x % 2, numbers) print(f&quot;The odd number is {list(odd_number)}.&quot;) # even number even_number = filter(lambda x: x % 2 == 0, numbers) print(f&quot;The even number is {list(even_number)}.&quot;) # positive number positive_number = filter(lambda x: x &gt; 0, numbers) print(f&quot;The positive number is {list(positive_number)}.&quot;) . The odd number is [1, -3, 5]. The even number is [0, 2, -8, 42]. The positive number is [1, 2, 5, 42]. . Always remember the Python philosophy： efficient, simple, elegent . Reduce() . reduce() is very useful built-in function, it can execuate iterable object&#39;s compuatation and return the result, it can rolling compute the continues values in an iterable sequance, such as cumulative product of integer list, or cumulative sum. . Syntex: reduce(func, iterable) | Parameter: func: a continues method to execuate on each element of the iterable object, last resut is the new parameter of next execuation. | Return value: the func return value | . In Python 3, reduce() moved to functools module, so before we use it, we need to import it from functools . Example: . from functools import reduce def do_sum(num1, num2): return num1 + num2 print(f&quot;The sum of 1, 2, 3, 4 is: {reduce(do_sum, [1, 2, 3, 4])}.&quot;) . The sum of 1, 2, 3, 4 is: 10. . # cumulative product example def multiply(num1, num2): return num1*num2 print(f&quot;The cumulative product of 1, 2, 3, 4 is: {reduce(multiply, [1, 2, 3, 4])}.&quot;) . The cumulative product of 1, 2, 3, 4 is: 24. . # more simple and elegent way with lambda numbers = [1, 2, 3, 4] result_multiply = reduce(lambda x, y: x*y, numbers) result_sum = reduce(lambda x, y: x+y, numbers) print(f&quot;The cumulative product of 1, 2, 3, 4 is: {result_multiply}&quot;) print(f&quot;The cumulative sum of 1, 2, 3, 4 is: {result_sum}.&quot;) . The cumulative product of 1, 2, 3, 4 is: 24 The cumulative sum of 1, 2, 3, 4 is: 10. . zip() . As it&#39;s name, zip() function is to put multiple iterable object together, and &quot;packed&quot; it as one single object, mapping with similar index. . Syntex: zip(*iterators) | Parameter: iterators is iterable object, such as List, String | Return value: Single iterator object, containing index value from the packed object. | . Example: . keys = [&#39;name&#39;, &#39;age&#39;] values = [&#39;Apple&#39;, &#39;44&#39;] apple_dict = dict(zip(keys, values)) print(apple_dict) . {&#39;name&#39;: &#39;Apple&#39;, &#39;age&#39;: &#39;44&#39;} . zip() it also support multiple objects: . names = [&#39;Apple&#39;, &#39;Google&#39;, &#39;Microsoft&#39;] ages = [&#39;44&#39;, &#39;21&#39;, &#39;44&#39;] values = [&#39;100&#39;, &#39;80&#39;, &#39;60&#39;] mapped_values = list(zip(names, ages, values)) print(mapped_values) . [(&#39;Apple&#39;, &#39;44&#39;, &#39;100&#39;), (&#39;Google&#39;, &#39;21&#39;, &#39;80&#39;), (&#39;Microsoft&#39;, &#39;44&#39;, &#39;60&#39;)] . We can use the zip() function to easily packed the values have same index from 3 list . But how about unpack? . Simple, just similar to unpanc tuple, we add the * to the object that we want to unpack . names, ages, values = zip(*mapped_values) print(f&quot;The names is {names}&quot;) print(f&quot;The ages is {ages}&quot;) print(f&quot;The values is {values}&quot;) . The names is (&#39;Apple&#39;, &#39;Google&#39;, &#39;Microsoft&#39;) The ages is (&#39;44&#39;, &#39;21&#39;, &#39;44&#39;) The values is (&#39;100&#39;, &#39;80&#39;, &#39;60&#39;) . lambda() . While normal functions are defined using the def keyword, in Python anonymous functions are defined using the lambda keyword. Hence, anonymous functions are also called lambda functions. . Lambda function can use any quantity of parameter, but only have one expression . Syntex:lambda argument: manipulate(argument)* Parameter: argument is the passing parameter, after the : is the manipulate | . Example: . add_one = lambda x: x+1 add_sum = lambda x, y: x+y print(add_one(2)) print(add_sum(5, 5)) . 3 10 . Normally we will not use lambda function individually, we will use it along with other built-in function or def function, which we have already shows above, use it along with map(), filter(), reduce() and zip() function. . Let&#39;s see one more example of lambda interacting with dict . university = [{&#39;name&#39;: &#39;NYU&#39;, &#39;city&#39;: &#39;New York&#39;}, {&#39;name&#39;: &#39;NUS&#39;, &#39;city&#39;: &quot;Singapore&quot;}] names = list(map(lambda x: x[&#39;name&#39;], university)) print(names) . [&#39;NYU&#39;, &#39;NUS&#39;] . Above we interacting with dict with map() function, given the condition for the iterable list of dict that contain the name and the city of each University . We also can interacting dict with filter() function, through the return value True or False to judge the filtering condition. . university = [{&#39;name&#39;: &#39;NYU&#39;, &#39;city&#39;: &#39;New York&#39;}, {&#39;name&#39;: &#39;NUS&#39;, &#39;city&#39;: &quot;Singapore&quot;}] names = list(filter(lambda x: x[&#39;name&#39;] == &#39;NUS&#39;, university)) print(names) . [{&#39;name&#39;: &#39;NUS&#39;, &#39;city&#39;: &#39;Singapore&#39;}] . Through the above example, you have seen the actual application scenario of lambda, but here I want to share my views with you: I think that the disadvantages of lambda are slightly more than the advantages, and you should avoid overusing lambda. . First of all, this is just my personal opinion. I hope everyone understands why I said this. First, let&#39;s compare the lambda method with the conventional def. I find that the main differences between lambda and def are as follows: . Passing the parameter immediately(no need to define variable). | one line of code, very simple (but, it doesn&#39;t mean is efficient). | Automatically return, no need return keyword. | lambda function DO NOT have a function name | . You can see the advantages. I mainly want to talk about its disadvantages. First of all, starting from real needs, we don’t need lambda most of the time, because we can always find better alternatives. Now let ’s take a look In the example of lambda + reduce(), the result we achieved with lambada is as follows: . from functools import reduce numbers = [1,2,3,4] result_multiply = reduce((lambda x, y: x * y), numbers) result_add = reduce((lambda x,y: x+y), numbers) . Above example, the lambda didn&#39;t achieved the simple and efficient purpose, as we have the bulit-in sum and mul method, even it can work with NumPy also . from functools import reduce import operator import numpy as np numbers = [1, 2, 3, 4] result_sum = sum(numbers) result_multiply = reduce(operator.mul, numbers) print(f&quot;The sum is {result_sum}&quot;) print(f&quot;The cumulative product is: {result_multiply}&quot;) matrixA = np.random.randn(3, 3) matrixB = np.random.randn(3, 3) matrixList = [matrixA, matrixB] mulmat = reduce(operator.mul, matrixList) print(f&quot;The Matrix Multipication is n {mulmat}&quot;) . The sum is 10 The cumulative product is: 24 The Matrix Multipication is [[-2.09128347 -0.96279072 -1.81723096] [-1.26743106 0.45465535 0.47941216] [-0.36291425 -0.02198572 -0.52437492]] . The result is the same, but obviously the solution using sum and mul is more efficient. Another common example shows that if we have a list that stores various colors, we now need to capitalize the first letter of each color. If we write it in lambda: . colors = [&#39;red&#39;,&#39;purple&#39;,&#39;green&#39;,&#39;blue&#39;] result = map(lambda c:c.capitalize(),colors) print(list(result)) . [&#39;Red&#39;, &#39;Purple&#39;, &#39;Green&#39;, &#39;Blue&#39;] . Seems ok, but, did we forgot the power of List ? . colors = [&#39;red&#39;,&#39;purple&#39;,&#39;green&#39;,&#39;blue&#39;] result = [c.capitalize() for c in colors] print(result) . [&#39;Red&#39;, &#39;Purple&#39;, &#39;Green&#39;, &#39;Blue&#39;] . Sorted can also handle the case of irregular initials, saving even more time: . colors = [&#39;Red&#39;,&#39;purple&#39;,&#39;Green&#39;,&#39;blue&#39;] print(sorted(colors,key=str.capitalize)) . [&#39;blue&#39;, &#39;Green&#39;, &#39;purple&#39;, &#39;Red&#39;] . There is another reason: lambda functions do not have function names. So in the case of code transfer and project migration, it will bring a lot of difficulties to the team. It is not bad to write a function add_one(), because everyone is easy to understand and know that it is a function of performing +1, but if you are in the team, use a lots of lambda will make peoples confused. . Scenario that fit lambda . Your method is too simple to have a name, such as adding 1, or just stiching the strings | Using lambda is much easier for people to understand | There&#39;s no any Python built-in function except lambda | Team members are all well understand lambda, most importantly, no one complain you. |",
            "url": "https://jl1829.github.io/turbo-funicular/python/2020/03/06/The-map,-filter,-reduce,-zip-and-lambda-in-Python.html",
            "relUrl": "/python/2020/03/06/The-map,-filter,-reduce,-zip-and-lambda-in-Python.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Using Scikit-Learn Neural Network Class to classify MNIST",
            "content": "About . Yann LeCun&#39;s MNIST is the most &quot;used&quot; dataset in Machine Learning I believe, lot&#39;s ML/DL practitioner will use it as the &quot;Hello World&quot; problem in Machine Learning, it&#39;s old, but golden, Even Geoffrey Hinton&#39;s Capsule Network also using MNIST as testing. . Most the tutorial online will guide the learner to use TensorFlow or Keras or PyTorch library to tackle MNIST problem, but actually it&#39;s not necessary， there&#39;s multiple solution for a single problem, we can tackle MNIST problem by &quot;Pure&quot; Python code, crafting the algorithm from scratch, or using the convential Machine Learning Library Scikit-Learn MLPClassifier . import matplotlib.pyplot as plt from sklearn.datasets import fetch_openml from sklearn.neural_network import MLPClassifier . # load the data from https://www.openml.org/d/554 X, y = fetch_openml(&#39;mnist_784&#39;, version=1, return_X_y=True) # use the traditional train/test split X_train, X_test = X[:60000], X[60000:] y_train, y_test = y[:60000], y[60000:] . Next we going to bulid a single hidden layer MLP model . mlp = MLPClassifier(hidden_layer_sizes=(50, ), max_iter=10, alpha=1e-4, solver=&#39;sgd&#39;, verbose=10, random_state=1, learning_rate_init=.1) . # start the training mlp.fit(X_train, y_train) . Iteration 1, loss = inf Iteration 2, loss = 94144.72785948 Iteration 3, loss = 94116.48942606 Iteration 4, loss = 94088.25915097 Iteration 5, loss = 94060.03773325 Iteration 6, loss = 94031.82434269 Iteration 7, loss = 94003.61939577 Iteration 8, loss = 93975.42334401 Iteration 9, loss = 93947.23547037 Iteration 10, loss = 93919.05614157 . MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(50,), learning_rate=&#39;constant&#39;, learning_rate_init=0.1, max_fun=15000, max_iter=10, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver=&#39;sgd&#39;, tol=0.0001, validation_fraction=0.1, verbose=10, warm_start=False) . Obsiously the loss is abnormal, is because we didnt scale the data . X = X / 255. . # use the traditional train/test split X_train, X_test = X[:60000], X[60000:] y_train, y_test = y[:60000], y[60000:] . mlp.fit(X_train, y_train) . Iteration 1, loss = 0.32009978 Iteration 2, loss = 0.15347534 Iteration 3, loss = 0.11544755 Iteration 4, loss = 0.09279764 Iteration 5, loss = 0.07889367 Iteration 6, loss = 0.07170497 Iteration 7, loss = 0.06282111 Iteration 8, loss = 0.05530788 Iteration 9, loss = 0.04960484 Iteration 10, loss = 0.04645355 . MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(50,), learning_rate=&#39;constant&#39;, learning_rate_init=0.1, max_fun=15000, max_iter=10, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver=&#39;sgd&#39;, tol=0.0001, validation_fraction=0.1, verbose=10, warm_start=False) . print(f&quot;Training set score: {mlp.score(X_train, y_train):.3f}&quot;) print(f&quot;Test set score: {mlp.score(X_test, y_test):.3f}&quot;) . Training set score: 0.987 Test set score: 0.970 . fig, axes = plt.subplots(4, 4) # use global min / max to ensure all weights are shown on the same scale vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max() for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()): ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax) ax.set_xticks(()) ax.set_yticks(()) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://jl1829.github.io/turbo-funicular/jupyter/2020/03/02/Sklearn_MLP_for_MNIST.html",
            "relUrl": "/jupyter/2020/03/02/Sklearn_MLP_for_MNIST.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Cost-Sensitive Decision Trees for Imbalanced Classification",
            "content": "Overview . This tutorial divided into 4 pars: . Imbalanced classfication dataset | Decision Trees for Imbalanced classfication | Weighted Decision Tree with Scikit-Learn | Grid Search Weighted Decision Trees | . Imbalanced classfication dataset . we use the make_classification()function to define a synthetic imbalanced two-class classfication dataset. We generate 10,000 examples with an approximate 1:100 minority to majority class ratio . # import the packages from collections import Counter from sklearn.datasets import make_classification from matplotlib import pyplot as plt import numpy as np . # generate dataset X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3) # examinate the info of X and y print(f&#39;The type of X is {type(X)}&#39;) print(f&#39;The type of y is {type(y)}&#39;) print(&#39; n&#39;) print(f&#39;The size of X is {X.shape}&#39;) print(f&#39;The size of y is {y.shape}&#39;) . The type of X is &lt;class &#39;numpy.ndarray&#39;&gt; The type of y is &lt;class &#39;numpy.ndarray&#39;&gt; The size of X is (10000, 2) The size of y is (10000,) . # summarize class distribution counter = Counter(y) print(counter) . Counter({0: 9900, 1: 100}) . # scatter plot of examples by class label for label, _ in counter.items(): row_ix = np.where(y == label)[0] plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) plt.legend() plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Fit it with standard decision tree model . A decision tree can be defined using the DecisionTreeClassifier class in the scikit-learn library . # import packages from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.tree import DecisionTreeClassifier # define model model = DecisionTreeClassifier() . # define evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate model score = cross_val_score(model, X, y, scoring=&#39;roc_auc&#39;, cv=cv, n_jobs=-1) # summarize performance print(f&#39;Mean ROC AUC: {np.mean(score):.3f}&#39;) . Mean ROC AUC: 0.734 . The model performance is achieving a ROC AUC above 0.5 which is 0.734 . This model can be the baseline for comparision for any modification performed to the standard decision tree algorithm . Decision Trees for Imbalanced Classification . The decision tree algorithm is also known as Classification and Regression Trees and involves growing a tree to classify examples from the training dataset. . The tree can be thought to divide the training dataset, where examples progress down the decision points of the tree to arrive in the leaves of the tree and are assigned a class label. . The tree is constructed by splitting the training dataset using values for variables in the dataset. At each point, the split in the data that results in the purest (least mixed) groups of examples is chosen in a greedy manner. . Here, purity means a clean separation of examples into groups where a group of examples of all 0 or all 1 class is the purest, and a 50-50 mixture of both classes is the least pure. Purity is most commonly calculated using Gini impurity, although it can also be calculated using entropy . The calculation of a purity measure involves calculating the probability of an example of a given class being misclassified by a split. Calculating these probabilities involves summing the number of examples in each class within each group. . The splitting criterion can be updated to not only take the purity of the split into account, but also be weighted by the importance of each class. . This can be achieved by replacing the count of examples in each group by a weighted sum, where the coefficient is provided to weight the sum. . Larger weight is assigned to the class with more importance, and a smaller weight is assigned to a class with less importance. . Small Weight: Less importance, lower impact on node purity | Large Weight: More importance, higher impact on node purity | . A small weight can be assigned to the majority class, which has the effect of improving (lowering) the purity score of a node that may otherwise look less well sorted. In turn, this may allow more examples from the majority class to be classified for the minority class, better accommodating those examples in the minority class. . As such, this modification of the decision tree algorithm is referred to as a weighted decision tree, a class-weighted decision tree, or a cost-sensitive decision tree. . Modification of the split point calculation is the most common, although there has been a lot of research into a range of other modifications of the decision tree construction algorithm to better accommodate a class imbalance. . Weighted Decision Tree with Scikit-learn . The scikit-learn Python machine learning library provides an implementation of the decision tree algorithm that supports class weighting. . The DecisionTreeClassifier class provides the class_weight argument that can be specified as a model hyperparameter. The class_weight is a dictionary that defines each class label (e.g. 0 and 1) and the weighting to apply in the calculation of group pruity for splits in the decision tree when fitting the model . For example: . # define model weights = {0:1.0, 1:1.0} model = DecisionTreeClassifier(class_weight=weights) . The class weighting can be defined multiple ways, for example: . Domain Expertise: determined by talking to subject matter experts | Tuning: determined by hyperparameter search such as GirdSearch | Huristic: spcified using a general best practice | . A best practice for using the class weighting is to use the inverse of the class distribution present in the training dataset. . For example, the class distribution of the test dataset is a 1:100 ratio for the minority class to the majority class. The invert of this ratio could be used with 1 for the majority class and 100 for the minority class. . for example: . # define model weights = {0:1.0, 1:100.0} model = DecisionTreeClassifier(class_weight=weights) . We might also define the same ratio using fractions and achieve the same results . # define model weights = {0:0.01, 1:1.0} model = DecisionTreeClassifier(class_weight=weights) . Or also can be defined by pre-loaded attributes: python model = DecisionTreeClassifier(class_weight=&#39;balanced&#39;) . Detail about class_weight in DecisionTreeClassifier class . **class_weightdict, list of dict or “balanced”, default=None** . Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. . Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. . The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) . For multi-output, the weights of each column of y will be multiplied. . Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. . # decision tree with class weight on an imbalanced classification dataset model = DecisionTreeClassifier(class_weight=&#39;balanced&#39;) cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=&#39;roc_auc&#39;, cv=cv, n_jobs=-1) . # print out performance print(f&#39;The model ROC AUC mean is: {np.mean(scores):.3f}&#39;) . The model ROC AUC mean is: 0.749 . Ok, very well, the performance just a little bit imporved, from 0.734 to 0.749 . Grid Search Weighted Decision Tree . Using a class weighting that is the inverse ratio of the training data is just a heuristic. . It is possible that better performance can be achieved with a different class weighting, and this too will depend on the choice of performance metric used to evaluate the model. . In this section, we will grid search a range of different class weightings for the weighted decision tree and discover which results in the best ROC AUC score. . We will try the following weightings for class 0 and 1: . Class 0: 100, Class 1: 1 | Class 0: 10, Class 1: 1 | Class 0: 1, Class 1: 1 | Class 0: 1, Class 1: 10 | Class 0: 1, Class 1: 100 | . This can be defined as grid search parameter for the GridSearchCV class as follow: . # define grid balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] param_grid = dict(class_weight=balance) . # define grid balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] param_grid = dict(class_weight=balance) print(param_grid) . {&#39;class_weight&#39;: [{0: 100, 1: 1}, {0: 10, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 100}]} . from sklearn.model_selection import GridSearchCV # define evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define model model = DecisionTreeClassifier() # define grid search grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring=&#39;roc_auc&#39;, verbose=1) # fit the GridSearch grid_result = grid.fit(X, y) . Fitting 30 folds for each of 5 candidates, totalling 150 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 0.4s finished . # report the best configuration print(f&#39;Best {grid_result.best_score_} using {grid_result.best_params_}&#39;) . Best 0.750925925925926 using {&#39;class_weight&#39;: {0: 1, 1: 10}} . # report all configurations means = grid_result.cv_results_[&#39;mean_test_score&#39;] stds = grid_result.cv_results_[&#39;std_test_score&#39;] params = grid_result.cv_results_[&#39;params&#39;] for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) . 0.740556 (0.071391) with: {&#39;class_weight&#39;: {0: 100, 1: 1}} 0.735539 (0.070241) with: {&#39;class_weight&#39;: {0: 10, 1: 1}} 0.735707 (0.068985) with: {&#39;class_weight&#39;: {0: 1, 1: 1}} 0.750926 (0.071732) with: {&#39;class_weight&#39;: {0: 1, 1: 10}} 0.746212 (0.075781) with: {&#39;class_weight&#39;: {0: 1, 1: 100}} . Very well, fine turning the Class_weight hyperparameter, we have 0.01 performance imporve . Further Reading . Paper . An instance-weighting Method To Induce Cost-sensitive Tree, 2002 | . Books . Learning from Imbalanced Datasets, 2018 | Imbalanced Learning: Foundations, Algorithms, and Applications, 2013. | . APIs . sklearn.utils.class_weight.compute_class_weight | sklearn.tree.DecisionTreeClassifier | sklearn.model_selection.GridSearchCV | .",
            "url": "https://jl1829.github.io/turbo-funicular/jupyter/2020/02/26/Decision_Tree_Imbalance.html",
            "relUrl": "/jupyter/2020/02/26/Decision_Tree_Imbalance.html",
            "date": " • Feb 26, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jl1829.github.io/turbo-funicular/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jl1829.github.io/turbo-funicular/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi and welcome to my blog, I am Johnny.Lu, staying in the “Fine” city called Singapore(joke), currently I am the Lead Consulting Engineer(Data Science, Machine Learning)in Allied Telesis, a Japanese pioneer company in digital communication. . Experiences . Allied Telesis . Lead Consulting Engineerr, (Machine Learning, Data Science). Singapore, Jun/2017 – Current . Leading Data Science and System Engineer team to develop products that measurably and efficiently improve sales and top-line growth. | Interfacing with customers to receive valuable product feedback. | Driving strategy and vision for products by translating research, customer insights, and data discovery into innovative solutions for customers. | Actively collaborating with global IT, Architectures, Infrastructure, and Sales teams to deploy products. | Develop End-to-End Data Science, Machine Learning Project using: MySQL, Scikit-Learn, NumPy, Pandas, PySpark, TensorFlow 2, Keras | LightGBM, XGBoost, SpaCy, NLTK | . | . | Regional System Engineer Singapore, May/2016 – Jun/2017 . Response to business development, including leads generating, roadshow/seminar conducting, solution designing, quoting/bidding, solution deploying, and after sales servicing. | Promote Allied Telesis SDN, Service Cloud solution; initiative in the region, managing the technical team to provide high reliability services. | Managing key account, provide advisory of the option for new technology adoption. | . | . NETGEAR Inc. . Regional System Engineer Singapore, Jun/2013 – May/2016 Act as a constant performer, teaming with Regional Sales Director, perform annually business growth. | Provide consultation service to downstream partner &amp; end user, including basic infra network design and deployment, integration with virtualization, customized solution for individual customers. | . | Regional System Engineer Guangzhou, China, Nov/2012 – Oct/2013 Design, develop and carry out technical solutions | Establish and develop technical marketing objectives and goals | Analyze and interpret marketing trends concerning technical products or services | . | . Professional Certification . Machine Learning from Stanford University Online. Oct/2018 | Mathematics for Machine Learning: Linear Algebra, from Imperial College London. Feb/2019 | Mathematics for Machine Learning: Multivariate Calculus, from from Imperial College London. Mar/2019 | Neural Network and Deep Learning, from deeplearning.ai. Mar/2019 | Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization, from deeplearning.ai. Apr/2019 | Convolutional Neural Networks, from deeplearning.ai. May/2019 | Sequance Models, from deeplearning.ai. Jul/2019 | . Education Background . National University of Singapore . Master of Science - MS, Technology &amp; Intelligent System . Machine Reasoning | Reasoning Systems | Cognitive Systems | Problem Solving using Pattern Recognition | Intelligent Sensing and Sense Making | Pattern Recognition and Machine Learning Systems | Text Analytics | New Media and Sentiment Mining | Text Processing using Machine Learning | Conversational UIs | Vision Systems | Spatial Reasoning from Sensor Data | Real Time Audio-Visual Sensing and Sense Making | . Royal Holloway, University of London . Bachelor of Science (B.S.), Marketing/Marketing Management, General . MN2041K Managerial Accounting | MN2061K Marketing Management | MN2155K Asia Pacific Businesses | MN2165K The Global Economy | MN22201K Strategic Management | MN3215K Asia Pacific Multinationals in Europe | MN3455K Advertising And Promotion in Brand Marketing | MN3495K Clusters, Small Business and International Competition | MN3555K E-Commerce | MN3035K Marketing Research | MN3055K Consumer Behaviour | MN3301K Modern Business in Comparative Perspective | . Guangzhou Civil Aviation College . Associate’s degree, Electrical and Electronics Engineering . Further Mathematics | College English | Circuit Analysis | Analog Electronic Technology | Digital Electronic Technology | Single-chip Microcomputer Design &amp; Develop | The C Programming Language | Computer Network | Digital Communication Theory | Stored Program Control &amp; Mobile Communication Theory | .",
          "url": "https://jl1829.github.io/turbo-funicular/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jl1829.github.io/turbo-funicular/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}