{
  
    
        "post0": {
            "title": "Bayesian Optimazation Classification and Regression",
            "content": "import numpy as np import pandas as pd from sklearn.ensemble import RandomForestClassifier from sklearn.ensemble import RandomForestRegressor from bayes_opt import BayesianOptimization from sklearn.datasets import make_classification, make_regression from sklearn.model_selection import cross_val_score import warnings warnings.simplefilter(&#39;ignore&#39;) . Classification Problem Bayesian Optimazation . X, y = make_classification(n_samples=10000, n_features=10, n_classes=2) . We using the default hyperparameter to fit the data . rfc = RandomForestClassifier() np.mean(cross_val_score(rfc, X, y, cv=5, scoring=&#39;roc_auc&#39;)) . 0.9897763781114314 . Default hyperparameter perform aroudn 0.98 ROC_AUC, next step we use Bayesian Optimazation to fine turn the hyperparameter. . Define the blackBox function . def rfc_cv(n_estimators, min_samples_split, max_features, max_depth): val = np.mean(cross_val_score(RandomForestClassifier(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split), max_features=min(max_features, 0.999), max_depth=int(max_depth), random_state=42), X, y, scoring=&#39;roc_auc&#39;, cv=5)) return val . # define Bayesian Optimazation rfc_bo = BayesianOptimization( rfc_cv, {&#39;n_estimators&#39;: (10, 250), &#39;min_samples_split&#39;: (2, 25), &#39;max_features&#39;: (0.1, 0.999), &#39;max_depth&#39;: (5, 30)}) . # start the optimazation rfc_bo.maximize() . | iter | target | max_depth | max_fe... | min_sa... | n_esti... | - | 1 | 0.9904 | 18.08 | 0.3725 | 2.766 | 249.1 | | 2 | 0.9898 | 15.31 | 0.6883 | 24.87 | 84.16 | | 3 | 0.99 | 12.87 | 0.416 | 24.62 | 51.27 | | 4 | 0.9903 | 19.94 | 0.4735 | 5.652 | 244.5 | | 5 | 0.9895 | 25.96 | 0.9587 | 12.6 | 244.6 | | 6 | 0.9901 | 16.89 | 0.7025 | 2.421 | 244.6 | | 7 | 0.9904 | 16.8 | 0.4017 | 6.605 | 248.0 | | 8 | 0.9901 | 21.43 | 0.5085 | 6.751 | 249.4 | | 9 | 0.9901 | 11.14 | 0.6435 | 4.991 | 248.4 | | 10 | 0.9902 | 16.72 | 0.5588 | 9.387 | 242.5 | | 11 | 0.9897 | 13.73 | 0.7405 | 12.09 | 248.8 | | 12 | 0.9895 | 19.62 | 0.5513 | 4.749 | 237.9 | | 13 | 0.9901 | 23.44 | 0.4172 | 2.239 | 247.5 | | 14 | 0.99 | 5.734 | 0.5701 | 3.207 | 241.1 | | 15 | 0.9878 | 19.07 | 0.1093 | 4.803 | 246.5 | | 16 | 0.9896 | 17.28 | 0.7474 | 21.93 | 203.7 | | 17 | 0.9894 | 9.389 | 0.7191 | 7.547 | 34.61 | | 18 | 0.9904 | 24.09 | 0.5541 | 22.82 | 39.02 | | 19 | 0.9905 | 6.456 | 0.7869 | 22.56 | 17.81 | | 20 | 0.9896 | 22.42 | 0.3757 | 9.123 | 42.82 | | 21 | 0.9882 | 25.08 | 0.6047 | 4.696 | 19.99 | | 22 | 0.9898 | 18.56 | 0.2698 | 6.795 | 65.66 | | 23 | 0.9889 | 5.348 | 0.4807 | 19.61 | 69.79 | | 24 | 0.99 | 14.29 | 0.2061 | 13.03 | 224.5 | | 25 | 0.9902 | 10.75 | 0.4831 | 5.161 | 248.3 | | 26 | 0.9902 | 17.2 | 0.676 | 7.752 | 246.8 | | 27 | 0.9892 | 23.25 | 0.8133 | 22.96 | 39.82 | | 28 | 0.99 | 6.223 | 0.8502 | 22.09 | 17.42 | | 29 | 0.9904 | 15.85 | 0.4259 | 6.132 | 247.7 | | 30 | 0.9891 | 24.09 | 0.2956 | 23.08 | 38.13 | ========================================================================= . # check the best hyperparameter rfc_bo.max . {&#39;target&#39;: 0.9905380799798376, &#39;params&#39;: {&#39;max_depth&#39;: 6.456055231994655, &#39;max_features&#39;: 0.7869473158265811, &#39;min_samples_split&#39;: 22.558253615710782, &#39;n_estimators&#39;: 17.814015466174588}} . rfc_Optimazed = RandomForestClassifier(n_estimators=18, max_depth=6, max_features=0.78, min_samples_split=22) . np.mean(cross_val_score(rfc_Optimazed, X, y, cv=5, scoring=&#39;roc_auc&#39;)) . 0.9900614797906387 . Original roc_auc: 0.989776 | Optimized roc_auc: 0.99006 | . Regression Problem Bayesian Optimazation . X, y = make_regression(n_samples=10000, n_features=10) . rfe = RandomForestRegressor() np.mean(cross_val_score(rfe, X, y, cv=5, scoring=&#39;neg_mean_squared_error&#39;)) . -1409.2889528620326 . Define the blackbox function . def rfe_cv(n_estimators, min_samples_split, max_features, max_depth): val = np.mean(cross_val_score(RandomForestRegressor(n_estimators=int(n_estimators), min_samples_split=int(min_samples_split), max_features=min(max_features, 0.999), max_depth=int(max_depth), random_state=42), X, y, scoring=&#39;neg_mean_squared_error&#39;, cv=5)) return val . score = rfe_cv(n_estimators=100, min_samples_split=10, max_depth=6, max_features=0.78) score . # define Bayesian Optimazation rfe_bo = BayesianOptimization( rfe_cv, {&#39;n_estimators&#39;: (10, 250), &#39;min_samples_split&#39;: (2, 25), &#39;max_features&#39;: (0.1, 0.999), &#39;max_depth&#39;: (5, 30)}) . # start the optimazation rfe_bo.maximize() . | iter | target | max_depth | max_fe... | min_sa... | n_esti... | - | 1 | -2.702e+0 | 22.15 | 0.2902 | 20.4 | 211.9 | | 2 | -2.788e+0 | 25.96 | 0.2216 | 22.83 | 166.3 | | 3 | -1.651e+0 | 11.15 | 0.8612 | 10.86 | 153.0 | | 4 | -5.608e+0 | 5.331 | 0.4747 | 10.79 | 49.97 | | 5 | -1.862e+0 | 12.6 | 0.9883 | 21.26 | 124.8 | | 6 | -5.684e+0 | 5.0 | 0.999 | 2.0 | 250.0 | | 7 | -1.568e+0 | 12.54 | 0.9491 | 10.55 | 149.3 | | 8 | -4.212e+0 | 30.0 | 0.1 | 2.0 | 117.6 | | 9 | -5.682e+0 | 5.0 | 0.999 | 25.0 | 144.0 | | 10 | -1.443e+0 | 20.34 | 0.758 | 8.014 | 154.0 | | 11 | -1.412e+0 | 14.26 | 0.999 | 2.0 | 165.3 | | 12 | -1.399e+0 | 30.0 | 0.999 | 2.0 | 172.4 | | 13 | -4.4e+03 | 17.47 | 0.1 | 2.0 | 186.3 | | 14 | -1.402e+0 | 30.0 | 0.999 | 2.0 | 160.2 | | 15 | -4.4e+03 | 6.677 | 0.999 | 25.0 | 106.8 | | 16 | -5.683e+0 | 5.477 | 0.999 | 2.0 | 149.3 | | 17 | -1.443e+0 | 22.98 | 0.999 | 7.414 | 165.7 | | 18 | -1.635e+0 | 12.45 | 0.999 | 13.44 | 165.8 | | 19 | -1.654e+0 | 24.78 | 0.999 | 15.16 | 144.4 | | 20 | -1.864e+0 | 26.15 | 0.6263 | 23.02 | 129.2 | | 21 | -1.391e+0 | 28.7 | 0.8468 | 2.122 | 140.7 | | 22 | -1.46e+03 | 18.32 | 0.6642 | 9.14 | 133.6 | | 23 | -2.242e+0 | 30.0 | 0.999 | 25.0 | 10.0 | | 24 | -1.758e+0 | 30.0 | 0.999 | 4.534 | 10.0 | | 25 | -1.859e+0 | 14.99 | 0.5897 | 10.89 | 10.73 | | 26 | -5.012e+0 | 27.24 | 0.1 | 11.51 | 23.43 | | 27 | -5.892e+0 | 5.0 | 0.999 | 2.0 | 10.0 | | 28 | -1.568e+0 | 28.81 | 0.9138 | 12.15 | 134.3 | | 29 | -2.17e+03 | 15.28 | 0.6976 | 23.56 | 10.48 | | 30 | -6.18e+03 | 5.125 | 0.352 | 23.98 | 176.2 | ========================================================================= . rfe_bo.max . {&#39;target&#39;: -1390.7849548765093, &#39;params&#39;: {&#39;max_depth&#39;: 28.70255259053527, &#39;max_features&#39;: 0.8468279746142502, &#39;min_samples_split&#39;: 2.1219418980976834, &#39;n_estimators&#39;: 140.748505191585}} . # use the best hyperparameter rfe = RandomForestRegressor(n_estimators=140, max_depth=29, max_features=0.84, min_samples_split=2) . np.mean(cross_val_score(rfe, X, y, cv=5, scoring=&#39;neg_mean_squared_error&#39;)) . -1383.4479089516929 . Origin neg_mean_squared_error: -1409.2889528620326 | Optimazed neg_mean_squared_error: -1383.4479089516929 | .",
            "url": "https://jl1829.github.io/turbo-funicular/bayesian/2020/07/20/Bayesian-Optimazation-Classification-and-Regression.html",
            "relUrl": "/bayesian/2020/07/20/Bayesian-Optimazation-Classification-and-Regression.html",
            "date": " • Jul 20, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Data Science Project imporvement, using LightGBM to gain more accuracy and no need to One-Hot Encoding",
            "content": "Overview . We have done the last Data Science Project with U.S. Adult Income Dataset with around 86% model accuracy, which can go into the top level accuracy in Kaggle competition. we have done the following steps: . Understand the business problem. | EDA(Exploratory Data Analysis): Look through and investigate the overall dataset, visualize it with matplotlib and finding any missing value and outliers. | Data cleaning: impute the missing and outliers value. | Baseline model: Dummy classifier gave us 75% accuracy as baseline, meaning that anything below 75% accuracy, the model do nothing better than flipping a coin, and above this value, the model have some skill to classify the labels. | Model evaluate and fine turn: we have evaluate Support Vector Machine; RamdonForestClassifier; BaggingClassifier; GradientBoostingClassifier and Neural Network, the best performance model is GradientBoostingClassifier which providing 86% of accuracy. | Today we are going to use another light weight, powerful, and fast algorithem: lightGBM, open source at 2017, and now maintainced by Microsoft . As the EDA was already done by the last blog, we will just skip it and move directly into today&#39;s topic. . Get the imports done and read the dataset . import numpy as np import pandas as pd import matplotlib.pyplot as plt import lightgbm as lgb import warnings warnings.filterwarnings(&#39;ignore&#39;) . df = pd.read_csv(&#39;datasets/adult.csv&#39;, na_values=&quot;?&quot;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null int64 1 workclass 46043 non-null object 2 fnlwgt 48842 non-null int64 3 education 48842 non-null object 4 educational-num 48842 non-null int64 5 marital-status 48842 non-null object 6 occupation 46033 non-null object 7 relationship 48842 non-null object 8 race 48842 non-null object 9 gender 48842 non-null object 10 capital-gain 48842 non-null int64 11 capital-loss 48842 non-null int64 12 hours-per-week 48842 non-null int64 13 native-country 47985 non-null object 14 income 48842 non-null object dtypes: int64(6), object(9) memory usage: 5.6+ MB . Data Cleaning part: Scaled the numerical column, and Label encoding the binary categorical column . The target column income need to be encoded as 1 and 0 | As well as the gender column | . import seaborn as sns from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import MinMaxScaler scaler = MinMaxScaler() num_col = df.select_dtypes(exclude=[&#39;object&#39;, &#39;datetime&#39;]).columns df[num_col] = scaler.fit_transform(df[num_col]) . le = LabelEncoder() df[&#39;gender&#39;] = le.fit_transform(df[&#39;gender&#39;]) df[&#39;income&#39;] = le.fit_transform(df[&#39;income&#39;]) . df . age workclass fnlwgt education educational-num marital-status occupation relationship race gender capital-gain capital-loss hours-per-week native-country income . 0 0.109589 | Private | 0.145129 | 11th | 0.400000 | Never-married | Machine-op-inspct | Own-child | Black | 1 | 0.000000 | 0.0 | 0.397959 | United-States | 0 | . 1 0.287671 | Private | 0.052451 | HS-grad | 0.533333 | Married-civ-spouse | Farming-fishing | Husband | White | 1 | 0.000000 | 0.0 | 0.500000 | United-States | 0 | . 2 0.150685 | Local-gov | 0.219649 | Assoc-acdm | 0.733333 | Married-civ-spouse | Protective-serv | Husband | White | 1 | 0.000000 | 0.0 | 0.397959 | United-States | 1 | . 3 0.369863 | Private | 0.100153 | Some-college | 0.600000 | Married-civ-spouse | Machine-op-inspct | Husband | Black | 1 | 0.076881 | 0.0 | 0.397959 | United-States | 1 | . 4 0.013699 | NaN | 0.061708 | Some-college | 0.600000 | Never-married | NaN | Own-child | White | 0 | 0.000000 | 0.0 | 0.295918 | United-States | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 48837 0.136986 | Private | 0.165763 | Assoc-acdm | 0.733333 | Married-civ-spouse | Tech-support | Wife | White | 0 | 0.000000 | 0.0 | 0.377551 | United-States | 0 | . 48838 0.315068 | Private | 0.096129 | HS-grad | 0.533333 | Married-civ-spouse | Machine-op-inspct | Husband | White | 1 | 0.000000 | 0.0 | 0.397959 | United-States | 1 | . 48839 0.561644 | Private | 0.094462 | HS-grad | 0.533333 | Widowed | Adm-clerical | Unmarried | White | 0 | 0.000000 | 0.0 | 0.397959 | United-States | 0 | . 48840 0.068493 | Private | 0.128004 | HS-grad | 0.533333 | Never-married | Adm-clerical | Own-child | White | 1 | 0.000000 | 0.0 | 0.193878 | United-States | 0 | . 48841 0.479452 | Self-emp-inc | 0.186482 | HS-grad | 0.533333 | Married-civ-spouse | Exec-managerial | Wife | White | 0 | 0.150242 | 0.0 | 0.397959 | United-States | 1 | . 48842 rows × 15 columns . Data Cleaning Part: Impute the missing value . The missing value are all fall into categorical features . df.isnull().sum() . age 0 workclass 2799 fnlwgt 0 education 0 educational-num 0 marital-status 0 occupation 2809 relationship 0 race 0 gender 0 capital-gain 0 capital-loss 0 hours-per-week 0 native-country 857 income 0 dtype: int64 . Impute the missing value with the most frequent value . df = df.fillna(df.mode().iloc[0]) . df.isnull().sum() . age 0 workclass 0 fnlwgt 0 education 0 educational-num 0 marital-status 0 occupation 0 relationship 0 race 0 gender 0 capital-gain 0 capital-loss 0 hours-per-week 0 native-country 0 income 0 dtype: int64 . Data Cleaning Part: Convert the object Data type into category Data type . LightGBM can handle the category feature by itself, but before that, we need to convert the object dtype to category dtype, so that LightGBM can handle it. . for column in df.columns: if df[column].dtype == &#39;object&#39;: df[column] = df[column].astype(&#39;category&#39;) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null float64 1 workclass 48842 non-null category 2 fnlwgt 48842 non-null float64 3 education 48842 non-null category 4 educational-num 48842 non-null float64 5 marital-status 48842 non-null category 6 occupation 48842 non-null category 7 relationship 48842 non-null category 8 race 48842 non-null category 9 gender 48842 non-null int64 10 capital-gain 48842 non-null float64 11 capital-loss 48842 non-null float64 12 hours-per-week 48842 non-null float64 13 native-country 48842 non-null category 14 income 48842 non-null int64 dtypes: category(7), float64(6), int64(2) memory usage: 3.3 MB . Modeling Part . X = df.drop(&#39;income&#39;, axis=1) y = df[&#39;income&#39;] . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y) . clf = lgb.LGBMClassifier(objective=&#39;binary&#39;, silent=False, colsample_bytree=0.9, subsample=0.9, learning_rate=0.05) . fit_params = { &#39;early_stopping_rounds&#39;: 10, &#39;eval_metric&#39;: &#39;accuracy&#39;, &#39;eval_set&#39;: [(X_test, y_test)], &#39;eval_names&#39;: [&#39;valid&#39;], &#39;verbose&#39;: 100, &#39;feature_name&#39;: &#39;auto&#39;, # actually this is default &#39;categorical_feature&#39;: &#39;auto&#39; # actually this is default } . clf.fit(X_train, y_train, **fit_params) . Training until validation scores don&#39;t improve for 10 rounds [100] valid&#39;s binary_logloss: 0.2779 Did not meet early stopping. Best iteration is: [100] valid&#39;s binary_logloss: 0.2779 . LGBMClassifier(boosting_type=&#39;gbdt&#39;, class_weight=None, colsample_bytree=0.9, importance_type=&#39;split&#39;, learning_rate=0.05, max_depth=-1, min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0, n_estimators=100, n_jobs=-1, num_leaves=31, objective=&#39;binary&#39;, random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=False, subsample=0.9, subsample_for_bin=200000, subsample_freq=0) . print(f&quot;The Model Accuracy: {(clf.score(X_test, y_test)*100):.2f}%&quot;) . The Model Accuracy: 87.53% . Accuracy imporvement . Compare to last blog, the best performing model: GradientBoostingClassifier have achieved around 86% of Accuracy, here using LightGBM, without One-Hot Encoding the categorical feature, it have around 1% of Accuracy improving. . %matplotlib inline feat_imp = pd.Series(clf.feature_importances_, index=X.columns) feat_imp.nlargest(30).plot(kind=&#39;barh&#39;, figsize=(8,10)) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x127c9ac10&gt; . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Feature Importance . LightGBM has built-in Feature Importance examination, it shows clearly in the plot, the age and capital-gain feature is the most important features that impact the income target. .",
            "url": "https://jl1829.github.io/turbo-funicular/lightgbm/2020/04/15/Data-Science-Project-imporvement,-using-LightGBM-to-gain-more-accuracy.html",
            "relUrl": "/lightgbm/2020/04/15/Data-Science-Project-imporvement,-using-LightGBM-to-gain-more-accuracy.html",
            "date": " • Apr 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "End-to-End Data Science Project with Adult Income Dataset",
            "content": "Overview . Recently I got a freelance Data Science Job from a LinkedIn member. It&#39;s typical binary classification task with applicant basic information from an online commerical startup. The business onwer consult me about building a simpel Machine Learning model to provide an automated account opening checking system, and gave me a small dataset(around 60,000 instances). I would like to share some of my experiences of this Data Science project, of course I won&#39;t be able to share the excat dataset and it&#39;s detail , but I found out the U.S. Adult Income Dataset could be one candidate to share the similarity, so I pick it as a sharing sample. . About U.S. Adult Income Dataset . Abstract: Based on every observation&#39;s attributes, predict whether a instance aka a person, income exceeds US$50,000 annually. Also known as &quot;Census Income&quot; dataset, it&#39;s originally donated by Ronny Kohavi and Barry Becker at 1996. It&#39;s Classification Task, with categorical and numerical features, some of the instance have missing values, the missing value was denoted as &quot;?&quot; . Total have 14 columns of features, the last column: income is the classification label: &gt;50k, &lt;=50k, other features as below: . age: numerical | workclass: categorical | fnlwgt: numerical | education: categorical | education-num: numerical | marital-status: categorical | occupation: categorical | relationship: categorical | race: categorical | sex: categorical | capital-gain: numerical | capital-loss: numerical | hours-per-week: numerical | native-country: categorical | . And one more things need to take note is the dataset is imbalanced, there are two classes values: &gt;50k and &lt;=50k. . &gt;50k: miniority class, around 25% | &lt;=50k: majority class, around 75% | . This dataset is openly accessable, either visit: UCI(University of California, Irvine) Machine Learning Repository, or Kaggle Adult Dataset. . Typical Data Science workflow . Before we get our hand dirty, let&#39;s understand our workflow so that we can follow the steps, normally when we receive a Data Science job, no matter is from which sector, it could be Financial, consumer, computer network, manufacturing, we need to have a proper understanding of the problem we are going to solve. An article from Towardsdatascience have a very good explaination: . . For most of the project, it will fall into the flows: . Understand the Business Problem: . In here, the problem is to predict whether a person&#39;s income is exceeded US$50,000.00 based on the features, in commercial, it could be predict a person will place order on particular products based on his/her browsing behavior | . | EDA(Exploratory Data Analysis): . Not all the data science project dataset is perfectly clean like the professior gave you in academic, real word dataset that the customer gave you will be very &quot;dirty&quot;, it contain lot&#39;s of outlier, missing value, or others intentionly wrong filling. We need to identify it and procss to next steps | . | Data Cleaning: . Following previous step, once we have identified the outlier, missing values, we will &quot;clean&quot; it via statistical method or others method. | . | Feature Engineering: . Well, this is one of the most time and brain power consuming steps, it mean we need to figure out each feature&#39;s corrlation between the label, selecting, or extracting the most relevent feature to feed into the model, it&#39;s very important step that help us to fight with the Curse of Dimensionality and the under-fit/over-fit problem. | . | Baseline Model Result: . Some of the Data Scientist will miss out this part, most of the time they will just simple feed the &quot;cleaned&quot; dataset into the model and get a simply 75% accuracy, well, in Adult dataset, I can say I can throw any dummy classification model it will get 75% accuracy, as it&#39;s imbalanced with 75% majority class, in order to understand how well the model is working, we must have some baseline data, so this is the part we figure the baseline. | . | Model Evaluate and Fine Turn: . How good is the model, can we improve more? here we fine turn the hyperparameter to make it closer to production level. | . | Iteration: . Present the work to customer, put the model into production, get feedback, imporve. | . | . Exploratory Data Analysis (EDA) . Let&#39;s get into Adult Dataset . Move to Dataset Kaggle Site to download it. Or we can use scikit-learn fetch_openml to fetch it. . # import import numpy as np import pandas as pd from collections import Counter # load dataset adult_df = pd.read_csv(&#39;/storage/adult.csv&#39;, na_values=&#39;?&#39;) # overview of the dataset print(adult_df.info()) print(&quot; n&quot;) print(adult_df.head()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null int64 1 workclass 46043 non-null object 2 fnlwgt 48842 non-null int64 3 education 48842 non-null object 4 educational-num 48842 non-null int64 5 marital-status 48842 non-null object 6 occupation 46033 non-null object 7 relationship 48842 non-null object 8 race 48842 non-null object 9 gender 48842 non-null object 10 capital-gain 48842 non-null int64 11 capital-loss 48842 non-null int64 12 hours-per-week 48842 non-null int64 13 native-country 47985 non-null object 14 income 48842 non-null object dtypes: int64(6), object(9) memory usage: 5.6+ MB None age workclass fnlwgt education educational-num marital-status 0 25 Private 226802 11th 7 Never-married 1 38 Private 89814 HS-grad 9 Married-civ-spouse 2 28 Local-gov 336951 Assoc-acdm 12 Married-civ-spouse 3 44 Private 160323 Some-college 10 Married-civ-spouse 4 18 NaN 103497 Some-college 10 Never-married occupation relationship race gender capital-gain capital-loss 0 Machine-op-inspct Own-child Black Male 0 0 1 Farming-fishing Husband White Male 0 0 2 Protective-serv Husband White Male 0 0 3 Machine-op-inspct Husband Black Male 7688 0 4 NaN Own-child White Female 0 0 hours-per-week native-country income 0 40 United-States &lt;=50K 1 50 United-States &lt;=50K 2 40 United-States &gt;50K 3 40 United-States &gt;50K 4 30 United-States &lt;=50K . # check the missing value print(&quot;Checking dataframe missing values: n&quot;) for column in adult_df.columns: if adult_df[column].isnull().sum() != 0: missingValue = adult_df[column].isnull().sum() percentage = missingValue / len(adult_df[column]) * 100 dtype = adult_df[column].dtype print(f&quot;The column: &#39;{column}&#39; with Data Type: &#39;{dtype}&#39; has missing value: {missingValue}, percentage: {percentage:.2f}%&quot;) # memory cleaning del missingValue del percentage del dtype . Checking dataframe missing values: The column: &#39;workclass&#39; with Data Type: &#39;object&#39; has missing value: 2799, percentage: 5.73% The column: &#39;occupation&#39; with Data Type: &#39;object&#39; has missing value: 2809, percentage: 5.75% The column: &#39;native-country&#39; with Data Type: &#39;object&#39; has missing value: 857, percentage: 1.75% . Well, that not too much, about how to handle the missing value, is a balance game, either throw it away, if there&#39;s not too much impact to the model performance, or impute it with some strategy, such as most-frquent since they are all categorical value, or mean, median etc if they are numerical value. In this case, as the missing value fall into the categorical features, we will use the pandas DataFrame mode() method to fill the missing value . # check the label class and it&#39;s distribution percentage label = adult_df.values[:, -1] counter = Counter(label) for key, value in counter.items(): percentage = value / len(label) * 100 print(f&quot;Class: {key}, Count = {value}, Percentage = {percentage:.1f}%.&quot;) . Class: &lt;=50K, Count = 37155, Percentage = 76.1%. Class: &gt;50K, Count = 11687, Percentage = 23.9%. . numerical_subset = adult_df.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]) . print(numerical_subset) . age fnlwgt educational-num capital-gain capital-loss 0 25 226802 7 0 0 1 38 89814 9 0 0 2 28 336951 12 0 0 3 44 160323 10 7688 0 4 18 103497 10 0 0 ... ... ... ... ... ... 48837 27 257302 12 0 0 48838 40 154374 9 0 0 48839 58 151910 9 0 0 48840 22 201490 9 0 0 48841 52 287927 9 15024 0 hours-per-week 0 40 1 50 2 40 3 40 4 30 ... ... 48837 38 48838 40 48839 40 48840 20 48841 40 [48842 rows x 6 columns] . import matplotlib.pyplot as plt numerical_subset.hist(bins=20, figsize=(20, 15)) plt.show() . Data Cleaning . After we have explored the overview of the dataset, we are going to dig out any anomaly in the dataset and clean it out. . # use DataFrame `mode()` method adult_df = adult_df.fillna(adult_df.mode().iloc[0]) print(adult_df.info()) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 48842 entries, 0 to 48841 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 age 48842 non-null int64 1 workclass 48842 non-null object 2 fnlwgt 48842 non-null int64 3 education 48842 non-null object 4 educational-num 48842 non-null int64 5 marital-status 48842 non-null object 6 occupation 48842 non-null object 7 relationship 48842 non-null object 8 race 48842 non-null object 9 gender 48842 non-null object 10 capital-gain 48842 non-null int64 11 capital-loss 48842 non-null int64 12 hours-per-week 48842 non-null int64 13 native-country 48842 non-null object 14 income 48842 non-null object dtypes: int64(6), object(9) memory usage: 5.6+ MB None . The Non-Null Count shows there&#39;s no more missing value from the dataset, it filled by the most_frequent value . # checking the outliers print(numerical_subset.describe()) . age fnlwgt educational-num capital-gain count 48842.000000 4.884200e+04 48842.000000 48842.000000 mean 38.643585 1.896641e+05 10.078089 1079.067626 std 13.710510 1.056040e+05 2.570973 7452.019058 min 17.000000 1.228500e+04 1.000000 0.000000 25% 28.000000 1.175505e+05 9.000000 0.000000 50% 37.000000 1.781445e+05 10.000000 0.000000 75% 48.000000 2.376420e+05 12.000000 0.000000 max 90.000000 1.490400e+06 16.000000 99999.000000 capital-loss hours-per-week count 48842.000000 48842.000000 mean 87.502314 40.422382 std 403.004552 12.391444 min 0.000000 1.000000 25% 0.000000 40.000000 50% 0.000000 40.000000 75% 0.000000 45.000000 max 4356.000000 99.000000 . Well, from the column capital-gain, the maximum value is 99999 which seems like little bit werid, we can check the dataset description, the capital-gain means the additional income from capital market, such as stocks, securities, 99999 indicated somebody wrongly imput or it repersented as None additional income, here we can try to replace it with the mean value, and we will replace the 99 hours in the hours-per-week column also . # check the quantity of this outlier print(f&quot;There&#39;s {adult_df[adult_df[&#39;capital-gain&#39;] == 99999].shape[0]} outlier in the capital-gain column&quot;) print(f&quot;There&#39;s {adult_df[adult_df[&#39;hours-per-week&#39;] == 99].shape[0]} outlier in the hours-per-week column&quot;) . There&#39;s 244 outlier in the capital-gain column There&#39;s 137 outlier in the hours-per-week column . # replace it with mean value adult_df[&#39;capital-gain&#39;].replace(99999, np.mean(adult_df[&#39;capital-gain&#39;].values), inplace=True) adult_df[&#39;hours-per-week&#39;].replace(99, np.mean(adult_df[&#39;hours-per-week&#39;].values), inplace=True) print(adult_df.describe()) . age fnlwgt educational-num capital-gain count 48842.000000 4.884200e+04 48842.000000 48842.000000 mean 38.643585 1.896641e+05 10.078089 584.893278 std 13.710510 1.056040e+05 2.570973 2530.549506 min 17.000000 1.228500e+04 1.000000 0.000000 25% 28.000000 1.175505e+05 9.000000 0.000000 50% 37.000000 1.781445e+05 10.000000 0.000000 75% 48.000000 2.376420e+05 12.000000 0.000000 max 90.000000 1.490400e+06 16.000000 41310.000000 capital-loss hours-per-week count 48842.000000 48842.000000 mean 87.502314 40.258074 std 403.004552 11.995662 min 0.000000 1.000000 25% 0.000000 40.000000 50% 0.000000 40.000000 75% 0.000000 45.000000 max 4356.000000 98.000000 . After the data exploration and cleaning, we save the cleaned DataFrame to adult_cleaned.csv file . adult_df.to_csv(&#39;/storage/adult_cleaned.csv&#39;, index=False) . Baseline Model Result . We will evaluate candidate models using repeated stratified k-fold cross-validation . The k-fold cross-validation procedure provides a good general estimate of model performance that is not too optimistically biased, at least compared to a single train-test split. We will use k=10, meaning each fold will contain about 45,222/10, or about 4,522 examples. . Stratified means that each fold will contain the same mixture of examples by class, that is about 75% to 25% for the majority and minority classes respectively. Repeated means that the evaluation process will be performed multiple times to help avoid fluke results and better capture the variance of the chosen model. We will use three repeats. . This means a single model will be fit and evaluated 10 * 3 or 30 times and the mean and standard deviation of these runs will be reported. . This can be achieved using the RepeatedStratifiedKFold scikit-learn class. . We will predict a class label for each example and measure model performance using classification accuracy. . The evaluate_model() function below will take the loaded dataset and a defined model and will evaluate it using repeated stratified k-fold cross-validation, then return a list of accuracy scores that can later be summarized. . import pandas as pd import numpy as np from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.compose import ColumnTransformer from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold def load_dataset(filename): df = pd.read_csv(filename) X, y = df.iloc[:, :-1], df.iloc[:, -1] cate_index = X.select_dtypes(include=[&#39;object&#39;]).columns num_index = X.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns y = LabelEncoder().fit_transform(y) return X, y, cate_index, num_index def evaluate_model(X, y, model): cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42) scores = cross_val_score(model, X, y, scoring=&#39;accuracy&#39;, cv=cv, n_jobs=-1) return scores . from sklearn.dummy import DummyClassifier X, y, cate_index, num_index = load_dataset(&#39;storage/adult_cleaned.csv&#39;) model = DummyClassifier(strategy=&#39;most_frequent&#39;) . scores = evaluate_model(X, y, model) print(scores) . [0.76069601 0.76069601 0.76085176 0.76085176 0.76085176 0.76064701 0.76064701 0.76064701 0.76064701 0.76064701 0.76069601 0.76069601 0.76085176 0.76085176 0.76085176 0.76064701 0.76064701 0.76064701 0.76064701 0.76064701 0.76069601 0.76069601 0.76085176 0.76085176 0.76085176 0.76064701 0.76064701 0.76064701 0.76064701 0.76064701] . print(f&quot;The Dummy Classifier mean accuracy: {(np.mean(scores)*100):.2f}%, with Standard Deviation: {np.std(scores):.2f}&quot;) . The Dummy Classifier mean accuracy: 76.07%, with Standard Deviation: 0.00 . print(f&quot;The type of dataset: {type(X)}.&quot;) print(f&quot;The shape of the dataset: Row: {X.shape[0]}, with {X.shape[1]} fetures&quot;) print(f&quot;The type of the target label: {type(y)}&quot;) print(f&quot;The shape of the target label is: {y.shape[0]} dimensional vector.&quot;) . The type of dataset: &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;. The shape of the dataset: Row: 48842, with 14 fetures The type of the target label: &lt;class &#39;numpy.ndarray&#39;&gt; The shape of the target label is: 48842 dimensional vector. . Now that we have a test harness and a baseline in performance. In this case, we can see that the baseline algorithm achieves an accuracy of about 76.07%. This score provides a lower limit on model skill; any model that achieves an average accuracy above about 76.07% has skill, whereas models that achieve a score below this value do not have skill on this dataset. Now we can begin to evaluate some models on this dataset . Model Evaluate and Fine Turn . Evaluate Machine Learning Algorithms . Let’s start by evaluating a mixture of machine learning models on the dataset. . It can be a good idea to spot check a suite of different nonlinear algorithms on a dataset to quickly flush out what works well and deserves further attention, and what doesn’t. . We will evaluate the following machine learning models on the adult dataset: . Decision Tree (CART) | Support Vector Machine (SVM) | Bagged Decision Trees (BAG) | Random Forest (RF) | Gradient Boosting Machine (GBM) | . We will use mostly default model hyperparameters, with the exception of the number of trees in the ensemble algorithms, which we will set to a reasonable default of 100. . We will define each model in turn and add them to a list so that we can evaluate them sequentially. The generate_models() function below defines the list of models for evaluation, as well as a list of model short names for plotting the results later. . import numpy as np import pandas as pd from sklearn.preprocessing import LabelEncoder from sklearn.preprocessing import OneHotEncoder from sklearn.preprocessing import MinMaxScaler from sklearn.pipeline import Pipeline from sklearn.model_selection import cross_val_score from sklearn.model_selection import train_test_split from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.ensemble import GradientBoostingClassifier from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from sklearn.neural_network import MLPClassifier from sklearn.svm import SVC from sklearn.ensemble import BaggingClassifier def load_dataset(filename): df = pd.read_csv(filename) X, y = df.iloc[:, :-1], df.iloc[:, -1] cate_index = X.select_dtypes(include=[&#39;object&#39;]).columns num_index = X.select_dtypes(include=[&#39;int64&#39;, &#39;float64&#39;]).columns y = LabelEncoder().fit_transform(y) return X, y, cate_index, num_index X, y, cate_index, num_index = load_dataset(&#39;/storage/adult_cleaned.csv&#39;) print(type(X)) print(X.shape) print(type(y)) print(y.shape) . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; (48842, 14) &lt;class &#39;numpy.ndarray&#39;&gt; (48842,) . def generate_models(): models, names = [], [] names.append(&#39;CART&#39;) models.append(DecisionTreeClassifier()) names.append(&#39;SVM&#39;) models.append(SVC(gamma=&#39;scale&#39;)) names.append(&#39;BAG&#39;) models.append(BaggingClassifier(n_estimators=100)) names.append(&#39;RF&#39;) models.append(RandomForestClassifier(n_estimators=100)) names.append(&#39;GBM&#39;) models.append(GradientBoostingClassifier(n_estimators=100)) names.append(&#39;Neural Network&#39;) models.append(MLPClassifier(early_stopping=True)) return models, names models, names = generate_models() . As now the X array still in pandas DataFrame with categorical values, here we need to &quot;encoding&quot; the categorical values into numerical values, OneHotEncoder with Scikit-Learn Pipeline are quite handy . steps = [(&#39;Categorical&#39;, OneHotEncoder(handle_unknown=&#39;ignore&#39;), cate_index), (&#39;Numerical&#39;, MinMaxScaler(), num_index)] from sklearn.compose import ColumnTransformer transformer = ColumnTransformer(steps, verbose=True) X = transformer.fit_transform(X) print(type(X)) print(X.shape) . [ColumnTransformer] ... (1 of 2) Processing Categorical, total= 0.1s [ColumnTransformer] ..... (2 of 2) Processing Numerical, total= 0.0s &lt;class &#39;scipy.sparse.csr.csr_matrix&#39;&gt; (48842, 105) . from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y) print(X_train.shape) print(X_test.shape) print(y_train.shape) print(y_test.shape) . (34189, 105) (14653, 105) (34189,) (14653,) . # filter unwanted warning import warnings warnings.filterwarnings(&#39;ignore&#39;) . # evaluate each model in default parameters for i in range(len(models)): print(f&quot;&quot;&quot; ******************************** Now evaluating {names[i]} model ******************************** n&quot;&quot;&quot;) scores = evaluate_model(X_train, y_train, models[i]) print(f&quot;The {names[i]} model average accuracy is: {(np.mean(scores)*100):.2f}%, with Standard Deviation: {(np.std(scores)*100):.2f}.&quot;) . ******************************** Now evaluating CART model ******************************** The CART model average accuracy is: 81.20%, with Standard Deviation: 0.70. ******************************** Now evaluating SVM model ******************************** The SVM model average accuracy is: 84.61%, with Standard Deviation: 0.61. ******************************** Now evaluating BAG model ******************************** The BAG model average accuracy is: 85.24%, with Standard Deviation: 0.54. ******************************** Now evaluating RF model ******************************** The RF model average accuracy is: 84.99%, with Standard Deviation: 0.58. ******************************** Now evaluating GBM model ******************************** The GBM model average accuracy is: 86.31%, with Standard Deviation: 0.47. ******************************** Now evaluating Neural Network model ******************************** The Neural Network model average accuracy is: 85.00%, with Standard Deviation: 0.58. . In this case, we can see that all of the chosen algorithms are skillful, achieving a classification accuracy above 76.07%. We can see that the ensemble decision tree algorithms perform the best with perhaps stochastic gradient boosting performing the best with a classification accuracy of about 86.3%. . This accuracy is using the default Hyperperameter, we can pick two top performance algorithms to use scikit-learn GridSearch() to fine turn the Hyperperameter to see whether it can get better performance. . The best two performance algorithms: . BaggingClassfier(n_estimators=100) | GradientBoostingClassfier(n_estimators=100) | . We can try to fine turn this two model. . # fine turn BaggingClassifier from sklearn.model_selection import GridSearchCV BAGgrid = {&#39;n_estimators&#39;: [100, 200]} cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=42) BAGclf = BaggingClassifier() BAGgrid_search = GridSearchCV(estimator=BAGclf, param_grid=BAGgrid, n_jobs=-1, cv=cv, scoring=&#39;accuracy&#39;, error_score=0) BAGgrid_result = BAGgrid_search.fit(X_train, y_train) . print(BAGgrid_result.best_score_) print(BAGgrid_result.best_params_) . 0.8518822471349845 {&#39;n_estimators&#39;: 200} . # fine turn GradientBoostingClassifier GBMgrid = {&#39;n_estimators&#39;: [100, 200]} GBMclf = GradientBoostingClassifier(learning_rate=0.1, min_samples_split=500, min_samples_leaf=50, max_depth=8, max_features=&#39;sqrt&#39;, subsample=0.8, random_state=42) GBMgrid_search = GridSearchCV(estimator=GBMclf, param_grid=GBMgrid, n_jobs=-1, cv=cv, scoring=&#39;accuracy&#39;, error_score=0) GBMgrid_result = GBMgrid_search.fit(X_train, y_train) . print(GBMgrid_result.best_score_) print(GBMgrid_result.best_params_) . 0.8669747643525415 {&#39;n_estimators&#39;: 200} . Well, seems like if the n_estimators is equal to 200, the GradientBoostingClassifier performance incrase to 86.69%, then we can update our hyperparameter for GradientBoostingClassfier and train it according to our Training Subset, now we have the winner, is GradientBoostingClassifier algorithm. . Actually using GridSearchCV is quite computational expensive, I would suggest to use Cloud Notebook Envirnoment, such Google Colab, AWS, or Google Cloud or Gradient, or Kaggle, both of them provide quite power CPU and tons of memory, and most important, they provide free GPU in certain amount of time . Final: Train the best model, save it, and deliver to customer. . We pick GradientBoostingClassifier as our final model, we will train it with Training Subset, and see how it goes in Testing Subset, then finally we will save it and deliver to our customer. . model = GradientBoostingClassifier(n_estimators=200, learning_rate=0.1, min_samples_split=500, min_samples_leaf=50, max_depth=8, max_features=&#39;sqrt&#39;, subsample=0.8, random_state=42) model.fit(X_train, y_train) . GradientBoostingClassifier(ccp_alpha=0.0, criterion=&#39;friedman_mse&#39;, init=None, learning_rate=0.1, loss=&#39;deviance&#39;, max_depth=8, max_features=&#39;sqrt&#39;, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=50, min_samples_split=500, min_weight_fraction_leaf=0.0, n_estimators=200, n_iter_no_change=None, presort=&#39;deprecated&#39;, random_state=42, subsample=0.8, tol=0.0001, validation_fraction=0.1, verbose=0, warm_start=False) . # evaluate the testing subset TestScore = model.score(X_test, y_test) print(f&quot;The model test set accuracy is: {(TestScore*100):.1f}%.&quot;) . The model test set accuracy is: 87.4%. . # perform the Classification Report from sklearn.metrics import classification_report predicted = model.predict(X_test) print(classification_report(y_test, predicted)) . precision recall f1-score support 0 0.90 0.94 0.92 11147 1 0.79 0.65 0.71 3506 accuracy 0.87 14653 macro avg 0.84 0.80 0.82 14653 weighted avg 0.87 0.87 0.87 14653 . # to save the model import joblib joblib.dump(model, &#39;storage/final_model.sav&#39;) . [&#39;storage/final_model.sav&#39;] . Further Reading . Paper . Scaling Up The Accuracy of Naive-bayes Classifiers: A Decision-tree Hybrid, 1996 | . APIs . pandas.DataFrame.select_dtypes API | sklearn.model_selection.RepeatedStratifiedKFold API | sklearn.dummy.DummyClassifier API | pandas.DataFrame.mode() API | .",
            "url": "https://jl1829.github.io/turbo-funicular/machine%20learning/2020/03/24/End-to-End-Data-Science-Project-with-Adult-Income-Dataset.html",
            "relUrl": "/machine%20learning/2020/03/24/End-to-End-Data-Science-Project-with-Adult-Income-Dataset.html",
            "date": " • Mar 24, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "What does auc stand for and what is it?",
            "content": "What does AUC stand for and what is it? . For classification machine learning model, it’s unwise to use Accuracy as single measurement for model performance, most of the time, we use AUC (Area Under the Curve) and Confusion Matrix, F1 score as a combination measurement to justify the performance of a classficiation machine learning model, but what exactly is AUC, what exactly is Area and the Curve? I wanna to take a note for future reference. . Abbreviations . AUC = Area Under the Curve | AUROC = Area Under the Receiver Operating Characteristic Curve. | . AUC is used most of the time mean AUROC, which is bad practice since as Marc Claesen pointed out AUC is ambiguous (could be any curve) while AUROC is not. . Interpreting the AUROC . The AUROC has several equivalent interpretations . The expectation that a uniformly drawn random positive is ranked before a uniformly drawn random negative. | The expected proportion of positives ranked before a uniformly drawn random negative. | The expected true positive rate if the ranking is split just before a uniformly drawn random negative. | The expected proportion of negatives ranked after a uniformly drawn random positive. | The expected false positive rate if the ranking is split just after a uniformly drawn random positive. | . Going further: How to derive the probabilistic interpretation of the AUROC? . Computing the AUROC . Assume we have a probabilistic, binary classifier such as logistic regression. . Before presenting the ROC curve (= Receiver Operating Characteristic curve), the concept of confusion matrix must be understood. When we make a binary prediction, there can be 4 types of outcomes: . We predict 0 while the true class is actually 0: this is called a True Negative, i.e. we correctly predict that the class is negative (0). For example, an antivirus did not detect a harmless file as a virus. | We predict 0 while the true class is actually 1: this is called a False Negative, i.e. we incorrectly predict that the class is negative (0). For example, an antivirus failed to detect a virus. | We predict 1 while the true class is actually 0: this is called a False Positive, i.e. we incorrectly predict that the class is positive (1). For example, an antivirus considered a harmless file to be a virus. | We predict 1 while the true class is actually 1: this is called a True Positive, i.e. we correctly predict that the class is positive (1). For example, an antivirus rightfully detected a virus. | . To get the confusion matrix, we go over all the predictions made by the model, and count how many times each of those 4 types of outcomes occur: . In this example of a confusion matrix, among the 50 data points that are classified, 45 are correctly classified and the 5 are misclassified. . Since to compare two different models it is often more convenient to have a single metric rather than several ones, we compute two metrics from the confusion matrix, which we will later combine into one: . True Positive Rate, aka. Sensitivity, Hit Rate or Recall Rate, which defined as $ frac{TP}{TP+FN}$. Intuitively this metric corresponds to the proportion of positive data points that are correctly considered as positive, with respect to all positive data points. In other words, the higher True Positive Rate, the fewer positive data points we will miss. . | False Positive Rate, aka fall-out, which is defined as $ frac{FP}{FP+TN}$. Intuitively this metric corresponds to the proportion of negative data points that are mistakenly considered as positive, with respect to all negative data points. In other words, the higher False Positive Rate, the more negative data points will be missclassified. . | . To combine the False Positive Rate and the True Positive Rate into one single metric, we first compute the two former metrics with many different threshold (for example 0.00, 0.01, 0.02, ..., 1.00) for the logistic regression, then plot them on a single graph, with the False Positive Rate values on the abscissa and the True Positive Rate values on the ordinate. The resulting curve is called ROC curve, and the metric we consider is the AUC of this curve, which we call AUROC. . The following figure shows the AUROC graphically: . In this figure, the blue area corresponds to the Area Under the curve of the Receiver Operating Characteristic (AUROC). The dashed line in the diagonal we present the ROC curve of a random predictor: it has an AUROC of 0.5. The random predictor is commonly used as a baseline to see whether the model is useful. . If you want to get some first-hand experience: . Python | MATLAB | .",
            "url": "https://jl1829.github.io/turbo-funicular/machine%20learning/2020/03/23/What-does-AUC-stand-for-and-what-is-it.html",
            "relUrl": "/machine%20learning/2020/03/23/What-does-AUC-stand-for-and-what-is-it.html",
            "date": " • Mar 23, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "The git branch developing standard you shoud know",
            "content": "The Git branches developing standard you should know . . Git is by far the most popular source code management tool. In order to standardize development, keep the code commit record and git branch structure clear, and facilitate subsequent maintenance. Coder should bear in mind there’s some specification of branch naming, commit messages formating, and what &amp; when to merge different branches. In this blog, we are going to talk about the following topic: . Branch Mangement Naming convention | Routine Task | . | Message Specification Commit messages | . | . Branch Management . Branch Naming . Master Branch . master is the major branch, it’s also the production deployment branch, stablity is the uppermost important. | Normally master branch is merged by develop and hotfix branch, modfying code directly into the master branch is not allowed at all time. | . Develop branch . develop branch is the branch for further developing, keep updated all the time and merging the code when bugs was fixed. | When developing some new feature, there will be a feature branch breakout from develop branch in order to provide a “clean” environment for new feature developing. | . Feature branch . Based on develop branch, create feature branch | Naming: different features is named after the feature/, such as feature/preprocessing_module, feature/unsupervised_module | . Release branch . Release is a pre-launch branch. During the release and testing phase, the release branch code is used as a benchmark for testing. | . When a group of feature development is completed, it will first be merged into the develop branch. When entering the test, a release branch will be created. . If there are bugs that need to be fixed during the test, the developers will fix and submit them directly in the release branch. . After the test is completed, merge the release branch into the master and develop branches. At this time, the master is the latest code and is used for going online. . Hotfix branch . Branch naming: Started with hotfix/, and its naming rules are similar to feature branches, it build for bug fixing. | When there’s emergency, the bug need to be fixed in time. Use the master branch as the baseline and create a hotfix branch. After the debug is completed, it need to be merged it into the master branch and develop branch. | . Routine Task . New Function developing . (dev)$: git checkout -b feature/xxx # create feature from dev (feature/xxx)$: vi nlp_train.py # develop (feature/xxx)$: git add xxx (feature/xxx)$: git commit -m &#39;commit comment&#39; (dev)$: git merge feature/xxx --no-ff # merge feature into dev . Bug fix . (master)$: git checkout -b hotfix/xxx # create hotfix from master (hotfix/xxx)$: vi nlp_train.py # bug fix (hotfix/xxx)$: git add xxx (hotfix/xxx)$: git commit -m &#39;commit comment&#39; (master)$: git merge hotfix/xxx --no-ff # merge hotfix into master (dev)$: git merge hotfix/xxx --no-ff # merge hotfix into dev . Testing . (release)$: git merge dev --no-ff # merge dev into release . Bring it into production . (master)$: git merge release --no-ff # merge release into master (master)$: git tag -a v0.1 -m &#39;Version name&#39; # name the version and tag . Above process in one picture . Message Specification . In a team collaboration project, developers often need to submit some code to fix bugs or implement new features. The files in the project, what functions are implemented, and what problems are solved are gradually forgotten, and finally you need to waste time reading the code. But writing good formatting commit messages helps us, and it also reflects whether a developer is a good collaborator. . A good formatting commit messages can have this benefits: . Speed up the review process | Help the team to write up Release Note | Let the team know what and why this features were added and how the bugs were fixed. | . Currently there’s different version of commit messages, the Angular is the most accepted version, such as: . Commit messages format . Please refer to Angular Git Commit Guidelines . &lt;type&gt;: &lt;subject&gt; &lt;BLANK LINE&gt; &lt;body&gt; &lt;BLANK LINE&gt; &lt;footer&gt; . Type: what’s the type of this commit, is bugfix or docs or style etc. | Scope: the affecting range of this commit. | Subject: Describe the main idea of this commit. | Body: Detail explaination of this commit, such as motivation of this commit | Footer: Is there any related issue or break change? | . Different Types: . feat: New Features | fix: Bug fix | docs: Documentation modification | style: Changing coding style, such as whitespace, indentation. | refactor: Refactor the code, but did not change the logic | perf: adding some new code for performance test | test: Adding test code | .",
            "url": "https://jl1829.github.io/turbo-funicular/git/2020/03/12/The-Git-branch-developing-standard-you-shoud-know.html",
            "relUrl": "/git/2020/03/12/The-Git-branch-developing-standard-you-shoud-know.html",
            "date": " • Mar 12, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "The map(), filter(), reduce(), zip() and Lambda() funcion in Python",
            "content": "About . Today I would like to share some straightforward example of these important built-in functions in Python: . map | filter | reduce | zip | lambda | . They are some convenient function for us to deal with sequences, benefited by the world of functional programming, we can apply this functions into every single elements of a sequances, in order to reduce the time of explicit Loop. What&#39;s more, all this functions are pure function, both have return value, we can use this return value to repersent our expression. . So in layman term, why to use them is it cam much simplify our code, to execute those loop and iteration mission in a simple, elegance and efficient way. . map() . The map() function is going to return an iterator that applies function to every item of iterable, yielding the results. Such as list we can check whether an object have iterable attribute by using hasattr such as: . &gt; &gt;&gt; a = [1, 2, 3, 4] &gt;&gt;&gt; hasattr(a, &#39;__iter__&#39;) &gt;&gt;&gt; True . map() function simple syntex:map(func, iterable)* parameter: func is an function that map() pass to the every elements in the iterable object, the iterable is an object that has __iter__ attribute, so every elements can execute the func | return value: a map object | . Sounds like complicated? let&#39;s see an example: Assume we have a list that contain 1 - 5 digits, we want to every number add 1, before map() function, most likely we will do this: . numbers = [1, 2, 3, 4, 5] for i in range(0, len(numbers)): numbers[i] += 1 print(numbers) . [2, 3, 4, 5, 6] . Or, in another way: . numbers = [1, 2, 3, 4, 5] # create empty list result = [] for n in numbers: result.append(n+1) print(result) . [2, 3, 4, 5, 6] . Obviously, no matter in which way, we all need to handle loop, so, we can try to use map() function: . def add_one(n): return n+1 numbers = [1, 2, 3, 4, 5] result = map(add_one, numbers) print(result) print(type(result)) print(list(result)) . &lt;map object at 0x10a3c7150&gt; &lt;class &#39;map&#39;&gt; [2, 3, 4, 5, 6] . I believe you already notice the beauty of map(), we have achieved our purpose by not using loop, meanwhile, the code written in an elegent and simplicity way. So the map() function will return a map object if we planning to use this object in the future, this object type will help us to save the memory utilization, we use the getsizeof() function from sys to see the memory utilization of each object, map object and list . from sys import getsizeof print(f&#39;The size of map object in memory is {getsizeof(result)} bytes&#39;) print(f&#39;Convert it into list: {getsizeof(list(result))} bytes&#39;) . The size of map object in memory is 64 bytes Convert it into list: 72 bytes . The requirement of object to passed in map() function is iterable so as long as the object has attribute of __iter__ it works, not only list, but also tuple, such as: . numbers = (1, 2, 3, 4, 5) print(f&quot;Is tuple numbers iterable? Answer: {hasattr(numbers, &#39;__iter__&#39;)}&quot;) result = map(add_one, numbers) print(result) print(type(result)) print(tuple(result)) . Is tuple numbers iterable? Answer: True &lt;map object at 0x109bb9410&gt; &lt;class &#39;map&#39;&gt; (2, 3, 4, 5, 6) . Have you notice in order to achieved this, we need to create a function called add_one(n)? and it just simply return n+1, possible to reduce this? to make the code more simply and elegent? Yes, we can use lambda . numbers = (1, 2, 3, 4, 5) result = map(lambda x: x + 1, numbers) print(tuple(result)) . (2, 3, 4, 5, 6) . Three lines of code, simple, elegent, Pythonic . Beside using defined function or lambda function to execute the iterable, we also can utilize Python bulit-in function, bulit-in type to execute the iterable, such this case: . # list of strings words = [&#39;Singapore&#39;, &#39;Guangzhou&#39;, &#39;Tokyo&#39;] # convert every elements in the array into List converted = list(map(list, words)) print(converted) print(f&quot;The type of converted: {type(converted)}&quot;) print(f&quot;The lenght of converted: {len(converted)}&quot;) . [[&#39;S&#39;, &#39;i&#39;, &#39;n&#39;, &#39;g&#39;, &#39;a&#39;, &#39;p&#39;, &#39;o&#39;, &#39;r&#39;, &#39;e&#39;], [&#39;G&#39;, &#39;u&#39;, &#39;a&#39;, &#39;n&#39;, &#39;g&#39;, &#39;z&#39;, &#39;h&#39;, &#39;o&#39;, &#39;u&#39;], [&#39;T&#39;, &#39;o&#39;, &#39;k&#39;, &#39;y&#39;, &#39;o&#39;]] The type of converted: &lt;class &#39;list&#39;&gt; The lenght of converted: 3 . words is a list that contain string type of elements, we can use map() and Python bulit-in list to convert every elements in words into List, but do take note, every elements must have __iter__ attribute, otherwise, it will raise TypeError, such as int type: . numbers = [3, &#39;23&#39;, 42] print(list(map(list, numbers))) . TypeError Traceback (most recent call last) &lt;ipython-input-14-58446c133a68&gt; in &lt;module&gt; 1 numbers = [3, &#39;23&#39;, 42] -&gt; 2 print(list(map(list, numbers))) TypeError: &#39;int&#39; object is not iterable . We can see: TypeError, int object is not iterable, we can avoid it by this way: . numbers = [3, &#39;23&#39;, 42] print(list(map(float, numbers))) . [3.0, 23.0, 42.0] . filter() . filter() function is using a function to &quot;filter&quot; the sequence, the function is going to examinate every elements in the sequence is True or False . filter() syntex: filter(func, iterable) | Parameter: func test iterable sequances&#39; elements is True or False, iterable is the iterable sequances that been filter | Return value: an iterable sequance that every elements is True to the filter function func | . Layman term: filter() is to filter a set of data based on the given conditions . Example: . # filter vowel def func(variable): letters = [&#39;a&#39;, &#39;e&#39;, &#39;i&#39;, &#39;o&#39;, &#39;u&#39;] if (variable.lower() in letters): return True else: return False # given sequance sequance = [&#39;I&#39;, &#39;l&#39;, &#39;o&#39;, &#39;v&#39;, &#39;e&#39;, &#39;p&#39;, &#39;y&#39;, &#39;t&#39;, &#39;h&#39;, &#39;o&#39;, &#39;n&#39;] filtered = list(filter(func, sequance)) print(f&quot;The vowel in the sequance is {filtered}&quot;) . The vowel in the sequance is [&#39;I&#39;, &#39;o&#39;, &#39;e&#39;, &#39;o&#39;] . Above we create a method to pull the vowel from a given sequance, the given sequance is List, so it have &#39;__iter__&#39;, and apply it to filter() to pull out the vowel. . Here we have another example: . # positive or negitive number def positive(num): if num &gt; 0: return True else: return False # odd or even number def even_number(num): if num % 2 == 0: return True else: return False numbers = [1, -3, 5, -20, 0, 9, 12] positive_number = list(filter(positive, numbers)) even_number = list(filter(even_number, numbers)) print(f&quot;The positive number is: {positive_number}.&quot;) print(f&quot;The even number is {even_number}.&quot;) . The positive number is: [1, 5, 9, 12]. The even number is [-20, 0, 12]. . So, how to use filter() function is quite simple: . define a method that can filter out True or False | apply it to iterable object | Integrate it into your bigger code block | Now let&#39;s see how to use lambda together: . numbers = [0, 1, 2, -3, 5, -8, 42] # odd number odd_number = filter(lambda x: x % 2, numbers) print(f&quot;The odd number is {list(odd_number)}.&quot;) # even number even_number = filter(lambda x: x % 2 == 0, numbers) print(f&quot;The even number is {list(even_number)}.&quot;) # positive number positive_number = filter(lambda x: x &gt; 0, numbers) print(f&quot;The positive number is {list(positive_number)}.&quot;) . The odd number is [1, -3, 5]. The even number is [0, 2, -8, 42]. The positive number is [1, 2, 5, 42]. . Always remember the Python philosophy： efficient, simple, elegent . Reduce() . reduce() is very useful built-in function, it can execuate iterable object&#39;s compuatation and return the result, it can rolling compute the continues values in an iterable sequance, such as cumulative product of integer list, or cumulative sum. . Syntex: reduce(func, iterable) | Parameter: func: a continues method to execuate on each element of the iterable object, last resut is the new parameter of next execuation. | Return value: the func return value | . In Python 3, reduce() moved to functools module, so before we use it, we need to import it from functools . Example: . from functools import reduce def do_sum(num1, num2): return num1 + num2 print(f&quot;The sum of 1, 2, 3, 4 is: {reduce(do_sum, [1, 2, 3, 4])}.&quot;) . The sum of 1, 2, 3, 4 is: 10. . # cumulative product example def multiply(num1, num2): return num1*num2 print(f&quot;The cumulative product of 1, 2, 3, 4 is: {reduce(multiply, [1, 2, 3, 4])}.&quot;) . The cumulative product of 1, 2, 3, 4 is: 24. . # more simple and elegent way with lambda numbers = [1, 2, 3, 4] result_multiply = reduce(lambda x, y: x*y, numbers) result_sum = reduce(lambda x, y: x+y, numbers) print(f&quot;The cumulative product of 1, 2, 3, 4 is: {result_multiply}&quot;) print(f&quot;The cumulative sum of 1, 2, 3, 4 is: {result_sum}.&quot;) . The cumulative product of 1, 2, 3, 4 is: 24 The cumulative sum of 1, 2, 3, 4 is: 10. . zip() . As it&#39;s name, zip() function is to put multiple iterable object together, and &quot;packed&quot; it as one single object, mapping with similar index. . Syntex: zip(*iterators) | Parameter: iterators is iterable object, such as List, String | Return value: Single iterator object, containing index value from the packed object. | . Example: . keys = [&#39;name&#39;, &#39;age&#39;] values = [&#39;Apple&#39;, &#39;44&#39;] apple_dict = dict(zip(keys, values)) print(apple_dict) . {&#39;name&#39;: &#39;Apple&#39;, &#39;age&#39;: &#39;44&#39;} . zip() it also support multiple objects: . names = [&#39;Apple&#39;, &#39;Google&#39;, &#39;Microsoft&#39;] ages = [&#39;44&#39;, &#39;21&#39;, &#39;44&#39;] values = [&#39;100&#39;, &#39;80&#39;, &#39;60&#39;] mapped_values = list(zip(names, ages, values)) print(mapped_values) . [(&#39;Apple&#39;, &#39;44&#39;, &#39;100&#39;), (&#39;Google&#39;, &#39;21&#39;, &#39;80&#39;), (&#39;Microsoft&#39;, &#39;44&#39;, &#39;60&#39;)] . We can use the zip() function to easily packed the values have same index from 3 list . But how about unpack? . Simple, just similar to unpanc tuple, we add the * to the object that we want to unpack . names, ages, values = zip(*mapped_values) print(f&quot;The names is {names}&quot;) print(f&quot;The ages is {ages}&quot;) print(f&quot;The values is {values}&quot;) . The names is (&#39;Apple&#39;, &#39;Google&#39;, &#39;Microsoft&#39;) The ages is (&#39;44&#39;, &#39;21&#39;, &#39;44&#39;) The values is (&#39;100&#39;, &#39;80&#39;, &#39;60&#39;) . lambda() . While normal functions are defined using the def keyword, in Python anonymous functions are defined using the lambda keyword. Hence, anonymous functions are also called lambda functions. . Lambda function can use any quantity of parameter, but only have one expression . Syntex:lambda argument: manipulate(argument)* Parameter: argument is the passing parameter, after the : is the manipulate | . Example: . add_one = lambda x: x+1 add_sum = lambda x, y: x+y print(add_one(2)) print(add_sum(5, 5)) . 3 10 . Normally we will not use lambda function individually, we will use it along with other built-in function or def function, which we have already shows above, use it along with map(), filter(), reduce() and zip() function. . Let&#39;s see one more example of lambda interacting with dict . university = [{&#39;name&#39;: &#39;NYU&#39;, &#39;city&#39;: &#39;New York&#39;}, {&#39;name&#39;: &#39;NUS&#39;, &#39;city&#39;: &quot;Singapore&quot;}] names = list(map(lambda x: x[&#39;name&#39;], university)) print(names) . [&#39;NYU&#39;, &#39;NUS&#39;] . Above we interacting with dict with map() function, given the condition for the iterable list of dict that contain the name and the city of each University . We also can interacting dict with filter() function, through the return value True or False to judge the filtering condition. . university = [{&#39;name&#39;: &#39;NYU&#39;, &#39;city&#39;: &#39;New York&#39;}, {&#39;name&#39;: &#39;NUS&#39;, &#39;city&#39;: &quot;Singapore&quot;}] names = list(filter(lambda x: x[&#39;name&#39;] == &#39;NUS&#39;, university)) print(names) . [{&#39;name&#39;: &#39;NUS&#39;, &#39;city&#39;: &#39;Singapore&#39;}] . Through the above example, you have seen the actual application scenario of lambda, but here I want to share my views with you: I think that the disadvantages of lambda are slightly more than the advantages, and you should avoid overusing lambda. . First of all, this is just my personal opinion. I hope everyone understands why I said this. First, let&#39;s compare the lambda method with the conventional def. I find that the main differences between lambda and def are as follows: . Passing the parameter immediately(no need to define variable). | one line of code, very simple (but, it doesn&#39;t mean is efficient). | Automatically return, no need return keyword. | lambda function DO NOT have a function name | . You can see the advantages. I mainly want to talk about its disadvantages. First of all, starting from real needs, we don’t need lambda most of the time, because we can always find better alternatives. Now let ’s take a look In the example of lambda + reduce(), the result we achieved with lambada is as follows: . from functools import reduce numbers = [1,2,3,4] result_multiply = reduce((lambda x, y: x * y), numbers) result_add = reduce((lambda x,y: x+y), numbers) . Above example, the lambda didn&#39;t achieved the simple and efficient purpose, as we have the bulit-in sum and mul method, even it can work with NumPy also . from functools import reduce import operator import numpy as np numbers = [1, 2, 3, 4] result_sum = sum(numbers) result_multiply = reduce(operator.mul, numbers) print(f&quot;The sum is {result_sum}&quot;) print(f&quot;The cumulative product is: {result_multiply}&quot;) matrixA = np.random.randn(3, 3) matrixB = np.random.randn(3, 3) matrixList = [matrixA, matrixB] mulmat = reduce(operator.mul, matrixList) print(f&quot;The Matrix Multipication is n {mulmat}&quot;) . The sum is 10 The cumulative product is: 24 The Matrix Multipication is [[-2.09128347 -0.96279072 -1.81723096] [-1.26743106 0.45465535 0.47941216] [-0.36291425 -0.02198572 -0.52437492]] . The result is the same, but obviously the solution using sum and mul is more efficient. Another common example shows that if we have a list that stores various colors, we now need to capitalize the first letter of each color. If we write it in lambda: . colors = [&#39;red&#39;,&#39;purple&#39;,&#39;green&#39;,&#39;blue&#39;] result = map(lambda c:c.capitalize(),colors) print(list(result)) . [&#39;Red&#39;, &#39;Purple&#39;, &#39;Green&#39;, &#39;Blue&#39;] . Seems ok, but, did we forgot the power of List ? . colors = [&#39;red&#39;,&#39;purple&#39;,&#39;green&#39;,&#39;blue&#39;] result = [c.capitalize() for c in colors] print(result) . [&#39;Red&#39;, &#39;Purple&#39;, &#39;Green&#39;, &#39;Blue&#39;] . Sorted can also handle the case of irregular initials, saving even more time: . colors = [&#39;Red&#39;,&#39;purple&#39;,&#39;Green&#39;,&#39;blue&#39;] print(sorted(colors,key=str.capitalize)) . [&#39;blue&#39;, &#39;Green&#39;, &#39;purple&#39;, &#39;Red&#39;] . There is another reason: lambda functions do not have function names. So in the case of code transfer and project migration, it will bring a lot of difficulties to the team. It is not bad to write a function add_one(), because everyone is easy to understand and know that it is a function of performing +1, but if you are in the team, use a lots of lambda will make peoples confused. . Scenario that fit lambda . Your method is too simple to have a name, such as adding 1, or just stiching the strings | Using lambda is much easier for people to understand | There&#39;s no any Python built-in function except lambda | Team members are all well understand lambda, most importantly, no one complain you. |",
            "url": "https://jl1829.github.io/turbo-funicular/python/2020/03/06/The-map,-filter,-reduce,-zip-and-lambda-in-Python.html",
            "relUrl": "/python/2020/03/06/The-map,-filter,-reduce,-zip-and-lambda-in-Python.html",
            "date": " • Mar 6, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Using Scikit-Learn Neural Network Class to classify MNIST",
            "content": "About . Yann LeCun&#39;s MNIST is the most &quot;used&quot; dataset in Machine Learning I believe, lot&#39;s ML/DL practitioner will use it as the &quot;Hello World&quot; problem in Machine Learning, it&#39;s old, but golden, Even Geoffrey Hinton&#39;s Capsule Network also using MNIST as testing. . Most the tutorial online will guide the learner to use TensorFlow or Keras or PyTorch library to tackle MNIST problem, but actually it&#39;s not necessary， there&#39;s multiple solution for a single problem, we can tackle MNIST problem by &quot;Pure&quot; Python code, crafting the algorithm from scratch, or using the convential Machine Learning Library Scikit-Learn MLPClassifier . import matplotlib.pyplot as plt from sklearn.datasets import fetch_openml from sklearn.neural_network import MLPClassifier . # load the data from https://www.openml.org/d/554 X, y = fetch_openml(&#39;mnist_784&#39;, version=1, return_X_y=True) # use the traditional train/test split X_train, X_test = X[:60000], X[60000:] y_train, y_test = y[:60000], y[60000:] . Next we going to bulid a single hidden layer MLP model . mlp = MLPClassifier(hidden_layer_sizes=(50, ), max_iter=10, alpha=1e-4, solver=&#39;sgd&#39;, verbose=10, random_state=1, learning_rate_init=.1) . # start the training mlp.fit(X_train, y_train) . Iteration 1, loss = inf Iteration 2, loss = 94144.72785948 Iteration 3, loss = 94116.48942606 Iteration 4, loss = 94088.25915097 Iteration 5, loss = 94060.03773325 Iteration 6, loss = 94031.82434269 Iteration 7, loss = 94003.61939577 Iteration 8, loss = 93975.42334401 Iteration 9, loss = 93947.23547037 Iteration 10, loss = 93919.05614157 . MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(50,), learning_rate=&#39;constant&#39;, learning_rate_init=0.1, max_fun=15000, max_iter=10, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver=&#39;sgd&#39;, tol=0.0001, validation_fraction=0.1, verbose=10, warm_start=False) . Obsiously the loss is abnormal, is because we didnt scale the data . X = X / 255. . # use the traditional train/test split X_train, X_test = X[:60000], X[60000:] y_train, y_test = y[:60000], y[60000:] . mlp.fit(X_train, y_train) . Iteration 1, loss = 0.32009978 Iteration 2, loss = 0.15347534 Iteration 3, loss = 0.11544755 Iteration 4, loss = 0.09279764 Iteration 5, loss = 0.07889367 Iteration 6, loss = 0.07170497 Iteration 7, loss = 0.06282111 Iteration 8, loss = 0.05530788 Iteration 9, loss = 0.04960484 Iteration 10, loss = 0.04645355 . MLPClassifier(activation=&#39;relu&#39;, alpha=0.0001, batch_size=&#39;auto&#39;, beta_1=0.9, beta_2=0.999, early_stopping=False, epsilon=1e-08, hidden_layer_sizes=(50,), learning_rate=&#39;constant&#39;, learning_rate_init=0.1, max_fun=15000, max_iter=10, momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True, solver=&#39;sgd&#39;, tol=0.0001, validation_fraction=0.1, verbose=10, warm_start=False) . print(f&quot;Training set score: {mlp.score(X_train, y_train):.3f}&quot;) print(f&quot;Test set score: {mlp.score(X_test, y_test):.3f}&quot;) . Training set score: 0.987 Test set score: 0.970 . fig, axes = plt.subplots(4, 4) # use global min / max to ensure all weights are shown on the same scale vmin, vmax = mlp.coefs_[0].min(), mlp.coefs_[0].max() for coef, ax in zip(mlp.coefs_[0].T, axes.ravel()): ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax) ax.set_xticks(()) ax.set_yticks(()) plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt;",
            "url": "https://jl1829.github.io/turbo-funicular/jupyter/2020/03/02/Sklearn_MLP_for_MNIST.html",
            "relUrl": "/jupyter/2020/03/02/Sklearn_MLP_for_MNIST.html",
            "date": " • Mar 2, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Cost-Sensitive Decision Trees for Imbalanced Classification",
            "content": "Overview . This tutorial divided into 4 pars: . Imbalanced classfication dataset | Decision Trees for Imbalanced classfication | Weighted Decision Tree with Scikit-Learn | Grid Search Weighted Decision Trees | . Imbalanced classfication dataset . we use the make_classification()function to define a synthetic imbalanced two-class classfication dataset. We generate 10,000 examples with an approximate 1:100 minority to majority class ratio . # import the packages from collections import Counter from sklearn.datasets import make_classification from matplotlib import pyplot as plt import numpy as np . # generate dataset X, y = make_classification(n_samples=10000, n_features=2, n_redundant=0, n_clusters_per_class=1, weights=[0.99], flip_y=0, random_state=3) # examinate the info of X and y print(f&#39;The type of X is {type(X)}&#39;) print(f&#39;The type of y is {type(y)}&#39;) print(&#39; n&#39;) print(f&#39;The size of X is {X.shape}&#39;) print(f&#39;The size of y is {y.shape}&#39;) . The type of X is &lt;class &#39;numpy.ndarray&#39;&gt; The type of y is &lt;class &#39;numpy.ndarray&#39;&gt; The size of X is (10000, 2) The size of y is (10000,) . # summarize class distribution counter = Counter(y) print(counter) . Counter({0: 9900, 1: 100}) . # scatter plot of examples by class label for label, _ in counter.items(): row_ix = np.where(y == label)[0] plt.scatter(X[row_ix, 0], X[row_ix, 1], label=str(label)) plt.legend() plt.show() . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Fit it with standard decision tree model . A decision tree can be defined using the DecisionTreeClassifier class in the scikit-learn library . # import packages from sklearn.model_selection import cross_val_score from sklearn.model_selection import RepeatedStratifiedKFold from sklearn.tree import DecisionTreeClassifier # define model model = DecisionTreeClassifier() . # define evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # evaluate model score = cross_val_score(model, X, y, scoring=&#39;roc_auc&#39;, cv=cv, n_jobs=-1) # summarize performance print(f&#39;Mean ROC AUC: {np.mean(score):.3f}&#39;) . Mean ROC AUC: 0.734 . The model performance is achieving a ROC AUC above 0.5 which is 0.734 . This model can be the baseline for comparision for any modification performed to the standard decision tree algorithm . Decision Trees for Imbalanced Classification . The decision tree algorithm is also known as Classification and Regression Trees and involves growing a tree to classify examples from the training dataset. . The tree can be thought to divide the training dataset, where examples progress down the decision points of the tree to arrive in the leaves of the tree and are assigned a class label. . The tree is constructed by splitting the training dataset using values for variables in the dataset. At each point, the split in the data that results in the purest (least mixed) groups of examples is chosen in a greedy manner. . Here, purity means a clean separation of examples into groups where a group of examples of all 0 or all 1 class is the purest, and a 50-50 mixture of both classes is the least pure. Purity is most commonly calculated using Gini impurity, although it can also be calculated using entropy . The calculation of a purity measure involves calculating the probability of an example of a given class being misclassified by a split. Calculating these probabilities involves summing the number of examples in each class within each group. . The splitting criterion can be updated to not only take the purity of the split into account, but also be weighted by the importance of each class. . This can be achieved by replacing the count of examples in each group by a weighted sum, where the coefficient is provided to weight the sum. . Larger weight is assigned to the class with more importance, and a smaller weight is assigned to a class with less importance. . Small Weight: Less importance, lower impact on node purity | Large Weight: More importance, higher impact on node purity | . A small weight can be assigned to the majority class, which has the effect of improving (lowering) the purity score of a node that may otherwise look less well sorted. In turn, this may allow more examples from the majority class to be classified for the minority class, better accommodating those examples in the minority class. . As such, this modification of the decision tree algorithm is referred to as a weighted decision tree, a class-weighted decision tree, or a cost-sensitive decision tree. . Modification of the split point calculation is the most common, although there has been a lot of research into a range of other modifications of the decision tree construction algorithm to better accommodate a class imbalance. . Weighted Decision Tree with Scikit-learn . The scikit-learn Python machine learning library provides an implementation of the decision tree algorithm that supports class weighting. . The DecisionTreeClassifier class provides the class_weight argument that can be specified as a model hyperparameter. The class_weight is a dictionary that defines each class label (e.g. 0 and 1) and the weighting to apply in the calculation of group pruity for splits in the decision tree when fitting the model . For example: . # define model weights = {0:1.0, 1:1.0} model = DecisionTreeClassifier(class_weight=weights) . The class weighting can be defined multiple ways, for example: . Domain Expertise: determined by talking to subject matter experts | Tuning: determined by hyperparameter search such as GirdSearch | Huristic: spcified using a general best practice | . A best practice for using the class weighting is to use the inverse of the class distribution present in the training dataset. . For example, the class distribution of the test dataset is a 1:100 ratio for the minority class to the majority class. The invert of this ratio could be used with 1 for the majority class and 100 for the minority class. . for example: . # define model weights = {0:1.0, 1:100.0} model = DecisionTreeClassifier(class_weight=weights) . We might also define the same ratio using fractions and achieve the same results . # define model weights = {0:0.01, 1:1.0} model = DecisionTreeClassifier(class_weight=weights) . Or also can be defined by pre-loaded attributes: python model = DecisionTreeClassifier(class_weight=&#39;balanced&#39;) . Detail about class_weight in DecisionTreeClassifier class . **class_weightdict, list of dict or “balanced”, default=None** . Weights associated with classes in the form {class_label: weight}. If None, all classes are supposed to have weight one. For multi-output problems, a list of dicts can be provided in the same order as the columns of y. . Note that for multioutput (including multilabel) weights should be defined for each class of every column in its own dict. For example, for four-class multilabel classification weights should be [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of [{1:1}, {2:5}, {3:1}, {4:1}]. . The “balanced” mode uses the values of y to automatically adjust weights inversely proportional to class frequencies in the input data as n_samples / (n_classes * np.bincount(y)) . For multi-output, the weights of each column of y will be multiplied. . Note that these weights will be multiplied with sample_weight (passed through the fit method) if sample_weight is specified. . # decision tree with class weight on an imbalanced classification dataset model = DecisionTreeClassifier(class_weight=&#39;balanced&#39;) cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) scores = cross_val_score(model, X, y, scoring=&#39;roc_auc&#39;, cv=cv, n_jobs=-1) . # print out performance print(f&#39;The model ROC AUC mean is: {np.mean(scores):.3f}&#39;) . The model ROC AUC mean is: 0.749 . Ok, very well, the performance just a little bit imporved, from 0.734 to 0.749 . Grid Search Weighted Decision Tree . Using a class weighting that is the inverse ratio of the training data is just a heuristic. . It is possible that better performance can be achieved with a different class weighting, and this too will depend on the choice of performance metric used to evaluate the model. . In this section, we will grid search a range of different class weightings for the weighted decision tree and discover which results in the best ROC AUC score. . We will try the following weightings for class 0 and 1: . Class 0: 100, Class 1: 1 | Class 0: 10, Class 1: 1 | Class 0: 1, Class 1: 1 | Class 0: 1, Class 1: 10 | Class 0: 1, Class 1: 100 | . This can be defined as grid search parameter for the GridSearchCV class as follow: . # define grid balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] param_grid = dict(class_weight=balance) . # define grid balance = [{0:100,1:1}, {0:10,1:1}, {0:1,1:1}, {0:1,1:10}, {0:1,1:100}] param_grid = dict(class_weight=balance) print(param_grid) . {&#39;class_weight&#39;: [{0: 100, 1: 1}, {0: 10, 1: 1}, {0: 1, 1: 1}, {0: 1, 1: 10}, {0: 1, 1: 100}]} . from sklearn.model_selection import GridSearchCV # define evaluation procedure cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) # define model model = DecisionTreeClassifier() # define grid search grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, scoring=&#39;roc_auc&#39;, verbose=1) # fit the GridSearch grid_result = grid.fit(X, y) . Fitting 30 folds for each of 5 candidates, totalling 150 fits [Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers. [Parallel(n_jobs=-1)]: Done 150 out of 150 | elapsed: 0.4s finished . # report the best configuration print(f&#39;Best {grid_result.best_score_} using {grid_result.best_params_}&#39;) . Best 0.750925925925926 using {&#39;class_weight&#39;: {0: 1, 1: 10}} . # report all configurations means = grid_result.cv_results_[&#39;mean_test_score&#39;] stds = grid_result.cv_results_[&#39;std_test_score&#39;] params = grid_result.cv_results_[&#39;params&#39;] for mean, stdev, param in zip(means, stds, params): print(&quot;%f (%f) with: %r&quot; % (mean, stdev, param)) . 0.740556 (0.071391) with: {&#39;class_weight&#39;: {0: 100, 1: 1}} 0.735539 (0.070241) with: {&#39;class_weight&#39;: {0: 10, 1: 1}} 0.735707 (0.068985) with: {&#39;class_weight&#39;: {0: 1, 1: 1}} 0.750926 (0.071732) with: {&#39;class_weight&#39;: {0: 1, 1: 10}} 0.746212 (0.075781) with: {&#39;class_weight&#39;: {0: 1, 1: 100}} . Very well, fine turning the Class_weight hyperparameter, we have 0.01 performance imporve . Further Reading . Paper . An instance-weighting Method To Induce Cost-sensitive Tree, 2002 | . Books . Learning from Imbalanced Datasets, 2018 | Imbalanced Learning: Foundations, Algorithms, and Applications, 2013. | . APIs . sklearn.utils.class_weight.compute_class_weight | sklearn.tree.DecisionTreeClassifier | sklearn.model_selection.GridSearchCV | .",
            "url": "https://jl1829.github.io/turbo-funicular/jupyter/2020/02/26/Decision_Tree_Imbalance.html",
            "relUrl": "/jupyter/2020/02/26/Decision_Tree_Imbalance.html",
            "date": " • Feb 26, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jl1829.github.io/turbo-funicular/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Machine Learning Mathematical Foundation",
            "content": "Machine Learning Mathematical Foundation . 1.1 The relationship between scalars, vectors, matrices, and tensors . Scalar: . A scalar represents a single number that is different from most other objects studied in linear algebra (usually an array of multiple numbers). We use italics to represent scalars. Scalars are usually given a lowercase variable name. . Vector: . A vector represents a set of ordered numbers. By indexing in the order, we can determine each individual number. Usually we give the lowercase variable name of the vector bold, such as xx. Elements in a vector can be represented in italics with a footer. The first element of the vector $X$ is $X_1$, the second element is $X_2$, and so on. We will also indicate the type of element (real, imaginary, etc.) stored in the vector. . Matrix: . A matrix is ​​a collection of objects with the same features and latitudes, represented as a two-dimensional data table. The meaning is that an object is represented as a row in a matrix, and a feature is represented as a column in a matrix, and each feature has a numerical value. The name of an uppercase variable that is usually given to the matrix bold, such as $A$. . Tensor: . In some cases, we will discuss arrays with coordinates over two dimensions. In general, the elements in an array are distributed in a regular grid of several dimensional coordinates, which we call a tensor. Use $A$ to represent the tensor “A”. The element with a coordinate of $(i,j,k)$ in the tensor $A$ is denoted as $A_{(i,j,k)}$. . Relationship between the four . The scalar is a 0th order tensor and the vector is a first order tensor. Example: The scalar is the length of the stick, but you won’t know where the stick is pointing. Vector is not only knowing the length of the stick, but also knowing whether the stick points to the front or the back. The tensor is not only knowing the length of the stick, but also knowing whether the stick points to the front or the back, and how much the stick is deflected up/down and left/right. . 1.2 What is the difference between tensor and matrix? . From an algebra perspective, a matrix is ​​a generalization of vectors. The vector can be seen as a one-dimensional “table” (that is, the components are arranged in a row in order), the matrix is ​​a two-dimensional “table” (components are arranged in the vertical and horizontal positions), then the $n$ order tensor is the so-called $n$ dimension “Form”. The strict definition of tensors is described using linear mapping. | Geometrically, a matrix is ​​a true geometric quantity, that is, it is something that does not change with the coordinate transformation of the frame of reference. Vectors also have this property. | The tensor can be expressed in a 3×3 matrix form. | A three-dimensional array representing the number of scalars and the representation vector can also be regarded as a matrix of 1 × 1, 1 × 3, respectively. | . 1.3 Matrix and vector multiplication results . A matrix of $m$ rows of $n$ columns is multiplied by a $n$ row vector, and finally a vector of $m$ rows is obtained. The algorithm is that each row of data in the matrix is ​​treated as a row vector and multiplied by the vector. . 1.4 Vector and matrix norm induction . Vector norm Define a vector as: $ vec{a}=[-5, 6, 8, -10]$. Any set of vectors is set to $ vec{x}=(x_1,x_2,…,x_N)$. The different norms are solved as follows: . L1 norm of the vector: the sum of the absolute values ​​of the elements of the vector. The 1 norm result of the above vector $ vec{a}$ is: 29. | . ∥x⃗∥1=∑i=1N∣xi∣ Vert vec{x} Vert_1= sum_{i=1}^N vert{x_i} vert∥x . ∥1​=i=1∑N​∣xi​∣ . The L2 norm of the vector: the sum of the squares of each element of the vector and the square root. The result of the 2 norm of $ vec{a}$ above is: 15. | . ∥x⃗∥2=∑i=1N∣xi∣2 Vert vec{x} Vert_2= sqrt{ sum_{i=1}^N{ vert{x_i} vert}^2}∥x . ∥2​=i=1∑N​∣xi​∣2 . ​ . Negative infinite norm of the vector: the smallest of the absolute values ​​of all elements of the vector: the negative infinite norm of the above vector $ vec{a}$ is: 5. | . ∥x⃗∥−∞=min⁡∣xi∣ Vert vec{x} Vert_{- infty}= min{|{x_i}|}∥x . ∥−∞​=min∣xi​∣ . The positive infinite norm of the vector: the largest of the absolute values ​​of all elements of the vector: the positive infinite norm of the above vector $ vec{a}$ is: 10. | . ∥x⃗∥+∞=max⁡∣xi∣ Vert vec{x} Vert_{+ infty}= max{|{x_i}|}∥x . ∥+∞​=max∣xi​∣ . p-norm of vector: | . Lp=∥x⃗∥p=∑i=1N∣xi∣ppL_p= Vert vec{x} Vert_p= sqrt[p]{ sum_{i=1}^{N}|{x_i}|^p}Lp​=∥x . ∥p​=pi=1∑N​∣xi​∣p . ​ . Matrix of the matrix . Define a matrix $A= begin{bmatrix}-1 &amp; 2 &amp; -3 4 &amp; -6 &amp; 6 end{bmatrix}$. The arbitrary matrix is ​​defined as: $A_{m times n}$ with elements of $a_{ij}$. . The norm of the matrix is ​​defined as . ∥A∥p:=sup⁡x≠0∥Ax∥p∥x∥p Vert{A} Vert_p := sup_{x neq 0} frac{ Vert{Ax} Vert_p}{ Vert{x} Vert_p}∥A∥p​:=x​=0sup​∥x∥p​∥Ax∥p​​ . When the vectors take different norms, different matrix norms are obtained accordingly. . 1 norm of the matrix (column norm): The absolute values ​​of the elements on each column of the matrix are first summed, and then the largest one is taken, (column and maximum), the 1 matrix of the above matrix $A$ The number first gets $[5,8,9]$, and the biggest final result is: 9. | . ∥A∥1=max⁡1≤j≤∑i=1m∣aij∣ Vert A Vert_1= max_{1 le j le} sum_{i=1}^m|{a_{ij}}|∥A∥1​=1≤j≤max​i=1∑m​∣aij​∣ . 2 norm of matrix: The square root of the largest eigenvalue of the matrix $A^TA$, the final result of the 2 norm of the above matrix $A$ is: 10.0623. | . ∥A∥2=λmax(ATA) Vert A Vert_2= sqrt{ lambda_{max}(A^T A)}∥A∥2​=λmax​(ATA) . ​ . Where $ lambda_{max}(A^T A)$ is the maximum value of the absolute value of the eigenvalue of $A^T A$. . Infinite norm of the matrix (row norm): The absolute values ​​of the elements on each line of the matrix are first summed, and then the largest one (row and maximum) is taken, and the above matrix of $A$ is 1 The number first gets $[6;16]$, and the biggest final result is: 16. ∥A∥∞=max⁡1≤i≤n∑j=1n∣aij∣ Vert A Vert_{ infty}= max_{1 le i le n} sum_{j=1}^n |{a_{ij}}|∥A∥∞​=max1≤i≤n​∑j=1n​∣aij​∣ . | Matrix kernel norm: the sum of the singular values ​​of the matrix (decomposed of the matrix svd), this norm can be used for low rank representation (because the minimization of the kernel norm is equivalent to minimizing the rank of the matrix - Low rank), the final result of matrix A above is: 10.9287. . | Matrix L0 norm: the number of non-zero elements of the matrix, usually used to represent sparse, the smaller the L0 norm, the more elements, the more sparse, the final result of the above matrix $A$ is :6. | Matrix L1 norm: the sum of the absolute values ​​of each element in the matrix, which is the optimal convex approximation of the L0 norm, so it can also represent sparseness, the final result of the above matrix $A$ is: 22 . | **F norm of matrix **: the sum of the squares of the elements of the matrix and the square root of the square. It is also commonly called the L2 norm of the matrix. Its advantage is that it is a convex function, which can be solved and easy to calculate. The final result of the above matrix A is: 10.0995. | . ∥A∥F=(∑i=1m∑j=1n∣aij∣2) Vert A Vert_F= sqrt{( sum_{i=1}^m sum_{j=1}^n{| a_{ij}|}^2)}∥A∥F​=(i=1∑m​j=1∑n​∣aij​∣2) . ​ . Matrix L21 norm: matrix first in each column, find the F norm of each column (can also be considered as the vector’s 2 norm), and then the result obtained L1 norm (also It can be thought of as the 1 norm of the vector. It is easy to see that it is a norm between L1 and L2. The final result of the above matrix $A$ is: 17.1559. | p-norm of the matrix ∥A∥p=(∑i=1m∑j=1n∣aij∣p)p Vert A Vert_p= sqrt[p]{( sum_{i=1}^m sum_{j=1}^n{| a_{ij}|}^p)}∥A∥p​=p(∑i=1m​∑j=1n​∣aij​∣p)​ | . 1.5 How to judge a matrix as positive? . the order master subtype is all greater than 0; | There is a reversible matrix $C$ such that $C^TC$ is equal to the matrix; | Positive inertia index is equal to $n$; | Contract in unit matrix $E$ (ie: canonical form is $E$) | the main diagonal elements in the standard form are all positive; | the eigenvalues ​​are all positive; | is a measure matrix of a base. | . 1.6 Derivative Bias Calculation . Derivative definition: . The derivative represents the ratio of the change in the value of the function to the change in the independent variable when the change in the independent variable tends to infinity. Geometric meaning is the tangent to this point. The physical meaning is the (instantaneous) rate of change at that moment. . Note: In a one-way function, only one independent variable changes, that is, there is only one direction of change rate, which is why the unary function has no partial derivative. There is an average speed and instantaneous speed in physics. Average speed . v=stv= frac{s}{t}v=ts​ . Where $v$ represents the average speed, $s$ represents the distance, and $t$ represents the time. This formula can be rewritten as . vˉ=ΔsΔt=s(t0+Δt)−s(t0)Δt bar{v}= frac{ Delta s}{ Delta t}= frac{s(t_0+ Delta t)-s(t_0)}{ Delta t}vˉ=ΔtΔs​=Δts(t0​+Δt)−s(t0​)​ . Where $ Delta s$ represents the distance between two points, and $ Delta t$ represents the time it takes to walk through this distance. When $ Delta t$ tends to 0 ($ Delta t to 0$), that is, when the time becomes very short, the average speed becomes the instantaneous speed at time $t_0$, expressed as follows : . v(t0)=lim⁡Δt→0vˉ=lim⁡Δt→0ΔsΔt=lim⁡Δt→0s(t0+Δt)−s(t0)Δtv(t_0)= lim_{ Delta t to 0}{ bar{v}}= lim_{ Delta t to 0}{ frac{ Delta s}{ Delta t}}= lim_ { Delta t to 0}{ frac{s(t_0+ Delta t)-s(t_0)}{ Delta t}}v(t0​)=Δt→0lim​vˉ=Δt→0lim​ΔtΔs​=Δt→0lim​Δts(t0​+Δt)−s(t0​)​ . In fact, the above expression represents the derivative of the function $s$ on time $t$ at $t=t_0$. In general, the derivative is defined such that if the limit of the average rate of change exists, there is . lim⁡Δx→0ΔyΔx=lim⁡Δx→0f(x0+Δx)−f(x0)Δx lim_{ Delta x to 0}{ frac{ Delta y}{ Delta x}}= lim_{ Delta x to 0}{ frac{f(x_0+ Delta x)-f(x_0 )}{ Delta x}}Δx→0lim​ΔxΔy​=Δx→0lim​Δxf(x0​+Δx)−f(x0​)​ . This limit is called the derivative of the function $y=f(x)$ at point $x_0$. Remember as $f’(x_0)$ or $y’ vert_{x=x_0}$ or $ frac{dy}{dx} vert_{x=x_0}$ or $ frac{df(x)}{ Dx} vert_{x=x_0}$. . In layman’s terms, the derivative is the slope of the curve at a certain point. . Partial derivative: . Since we talk about partial derivatives, there are at least two independent variables involved. Taking two independent variables as an example, z=f(x,y), from the derivative to the partial derivative, that is, from the curve to the surface. At one point on the curve, there is only one tangent. But at one point on the surface, there are countless lines of tangent. The partial derivative is the rate of change of the multivariate function along the coordinate axis. . Note: Intuitively speaking, the partial derivative is the rate of change of the function along the positive direction of the coordinate axis at a certain point. . Let the function $z=f(x,y)$ be defined in the field of the point $(x_0,y_0)$. When $y=y_0$, $z$ can be regarded as a unary function $f on $x$ (x,y_0)$, if the unary function is derivable at $x=x_0$, there is . lim⁡Δx→0f(x0+Δx,y0)−f(x0,y0)Δx=A lim_{ Delta x to 0}{ frac{f(x_0+ Delta x,y_0)-f(x_0,y_0)}{ Delta x}}=AΔx→0lim​Δxf(x0​+Δx,y0​)−f(x0​,y0​)​=A . The limit of the function $A$ exists. Then say $A$ is the partial derivative of the argument $x=f(x,y)$ at the point $(x_0,y_0)$ about the argument $x$, denoted as $f_x(x_0,y_0)$ or $ Frac{ partial z}{ partial x} vert_{y=y_0}^{x=x_0}$ or $ frac{ partial f}{ partial x} vert_{y=y_0}^{x= X_0}$ or $z_x vert_{y=y_0}^{x=x_0}$. . When the partial derivative is solved, another variable can be regarded as a constant and solved by ordinary derivation. For example, the partial derivative of $z=3x^2+xy$ for $x$ is $z_x=6x+y$, this When $y$ is equivalent to the coefficient of $x$. . The geometric meaning of the partial derivative at a point $(x_0, y_0)$ is the intersection of the surface $z=f(x,y)$ with the face $x=x_0$ or the face $y=y_0$ at $y=y_0$ Or the slope of the tangent at $x=x_0$. . 1.7 What is the difference between the derivative and the partial derivative? . There is no essential difference between the derivative and the partial derivative. If the limit exists, it is the limit of the ratio of the change of the function value to the change of the independent variable when the variation of the independent variable tends to zero. . Unary function, a $y$ corresponds to a $x$, and the derivative has only one. | A binary function, a $z$ corresponding to a $x$ and a $y$, has two derivatives: one is the derivative of $z$ to $x$, and the other is the derivative of $z$ to $y$, Call it a partial guide. | Be careful when seeking partial derivatives. If you refer to one variable, then the other variable is constant. Only the amount of change is derived, and the solution of the partial derivative is transformed into the derivation of the unary function. | . 1.8 Eigenvalue decomposition and eigenvectors . eigenvalue decomposition can obtain eigenvalues ​​and eigenvectors; . | The eigenvalue indicates how important this feature is, and the eigenvector indicates what this feature is. . If a vector $ vec{v}$ is a feature vector of the square matrix $A$, it will definitely be expressed in the following form: . | . Aν=λνA nu = lambda nuAν=λν . $ lambda$ is the eigenvalue corresponding to the feature vector $ vec{v}$. Eigenvalue decomposition is the decomposition of a matrix into the following form: . A=Q∑Q−1A=Q sum Q^{-1}A=Q∑Q−1 . Where $Q$ is the matrix of the eigenvectors of the matrix $A$, $ sum$ is a diagonal matrix, and each diagonal element is a eigenvalue, and the eigenvalues ​​are arranged from large to small. The eigenvectors corresponding to these eigenvalues ​​describe the direction of the matrix change (from the primary change to the secondary change arrangement). That is to say, the information of the matrix $A$ can be represented by its eigenvalues ​​and eigenvectors. . 1.9 What is the relationship between singular values ​​and eigenvalues? . So how do singular values ​​and eigenvalues ​​correspond? We multiply the transpose of a matrix $A$ by $A$ and the eigenvalues ​​of $AA^T$, which have the following form: . (ATA)V=λV(A^TA)V = lambda V(ATA)V=λV . Here $V$ is the right singular vector above, in addition to: . σi=λi,ui=1σiAμi sigma_i = sqrt{ lambda_i}, u_i= frac{1}{ sigma_i}A mu_iσi​=λi​ . ​,ui​=σi​1​Aμi​ . Here $ sigma$ is the singular value, and $u$ is the left singular vector mentioned above. [Prove that the buddy did not give] The singular value $ sigma$ is similar to the eigenvalues, and is also ranked from large to small in the matrix $ sum$, and the reduction of $ sigma$ is particularly fast, in many cases, the first 10% or even the 1% singularity. The sum of the values ​​accounts for more than 99% of the sum of all the singular values. In other words, we can also approximate the description matrix with the singular value of the previous $r$($r$ is much smaller than $m, n$), that is, the partial singular value decomposition: . Am×n≈Um×r∑r×rVr×nTA_{m times n} approx U_{m times r} sum_{r times r}V_{r times n}^TAm×n​≈Um×r​r×r∑​Vr×nT​ . The result of multiplying the three matrices on the right will be a matrix close to $A$. Here, the closer $r$ is to $n$, the closer the multiplication will be to $A$. . 1.10 Why should machine use probability? . The probability of an event is a measure of the likelihood that the event will occur. Although the occurrence of an event in a randomized trial is accidental, randomized trials that can be repeated in large numbers under the same conditions tend to exhibit significant quantitative patterns. In addition to dealing with uncertainties, machine learning also needs to deal with random quantities. Uncertainty and randomness may come from multiple sources, using probability theory to quantify uncertainty. Probability theory plays a central role in machine learning because the design of machine learning algorithms often relies on probability assumptions about the data. . For example, in the course of machine learning (Andrew Ng), there is a naive Bayesian hypothesis that is an example of conditional independence. The learning algorithm makes assumptions about the content to determine if the email is spam. Assume that the probability condition that the word x appears in the message is independent of the word y, regardless of whether the message is spam or not. Obviously this assumption is not without loss of generality, because some words almost always appear at the same time. However, the end result is that this simple assumption has little effect on the results, and in any case allows us to quickly identify spam. . 1.11 What is the difference between a variable and a random variable? . Random variable . A real-valued function (all possible sample points) for various outcomes in a random phenomenon (a phenomenon that does not always appear the same result under certain conditions). For example, the number of passengers waiting at a bus stop at a certain time, the number of calls received by the telephone exchange at a certain time, etc., are all examples of random variables. The essential difference between the uncertainty of random variables and fuzzy variables is that the latter results are still uncertain, that is, ambiguity. . **The difference between a variable and a random variable: ** When the probability of the value of the variable is not 1, the variable becomes a random variable; when the probability of the random variable is 1, the random variable becomes a variable. . For example: When the probability of a variable $x$ value of 100 is 1, then $x=100$ is determined and will not change unless there is further operation. When the probability of the variable $x$ is 100, the probability of 50 is 0.5, and the probability of 100 is 0.5. Then the variable will change with different conditions. It is a random variable. The probability of 50 or 100 is 0.5, which is 50%. . 1.12 The relationship between random variables and probability distribution? . A random variable simply represents a state that may be achieved, and a probability distribution associated with it must be given to establish the probability of each state. The method used to describe the probability of the probability of each possible state of a random variable or a cluster of random variables is the probability distribution. . Random variables can be divided into discrete random variables and continuous random variables. . The corresponding function describing its probability distribution is . Probability Mass Function (PMF): Describes the probability distribution of discrete random variables, usually expressed in uppercase letters $P$. . Probability Density Function (PDF): A probability distribution describing a continuous random variable, usually expressed in lowercase letters $p$. . 1.12.1 Discrete random variables and probability mass functions . PMF maps each state that a random variable can take to a random variable to obtain the probability of that state. . In general, $P(x)$ represents the probability of $X=x $. | Sometimes to avoid confusion, explicitly write the name of the random variable $P( $x$=x) $ | Sometimes you need to define a random variable and then formulate the probability distribution x it follows. Obey $P($x $) $ | . PMF can act on multiple random variables simultaneously, ie joint probability distribution $P(X=x, Y=y) $* means $X=x $ and the same as $Y=y $ Probability can also be abbreviated as $P(x,y) $. . If a function $P $ is a PMF of the random variable $X $, then it must satisfy the following three conditions: . $P$’s domain must be a collection of all possible states | $∀x∈ $x, $0 leq P(x) leq 1 $. | $∑_{x∈X} P(x)=1$. We call this property normalized | . 1.12.2 Continuous Random Variables and Probability Density Functions . If a function $p $ is a PDF of x, then it must satisfy the following conditions . The domain of $p$ must be a collection of all possible states of xx. | $∀x∈X,p(x)≥0$. Note that we do not require $p(x)≤1$ because $p(x)$ is not the specific probability of representing this state, and Is a relative size (density) of probability. The specific probability requires integration to find. | $∫p(x)dx=1$, the score is down, the sum is still 1, and the sum of the probabilities is still 1. | . Note: PDF$p(x)$ does not directly give a probability to a particular state, giving a density. In contrast, it gives a probability that the area falling within a small area of ​​$δx$ is $ p(x)δx$. Thus, we can’t find the probability of a particular state. What we can find is that the probability that a state $x$ falls within a certain interval $[a,b]$ is $ int_{a}^{b}p(x)dx$. . 1.13 Common probability distribution . 1.13.1 Bernoulli Distribution . Bernoulli distribution is a single binary random variable distribution, single parameter $ phi $∈[0,1] control, $ phi $ gives the probability that the random variable is equal to 1. The main properties are: begin{align*} P(x=1) &amp;= phi P(x=0) &amp;= 1- phi P(x=x) &amp;= phi^x(1- phi)^{1-x} end{align*} Its expectations and variances are: begin{align*} E_x[x] &amp;= phi Var_x(x) &amp;= phi{(1- phi)} end{align*} Multinoulli distribution is also called category distribution, which is a random distribution of individual kk values, often used to represent the distribution of object classifications. where $k $ is a finite value. Multinoulli distribution consists of Vector $ vec{p} in[0,1]^{k-1} $parameterized, each component $p_i $ represents the probability of the $i $ state, and $p_k=1-1 ^Tp $. . Scope of application: Bernoulli distribution is suitable for modeling **discrete **random variables. . 1.13.2 Gaussian distribution . Gauss is also called Normal Distribution. The probability function is as follows: N(x;μ,σ2)=12πσ2exp(−12σ2(x−μ)2)N(x; mu, sigma^2) = sqrt{ frac{1}{2 pi sigma^2}}exp left ( - frac{1}{2 sigma^2}(x - mu)^2 right )N(x;μ,σ2)=2πσ21​​exp(−2σ21​(x−μ)2) Where $ mu $ and $ sigma $ are mean and variance, respectively. The center peak x coordinate is given by $ mu $, the width of the peak is controlled by $ sigma $, and the maximum point is $x= Obtained at mu $, the inflection point is $x= mu pm sigma $ . In the normal distribution, the probability of ±1$ sigma$, ±2$ sigma$, and ±3$ sigma$ are 68.3%, 95.5%, and 99.73%, respectively. These three numbers are best remembered. . In addition, let $ mu=0, sigma=1 $ Gaussian distribution be reduced to the standard normal distribution: N(x;μ,σ2)=12πexp(−12x2)N(x; mu, sigma^2) = sqrt{ frac{1}{2 pi}}exp left ( - frac{1}{2}x^2 right )N(x;μ,σ2)=2π1​​exp(−21​x2) Efficiently evaluate the probability density function: N(x;μ,β−1)=β2πexp(−12β(x− Mu)2)N(x; mu, beta^{-1})= sqrt{ frac{ beta}{2 pi}}exp left(- frac{1}{2} beta(x- Mu)^2 right)N(x;μ,β−1)=2πβ​​exp(−21​β(x− Mu)2) . Among them, $ beta= frac{1}{ sigma^2}$ controls the distribution precision by the parameter $ beta∈(0, infty) $. . 1.13.3 When is a normal distribution? . Q: When is a normal distribution? Answer: There is no prior knowledge distributed on real numbers. When I don’t know which form to choose, the default choice of normal distribution is always wrong. The reasons are as follows: . The central limit theorem tells us that many independent random variables approximate a normal distribution. In reality, many complex systems can be modeled as normally distributed noise, even if the system can be structurally decomposed. | Normal distribution is the distribution with the greatest uncertainty among all probability distributions with the same variance. In other words, the normal distribution is the distribution with the least knowledge added to the model. | Generalization of normal distribution: The normal distribution can be generalized to the $R^n$ space, which is called the multiple normal distribution, whose parameter is a positive definite symmetric matrix $ sum$: N(x;μ⃗,∑)=12πndet(∑)exp(−12(x⃗−μ⃗)T∑−1(x⃗−μ⃗))N(x; vec mu, sum)= sqrt{ frac{1}{2 pi^ndet( sum)}}exp left(- frac{1}{2}( vec{ x}- vec{ mu})^T sum^-1( vec{x}- vec{ mu}) right)N(x;μ​,∑)=2πndet(∑)1​​exp(−21​(x−μ​)T∑−1(x−μ​)) Efficiently evaluate the probability density of mostly normal distributions: N(x;μ⃗,β⃗−1)=det(β⃗)(2π)nexp(−12(x⃗−μ⃗)Tβ(x⃗−μ⃗))N(x; vec{ mu}, vec beta^{-1}) = sqrt{det( vec beta)}{(2 pi)^n}exp left(- frac{ 1}{2}( vec{x}- vec mu)^T beta( vec{x}- vec mu) right)N(x;μ​,β​−1)=det(β​)​(2π)nexp(−21​(x−μ​)Tβ(x−μ​)) Here, $ vec beta$ is a precision matrix. . 1.13.4 Exponential distribution . In deep learning, the exponential distribution is used to describe the distribution of the boundary points at $x=0$. The exponential distribution is defined as follows: p(x;λ)=λ1x≥0exp(−λx)p(x; lambda)= lambda1_{x geq 0}exp(- lambda{x})p(x;λ)=λ1x≥0​exp(−λx) The exponential distribution uses the indication function $I_{x&gt;=0}$ to make the probability of a negative value of $x$ zero. . 1.13.5 Laplace Distribution . A closely related probability distribution is the Laplace distribution, which allows us to set the peak of the probability mass at any point of $ mu$ Laplace(x;μ;γ)=12γexp(−∣x−μ∣γ)Laplace(x; mu; gamma)= frac{1}{2 gamma}exp left(- frac{|x- mu|}{ gamma} right)Laplace(x;μ;γ)=2γ1​exp(−γ∣x−μ∣​) . 1.13.6 . Dirac distribution and empirical distribution . The Dirac distribution ensures that all the masses in the probability distribution are concentrated at one point. The Diract-distributed Dirac $ delta $ function (also known as the unit pulse function) is defined as follows: p(x)=δ(x−μ),x≠μp(x)= delta(x- mu), x neq mup(x)=δ(x−μ),x​=μ . ∫abδ(x−μ)dx=1,a&lt;μ&lt;b int_{a}^{b} delta(x- mu)dx = 1, a &lt; mu &lt; b∫ab​δ(x−μ)dx=1,a&lt;μ&lt;b . Dirac distribution often appears as an integral part of the empirical distribution p^(x⃗)=1m∑i=1mδ(x⃗−x⃗(i)) hat{p}( vec{x})= frac{1}{m} sum_{i=1}^{m} delta( vec{x}-{ vec{x}}^{ (i)})p^​(x)=m1​∑i=1m​δ(x−x(i)) , where m points $x^{1},…,x^{m}$ is the given data set, experience distribution will have probability density $ frac{1}{m} $ Assigned to these points. . When we train the model on the training set, we can assume that the empirical distribution obtained from this training set indicates the source of the sample**. . ** Scope of application**: The Dirac δ function is suitable for the empirical distribution of **continuous ** random variables. . 1.14 Example Understanding Conditional Probability . The conditional probability formula is as follows: . P(A/B)=P(A∩B)/P(B)P(A/B) = P(A cap B) / P(B)P(A/B)=P(A∩B)/P(B) . Description: The event or subset $A$ and $B$ in the same sample space $ Omega$, if an element randomly selected from $ Omega$ belongs to $B$, then the next randomly selected element The probability of belonging to $A$ is defined as the conditional probability of $A$ on the premise of $B$. . According to the Venn diagram, it can be clearly seen that in the event of event B, the probability of event A occurring is $P(A bigcap B)$ divided by $P(B)$. Example: A couple has two children. What is the probability that one of them is a girl and the other is a girl? (I have encountered interviews and written tests) Exhaustive law: Knowing that one of them is a girl, then the sample space is for men, women, women, and men, and the probability that another is still a girl is 1/3. Conditional probability method: $P(female|female)=P(female)/P(female)$, couple has two children, then its sample space is female, male, female, male, male Male, $P (female) $ is 1/4, $P (female) = 1-P (male male) = 3/4$, so the last $1/3$. Everyone here may misunderstand that men, women and women are in the same situation, but in fact they are different situations like brothers and sisters. . 1.15 What is the difference between joint probability and edge probability? . The difference: Joint Probability: Joint Probability refers to a probability that, like $P(X=a, Y=b)$, contains multiple conditions, and all conditions are true at the same time. Joint probability refers to the probability that multiple random variables satisfy their respective conditions in a multivariate probability distribution. Edge Probability: An edge probability is the probability that an event will occur, regardless of other events. The edge probability refers to a probability similar to $P(X=a)$, $P(Y=b)$, which is only related to a single random variable. . **Contact: ** The joint distribution can find the edge distribution, but if only the edge distribution is known, the joint distribution cannot be obtained. . 1.16 The chain rule of conditional probability . From the definition of conditional probability, the following multiplication formula can be directly derived: Multiplication formula Let $A, B$ be two events, and $P(A) &gt; 0$, then . P(AB)=P(B∣A)P(A)P(AB) = P(B|A)P(A)P(AB)=P(B∣A)P(A) . Promotion . P(ABC)=P(C∣AB)P(B∣A)P(A)P(ABC)=P(C|AB)P(B|A)P(A)P(ABC)=P(C∣AB)P(B∣A)P(A) . In general, the induction method can be used to prove that if $P(A_1A_2…A_n)&gt;0$, then there is . P(A1A2...An)=P(An∣A1A2...An−1)P(An−1∣A1A2...An−2)...P(A2∣A1)P(A1)=P(A1)∏i=2nP(Ai∣A1A2...Ai−1)P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})P(A_{n-1}|A_1A_2...A_{n-2})...P(A_2 |A_1)P(A_1) =P(A_1) prod_{i=2}^{n}P(A_i|A_1A_2...A_{i-1})P(A1​A2​...An​)=P(An​∣A1​A2​...An−1​)P(An−1​∣A1​A2​...An−2​)...P(A2​∣A1​)P(A1​)=P(A1​)i=2∏n​P(Ai​∣A1​A2​...Ai−1​) . Any multi-dimensional random variable joint probability distribution can be decomposed into a conditional probability multiplication form with only one variable. . 1.17 Independence and conditional independence . Independence The two random variables $x$ and $y$, the probability distribution is expressed as a product of two factors, one factor containing only $x$ and the other factor containing only $y$, and the two random variables are independent. Conditions sometimes bring independence between events that are not independent, and sometimes they lose their independence because of the existence of this condition. Example: $P(XY)=P(X)P(Y)$, event $X$ is independent of event $Y$. Given $Z$ at this time, . P(X,Y∣Z)≠P(X∣Z)P(Y∣Z)P(X,Y|Z) not = P(X|Z)P(Y|Z)P(X,Y∣Z)​=P(X∣Z)P(Y∣Z) . When the event is independent, the joint probability is equal to the product of the probability. This is a very good mathematical nature, but unfortunately, unconditional independence is very rare, because in most cases, events interact with each other. . Conditional independence Given $Z$, $X$ and $Y$ are conditional, if and only if . X⊥Y∣Z  ⟺  P(X,Y∣Z)=P(X∣Z)P(Y∣Z)X bot Y|Z iff P(X,Y|Z) = P(X|Z)P(Y|Z)X⊥Y∣Z⟺P(X,Y∣Z)=P(X∣Z)P(Y∣Z) . The relationship between $X$ and $Y$ depends on $Z$, not directly. . Example defines the following events: $X$: It will rain tomorrow; $Y$: Today’s ground is wet; $Z$: Is it raining today? The establishment of the &gt;$Z$ event has an impact on both $X$ and $Y$. However, given the establishment of the $Z$ event, today’s ground conditions have no effect on whether it will rain tomorrow. . 1.18 Summary of Expectation, Variance, Covariance, Correlation Coefficient . Expectation In probability theory and statistics, the mathematical expectation (or mean, also referred to as expectation) is the sum of the probability of each possible outcome in the trial multiplied by the result. It reflects the average value of random variables. . Linear operation: $E(ax+by+c) = aE(x)+bE(y)+c$ | Promotion form: $E( sum_{k=1}^{n}{a_ix_i+c}) = sum_{k=1}^{n}{a_iE(x_i)+c}$ | Function expectation: Let $f(x)$ be a function of $x$, then the expectation of $f(x)$ is Discrete function: $E(f(x))= sum_{k=1}^{n}{f(x_k)P(x_k)}$ | Continuous function: $E(f(x))= int_{- infty}^{+ infty}{f(x)p(x)dx}$ | . | . Note: . The expectation of the function is not equal to the expected function, ie $E(f(x))=f(E(x))$ | In general, the expectation of the product is not equal to the expected product. | If $X$ and $Y$ are independent of each other, $E(xy)=E(x)E(y) $. | . Variance . The variance in probability theory is used to measure the degree of deviation between a random variable and its mathematical expectation (ie, mean). Variance is a special expectation. defined as: . Var(x)=E((x−E(x))2)Var(x) = E((x-E(x))^2)Var(x)=E((x−E(x))2) . Variance nature: . 1)$Var(x) = E(x^2) -E(x)^2$ 2) The variance of the constant is 0; 3) The variance does not satisfy the linear nature; 4) If $X$ and $Y$ are independent of each other, $Var(ax+by)=a^2Var(x)+b^2Var(y)$ . Covariance Covariance is a measure of the linear correlation strength and variable scale of two variables. The covariance of two random variables is defined as: . Cov(x,y)=E((x−E(x))(y−E(y)))Cov(x,y)=E((x-E(x))(y-E(y)))Cov(x,y)=E((x−E(x))(y−E(y))) . Variance is a special covariance. When $X=Y$, $Cov(x,y)=Var(x)=Var(y)$. . Covariance nature: . 1) The covariance of the independent variable is 0. 2) Covariance calculation formula: . Cov(∑i=1maixi,∑j=1mbjyj)=∑i=1m∑j=1maibjCov(xiyi)Cov( sum_{i=1}^{m}{a_ix_i}, sum_{j=1}^{m}{b_jy_j}) = sum_{i=1}^{m} sum_{j=1 }^{m}{a_ib_jCov(x_iy_i)}Cov(i=1∑m​ai​xi​,j=1∑m​bj​yj​)=i=1∑m​j=1∑m​ai​bj​Cov(xi​yi​) . 3) Special circumstances: . Cov(a+bx,c+dy)=bdCov(x,y)Cov(a+bx, c+dy) = bdCov(x, y)Cov(a+bx,c+dy)=bdCov(x,y) . Correlation coefficient The correlation coefficient is the amount by which the linear correlation between the variables is studied. The correlation coefficient of two random variables is defined as: . Corr(x,y)=Cov(x,y)Var(x)Var(y)Corr(x,y) = frac{Cov(x,y)}{ sqrt{Var(x)Var(y)}}Corr(x,y)=Var(x)Var(y) . ​Cov(x,y)​ . The nature of the correlation coefficient: 1) Bordered. The range of correlation coefficients is , which can be regarded as a dimensionless covariance. 2) The closer the value is to 1, the stronger the positive correlation (linearity) of the two variables. The closer to -1, the stronger the negative correlation, and when 0, the two variables have no correlation. .",
            "url": "https://jl1829.github.io/turbo-funicular/math/2020/01/23/Machine-Learning-Mathematical-Foundation.html",
            "relUrl": "/math/2020/01/23/Machine-Learning-Mathematical-Foundation.html",
            "date": " • Jan 23, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras",
            "content": "How to develop a Stacking Ensemble for Deep Learning Neural Networks in Python with Keras . Model averaging is an ensemble technique where multiple sub-models contribute equally to a combined prediction . Model averaging can be improved by weighting the contributions of each sub-model to the combined prediction by the expected performance of the submodel. This can be extended further by training an entirely new model to learn how to best combine the contributions from each submodel. This approach is called stacked generalization, or stacking for short, and can result in better predictive performance than any single contributing model. . In this tutorial, you will discover how to develop a stacked generalization ensemble for deep learning neural networks. . After completing this tutorial, you will know: . Stacked generalization is an ensemble method where a new model learns how to best combine the predictions from multiple existing models. . | How to develop a stacking model using neural networks as a submodel and a scikit-learn classifier as the meta-learner. . | How to develop a stacking model where neural network sub-models are embedded in a larger stacking ensemble model for training and prediction. . | . Tutorial Overview . This tutorial is divided into six parts, they are: . Stacked Generalization ensemble | Multi-Class Classification Problem | Multilayer Perceptron Model | Train and Save Sub-model | Separate Stacking Model | Integrated Stacking Model | Stacked Generalization Ensemble . A model averaging ensemble combines the predictions from multiple trained models. . A limitation of this approach is that each model contributes the same amount to the ensemble prediction, regardless of how well the model performed. A variation of this approach, called a weighted average ensemble, weighs the contribution of each ensemble member by the trust or expected performance of the model on a holdout dataset. This allows well-performing models to contribute more and less-well-performing models to contribute less. The weighted average ensemble provides an improvement over the model average ensemble. . A further generalization of this approach is replacing the linear weighted sum (e.g. linear regression) model used to combine the predictions of the sub-models with any learning algorithm. This approach is called stacked generalization, or stacking for short. . In stacking, an algorithm takes the outputs of sub-models as input and attempts to learn how to best combine the input predictions to make a better output prediction. . It may be helpful to think of the stacking procedure as having two levels: level 0 and level 1 . Level 0: The level 0 data is the training dataset inputs and level 0 models learn to make predictions from this data | Level 1: The level 1 data takes the output of the level 0 model s as input and the single level 1 model, or meta-learner, learns to make predictions from this data | . Unlike a weighted average ensemble, a stacked generalization ensemble can use the set of predictions as a context and conditionally decide to weigh the input predictions differently, potentially resulting in better performance. . Interestingly, although stacking is described as an ensemble learning method with two or more level 0 models, it can be used in the case where there is only a single level 0 model. In this case, the level 1, or meta-learner, model learns to correct the predictions from the level 0 model. . It is important that the meta-learner is trained on a separate dataset to the examples used to train the level 0 models to avoid overfitting. . A simple way that this can be achieved is by splitting the training dataset into a train and validation set. The level 0 models are then trained on the train set. The level 1 model is then trained using the validation set, where the raw inputs are first fed through the level 0 models to get predictions that are used as inputs to the level 1 model. . A limitation of the hold-out validation set approach to training a stacking model is that level 0 and level 1 models are not trained on the full dataset. . A more sophisticated approach to training a stacked model involves using k-fold cross-validation to develop the training dataset for the meta-learner model. Each level 0 model is trained using k-fold cross-validation (or even leave-one-out cross-validation for maximum effect); the models are then discarded, but the predictions are retained. This means for each model, there are predictions made by a version of the model that was not trained on those examples, e.g. like having holdout examples, but in this case for the entire training dataset. . The predictions are then used as inputs to train the meta-learner. Level 0 models are then trained on the entire training dataset and together with the meta-learner, the stacked model can be used to make predictions on new data. . In practice, it is common to use different algorithms to prepare each of the level 0 models, to provide a diverse set of predictions. . It is also common to use a simple linear model to combine the predictions. Because use of a linear model is common, stacking is more recently referred to as “model blending” or simply “blending,” especially in machine learning competitions. . A stacked generalization ensemble can be developed for regression and classification problems. In the case of classification problems, better results have been seen when using the prediction of class probabilities as input to the meta-learner instead of class labels. . Now that we are familiar with stacked generalization, we can work through a case study of developing a stacked deep learning model. . Multi-Class Classfication Problem . We will use a small multi-class classification problem as the basis to demonstrate the stacking ensemble. . The Scikit-learn class provides the make_blobs() function . The problem has two input variables (to represent the x and y coordinates of the points) and a standard deviation of 2.0 for points within each group. We will use the same random state (seed for the pseudorandom number generator to ensue that we always get the same data points . from sklearn.datasets import make_blobs from matplotlib import pyplot as plt import pandas as pd # generate 2d classification dataset X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2) # scatter plot, dots colored by class value df = pd.DataFrame(dict(x=X[:, 0], y=X[:, 1], label=y)) . The results are the input and output elements of a dataset that we can model. . In order to get a feeling for the complexity of the problem, we can graph each point on a two-dimensional scatter plot and color each point by class value. . colors = {0:&#39;red&#39;, 1:&#39;blue&#39;, 2:&#39;green&#39;} fig, ax = plt.subplots() grouded = df.groupby(&#39;label&#39;) for key, group in grouded: group.plot(ax=ax, kind=&#39;scatter&#39;, x=&#39;x&#39;, y=&#39;y&#39;, label=key, color=colors[key]) plt.show() . . Running the example creates a scatter plot of the entire dataset. We can see that the standard deviation of 2.0 means that the classes are not linearly separable (separable by a line) causing many ambiguous points. . This is desirable as it means that the problem is non-trivial and will allow a neural network model to find many different “good enough” candidate solutions, resulting in a high variance. . Multilayer Perceptron Model . Before we define a model, we need to contrive a problem that is appropriate for the stacking ensemble. . In our problem, the training dataset is relatively small. Specifically, there is a 10:1 ratio of examples in the training dataset to the holdout dataset. This mimics a situation where we may have a vast number of unlabeled examples and a small number of labeled examples with which to train a model. . We will create 1,100 data points from the blobs problem. The model will be trained on the first 100 points and the remaining 1,000 will be held back in a test dataset, unavailable to the model. . The problem is a multi-class classification problem, and we will model it using a softmax activation function on the output layer. This means that the model will predict a vector with three elements with the probability that the sample belongs to each of the three classes. Therefore, we must one hot encode the class values before we split the rows into the train and test datasets. We can do this using the Keras to_categorical() function. . # use PlaidML as backend intead of default TensorFlow, # so that can utilize the power of MacBook Pro&#39;s AMD GPU import os os.environ[&quot;KERAS_BACKEND&quot;] = &quot;plaidml.keras.backend&quot; . # import the modules from keras.utils import to_categorical from keras.models import Sequential from keras.layers import Dense . Using plaidml.keras.backend backend. . # generate 2d clasification dataset X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2) # apply one-hot encoding y = to_categorical(y) # split train test set n_train = 100 X_train, X_test = X[:n_train, :], X[n_train:, :] y_train, y_test = y[:n_train], y[n_train:] print(f&quot;The shape of X train set is {X_train.shape}, the shape of X test set is {X_test.shape}.&quot;) . The shape of X train set is (100, 2), the shape of X test set is (1000, 2). . Next, we can define and combine the model. . The model will expect samples with two input variables. The model then has a single hidden layer with 25 nodes and a rectified linear activation function, then an output layer with three nodes to predict the probability of each of the three classes and a softmax activation function. . Because the problem is multi-class, we will use the categorical cross entropy loss function to optimize the model and the efficient Adam flavor of stochastic gradient desent . # define model model = Sequential() model.add(Dense(25, input_dim=2, activation=&#39;relu&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) . INFO:plaidml:Opening device &quot;metal_amd_radeon_pro_560x.0&quot; . The model is fit for 500 training epochs and we will evaluate the model each epoch on the test set, using the test set as validation set . # fit the model history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=150, verbose=1) . Epoch 150/150 100/100 [==============================] - 0s 1ms/step - loss: 0.4755 - acc: 0.7900 - val_loss: 0.5328 - val_acc: 0.7800 . At the end of the run, we will evaluate the performance of the model on the train and test sets. . # evaluate the model _, train_acc = model.evaluate(X_train, y_train, verbose=1) . 100/100 [==============================] - 0s 450us/step . # evaluate the model _, test_acc = model.evaluate(X_test, y_test, verbose=1) . 1000/1000 [==============================] - 0s 112us/step . print(f&quot;The train accuracy is {train_acc}, and the test accuracy is {test_acc}.&quot;) . The train accuracy is 0.79, and the test accuracy is 0.78. . Then finally, we will plot learning curves of the model accuracy over each training epoch on both the training and validation datasets. . # learning curves of model accuracy plt.plot(history.history[&#39;acc&#39;], label=&#39;train&#39;) plt.plot(history.history[&#39;val_acc&#39;], label=&#39;test&#39;) plt.legend() plt.show() . . Running the example first prints the shape of each dataset for confirmation, then the performance of the final model on the train and test datasets. . Your specific results will vary (by design!) given the high variance nature of the model. . In this case, we can see that the model achieved about 78% accuracy on the training dataset, which we know is optimistic, and about 73.9% on the test dataset, which we would expect to be more realistic. . We can now look at using instances of this model as part of a stacking ensemble. . Train and Save Sub-Models . To keep this example simple, we will use multiple instances of the same model as level-0 or sub-models in the stacking ensemble. . We will also use a holdout validation dataset to train the level-1 or meta-learner in the ensemble. . A more advanced example may use different types of MLP models (deeper, wider, etc.) as sub-models and train the meta-learner using k-fold cross-validation . In this section, we will train multiple sub-models and save them to file for later use in our stacking ensembles. . The first step is to create a function that will define and fit an MLP model on the training dataset. . # fit model on dataset def fit_model(X_train, y_train): # define the model model = Sequential() model.add(Dense(25, input_dim=2, activation=&#39;relu&#39;)) model.add(Dense(3, activation=&#39;softmax&#39;)) model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) # fit the model model.fit(X_train, y_train, epochs=150, verbose=0) return model . Next, we can create a sub-directory to store the models. . Note, if the directory already exists, you may have to delete it when re-running this code. . from os import makedirs makedirs(&#39;models&#39;) . Finally, we can create multiple instances of the MLP and save each to the “models/” subdirectory with a unique filename. . In this case, we will create five sub-models, but you can experiment with a different number of models and see how it impacts model performance. . # fit and save models: n_members = 5 for i in range(n_members): # fit model model = fit_model(X_train, y_train) # save model filename = &#39;models/model_&#39; + str(i + 1) + &#39;.h5&#39; model.save(filename) print(f&quot;[INFO]&gt;&gt;Save {filename}.&quot;) . INFO:plaidml:Opening device &quot;metal_amd_radeon_pro_560x.0&quot; [INFO]&gt;&gt;Save models/model_1.h5. [INFO]&gt;&gt;Save models/model_2.h5. [INFO]&gt;&gt;Save models/model_3.h5. [INFO]&gt;&gt;Save models/model_4.h5. [INFO]&gt;&gt;Save models/model_5.h5. . !ls -l models . total 280 -rw-r--r-- 1 johnnylu staff 27936 Jan 16 12:40 model_1.h5 -rw-r--r-- 1 johnnylu staff 27936 Jan 16 12:40 model_2.h5 -rw-r--r-- 1 johnnylu staff 27936 Jan 16 12:40 model_3.h5 -rw-r--r-- 1 johnnylu staff 27936 Jan 16 12:41 model_4.h5 -rw-r--r-- 1 johnnylu staff 27936 Jan 16 12:41 model_5.h5 . Separate Stacking Model . We can now train a meta-learner that will best combine the predictions from the sub-models and ideally perform better than any single sub-model. . The first step is to load the saved models. . We can use the load_model() Keras function and create a Python list of loaded models. . n_members = 5 from keras.models import load_model # load models from file def load_all_model(n_models): all_models = [] for i in range(n_models): # define filename for this ensemble filename = &#39;models/model_&#39; + str(i + 1) + &#39;.h5&#39; model = load_model(filename) # add to list of members all_models.append(model) print(f&quot;[INFO]&gt;&gt;loaded {filename}.&quot;) return all_models . # load all models: members = load_all_model(n_members) print(f&quot;Loaded {len(members)} models.&quot;) . [INFO]&gt;&gt;loaded models/model_1.h5. [INFO]&gt;&gt;loaded models/model_2.h5. [INFO]&gt;&gt;loaded models/model_3.h5. [INFO]&gt;&gt;loaded models/model_4.h5. [INFO]&gt;&gt;loaded models/model_5.h5. Loaded 5 models. . It would be useful to know how well the single models perform on the test dataset as we would expect a stacking model to perform better. . We can easily evaluate each single model on the training dataset and establish a baseline of performance. . # evaluate standalone models on test dataset for model in members: _, acc = model.evaluate(X_test, y_test, verbose=1) print(f&quot;Model Test Set Accuracy: {acc}.&quot;) . 1000/1000 [==============================] - 0s 402us/step Model Test Set Accuracy: 0.78. 1000/1000 [==============================] - 0s 98us/step Model Test Set Accuracy: 0.724. 1000/1000 [==============================] - 0s 100us/step Model Test Set Accuracy: 0.755. 1000/1000 [==============================] - 0s 103us/step Model Test Set Accuracy: 0.778. 1000/1000 [==============================] - 0s 101us/step Model Test Set Accuracy: 0.752. . Next, we can train our meta-learner. This requires two steps: . Prepare a training dataset for the meta-learner. | Use the prepared training set to fit a meta-learner model | . We will prepare a training dataset for the meta-learner by providing examples from the test set to each of the submodels and collecting the predictions. In this case, each model will output three predictions for each example for the probabilities that a given example belongs to each of the three classes. Therefore, the 1,000 examples in the test set will result in five arrays with the shape [1000, 3]. . We can combine these arrays into a three-dimensional array with the shape [1000, 5, 3] by using the dstack() numpy functionthat will stack each new set of predictions . As input for new model, we will require 1,000 examples with some number of features. Given that we have 5 models and each model makes three predictions per example, then we would have 15 (3 x 5) features for each example provided to the submodels. We can transform the [1000, 5, 3] shaped predictions from the sub-models into a [1000, 15] shaped array to be used to train a meta-learner using the reshape() numpy function and flattening the final two dimensions. The stacked_dataset() function implements this steps . # check the sub-model prediction output shape test = model.predict(X_test) print(test) print(&quot; n&quot;) print(test.shape) print(&quot; n&quot;) print(y_test) . [[0.87745166 0.00568948 0.11685885] [0.00420502 0.08186033 0.91393465] [0.0560521 0.27464437 0.66930354] ... [0.22192474 0.524442 0.25363323] [0.8633721 0.05750959 0.0791183 ] [0.66364163 0.11609959 0.2202588 ]] (1000, 3) [[1. 0. 0.] [0. 0. 1.] [0. 0. 1.] ... [0. 1. 0.] [1. 0. 0.] [0. 0. 1.]] . # numpy dstack example import numpy as np from numpy import dstack a = np.array((1, 2, 3)) print(f&quot;The array a is {a}&quot;) print(&quot; n&quot;) b = np.array((4, 5, 6)) print(f&quot;The array b is {b}&quot;) print(&quot; n&quot;) c = dstack((a, b)) print(f&quot;dstack: {c}&quot;) print(f&quot;Shape of dstack: {c.shape}&quot;) . The array a is [1 2 3] The array b is [4 5 6] dstack: [[[1 4] [2 5] [3 6]]] Shape of dstack: (1, 3, 2) . # create stacked model input dataset as output from the ensemble def stacked_dataset(members, inputX): stackX = None for model in members: # make prediction yhat = model.predict(inputX, verbose=0) # stack predictions into [rows, members, probabilities] if stackX is None: stackX = yhat else: stackX = dstack((stackX, yhat)) # flatten predictions to [rows, members x probabilities] stackX = stackX.reshape((stackX.shape[0], stackX.shape[1]*stackX.shape[2])) return stackX . stackX = stacked_dataset(members, X_test) . stackX.shape . (1000, 15) . Once prepared, we can use this input dataset along with the output, or y part, of the test set to train a new meta-learner . In this case, we will train a simple logistic regression algorithm from the scikit-learn library . Logistic Regression only supports binary classification, although the implementation of logistic regression in scikit-learn in the LogisticRegression class support multi-class classification (more than two classes) using a one-vs-rest scheme. The function fit_stacked_model() below will prepare the training dataset for meta-learner by calling the stacked_dataset() function, then fit a logistic regression model that is then returned. . # import Logistic Regression class from sklearn.linear_model import LogisticRegression # fit a model based on the outputs from the ensemble members def fit_stacked_model(members, inputX, inputy): # create dataset using ensemble stackedX = stacked_dataset(members, inputX) # fit standalone mode model = LogisticRegression() model.fit(stackedX, inputy) return model . # make a prediction with the stacked model def stacked_prediction(members, model, inputX): # create dataset using ensemble stackedX = stacked_dataset(members, inputX) # make a prediction yhat = model.predict(stackedX) return yhat . We can call this function and pass in the list of loaded models and the training dataset . from sklearn.datasets import make_blobs # reset the X, y and X_test, y_test variable X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2) # split into train and test n_train = 100 X_train, X_test = X[:n_train, :], X[n_train:, :] y_train, y_test = y[:n_train], y[n_train:] print(X_train.shape, X_test.shape) . (100, 2) (1000, 2) . from sklearn.metrics import accuracy_score # load all models n_members = 5 members = load_all_model(n_members) print(&#39;Loaded %d models&#39; % len(members)) # evaluate standalone models on test dataset for model in members: testy_enc = to_categorical(y_test) _, acc = model.evaluate(X_test, testy_enc, verbose=0) print(&#39;Model Accuracy: %.3f&#39; % acc) # fit stacked model using the ensemble model = fit_stacked_model(members, X_test, y_test) # evaluate model on test set yhat = stacked_prediction(members, model, X_test) acc = accuracy_score(y_test, yhat) print(&#39;Stacked Test Accuracy: %.3f&#39; % acc) . [INFO]&gt;&gt;loaded models/model_1.h5. [INFO]&gt;&gt;loaded models/model_2.h5. [INFO]&gt;&gt;loaded models/model_3.h5. [INFO]&gt;&gt;loaded models/model_4.h5. [INFO]&gt;&gt;loaded models/model_5.h5. Loaded 5 models Model Accuracy: 0.780 Model Accuracy: 0.724 Model Accuracy: 0.755 Model Accuracy: 0.778 Model Accuracy: 0.752 Stacked Test Accuracy: 0.826 . Integrated Stacking Model . When using neural networks as sub-models, it may be desirable to use a neural network as a meta-learner. . Specifically, the sub-networks can be embedded in a larger multi-headed neural network that then learns how to best combine the predictions from each input sub-model. It allows the stacking ensemble to be treated as a single large model. . The benefit of this approach is that the outputs of the submodels are provided directly to the meta-learner. Further, it is also possible to update the weights of the submodels in conjunction with the meta-learner model, if this is desirable. . This can be achieved using the Keras functional interface for developing models. . After the models are loaded as a list, a larger stacking ensemble model can be defined where each of the loaded models is used as a separate input-head to the model. This requires that all of the layers in each of the loaded models be marked as not trainable so the weights cannot be updated when the new larger model is being trained. Keras also requires that each layer has a unique name, therefore the names of each layer in each of the loaded models will have to be updated to indicate to which ensemble member they belong. . # import modules from sklearn.datasets import make_blobs from sklearn.metrics import accuracy_score from keras.models import load_model from keras.utils import to_categorical from keras.utils import plot_model from keras.models import Model from keras.layers import Input from keras.layers import Dense from keras.layers.merge import concatenate from numpy import argmax . Once the sub-models have been prepared, we can define the stacking ensemble model. . The input layer for each of the sub-models will be used as separete input head to this new model. This means that k copies of any input data will have to be provided to the model, where k is the number of input models, in this case, k = 5. . The outputs of each of the models can then be merged. In this case, we will use a simple concatenation merge, where a single 15-elememts vector will be created from the 3 class-probabilities predicted by each of the 5 models. . We will then define a hidden layer to interpret this input to the meta-learner and an output layer that will make its own probabilistic prediction. The define_stacked_model() function below implements this and will return a stacked generalization neural network model given a list of trained sub-models. . # define stacked model from multiple member input models def define_stacked_model(members): # update all layers in all models to not be trainable for i in range(len(members)): model = members[i] for layer in model.layers: # make not trainable layer.trainable = False # rename to avoid &#39;unique layer name&#39; issue layer.name = &#39;ensemble_&#39; + str(i + 1) + &#39;_&#39; + layer.name # define multi-headed input ensemble_visible = [model.input for model in members] # concatenate merge output from each model ensemble_outputs = [model.output for model in members] merge = concatenate(ensemble_outputs) hidden = Dense(10, activation=&#39;relu&#39;)(merge) output = Dense(3, activation=&#39;softmax&#39;)(hidden) model = Model(inputs=ensemble_visible, outputs=output) # plot graph of ensemble plot_model(model, show_shapes=True, to_file=&#39;model_graph.png&#39;) # complie model.compile(loss=&#39;categorical_crossentropy&#39;, optimizer=&#39;adam&#39;, metrics=[&#39;accuracy&#39;]) return model . stacked_model = define_stacked_model(members) . A plot of the network graph is created when this function is called to give an idea of how the ensemble model fits together. . Once the model is defined, it can be fit. We can fit it directly on the holdout test dataset. . Because the sub-models are not trainable, their weights wil not be updated during training and only the weights of the new hidden and output layer will be updated. The fit_stacked_model() function below will fit the stacking neural network model on for 300 epochs. . # fit a stacked model def fit_stacked_model(model, inputX, inputy): # prepare input data X = [inputX for _ in range(len(model.input))] # encode y variable inputy_enc = to_categorical(inputy) # fit the mode model.fit(X, inputy_enc, epochs=300, verbose=0) . Once fit, we can use the new stacked model to make a prediction on new data. . This is as simple as calling the predict() function on the model. One minor change is that we require k copies of the input data in a list to be provided to the model for each of the k sub-models. the predict_stacked_model() function below simplifies this process of making a prediction with the stacking model. . # make a prediction with a stacked model def predict_stacked_model(model, inputX): # prepare input data X = [inputX for _ in range(len(model.input))] # make prediction return model.predict(X, verbose=1) . We can call this function to make a prediction for the test dataset and report the accuracy . We would expect the performance of the neural network learner to be better than any individual submodel and perhaps competitive with the linear meta-learner used in the previous section. . # generate 2d classification dataset from sklearn.model_selection import train_test_split X, y = make_blobs(n_samples=1100, centers=3, n_features=2, cluster_std=2, random_state=2) # split into train and test X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=3) # load all models n_members = 5 members = load_all_model(n_members) print(&#39;Loaded %d models&#39; % len(members)) # define ensemble model stacked_model = define_stacked_model(members) # fit stacked model on test dataset fit_stacked_model(stacked_model, X_test, y_test) # make predictions and evaluate yhat = predict_stacked_model(stacked_model, X_test) yhat = argmax(yhat, axis=1) acc = accuracy_score(y_test, yhat) print(&#39;Stacked Test Accuracy: %.3f&#39; % acc) . [INFO]&gt;&gt;loaded models/model_1.h5. [INFO]&gt;&gt;loaded models/model_2.h5. [INFO]&gt;&gt;loaded models/model_3.h5. [INFO]&gt;&gt;loaded models/model_4.h5. [INFO]&gt;&gt;loaded models/model_5.h5. Loaded 5 models 550/550 [==============================] - 0s 448us/step Stacked Test Accuracy: 0.820 . Running the example first loads the five sub-models. . A larger stacking ensemble neural network is defined and fit on the test dataset, then the new model is used to make a prediction on the test dataset. We can see that in this case, the model achieved an higher accuracy. out-performing the linear model from the previous section. . Extensions . This section lists some ideas for extending the tutorial that you may wish to explor . Alternate Meta-Learner. Update the example to use an alternate meta-learner classifier model to the logistic regression model | Single Level 0 Models. Update the example to use a single level-0 model and compare the results. | ** Vary Level0 Models**. Develop a study that demostrates the relationship between test classification accuracy and the number of sub-models used in the stacked ensemble. | Cross-Validation Stacking Ensemble. Update the example to use k-fold cross-validation to prepare the training dataset for the meta-learner model. | Use Raw Input in Meta-Learner. Update the example so that the meta-learner algorithms take the raw input data for the sample as well as the output from the sub-models and compare performance. | . Further Reading . Books . Section 8.8 Model Averaging and Stacking, The Elements of Statistical Learning: Data Mining, Inference and Prediction, 2nd Edition, 2016 | Section 7.5 Combining multiple models, Data Mining: Practical Machine Learning Tools and Techniques, 2nd Editions, 2005 | Section 9.8.2 Stacked Generalization, Neural Networks for Pattern Recognition, 1995 | . Papers . Stacked Generalization, 1992 | Issues in Stacked Generalization, 1999 | . API . Getting started with the Keras Sequential model | Keras Core Layers API | numpy.argmax API | sklearn.datasets.make_blobs() API | numpy.dstack API | sklearn.linear_model.LogisticRegression API | .",
            "url": "https://jl1829.github.io/turbo-funicular/ensemble/2020/01/18/Ensemble_Keras.html",
            "relUrl": "/ensemble/2020/01/18/Ensemble_Keras.html",
            "date": " • Jan 18, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jl1829.github.io/turbo-funicular/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi and welcome to my blog, I am Johnny.Lu, staying in the “Fine” city called Singapore(joke), currently I am the Lead Consulting Engineer(Data Science, Machine Learning)in Allied Telesis, a Japanese pioneer company in digital communication. . Experiences . Allied Telesis . Lead Consulting Engineerr, (Machine Learning, Data Science). Singapore, Jun/2017 – Current . Leading Data Science and System Engineer team to develop products that measurably and efficiently improve sales and top-line growth. | Interfacing with customers to receive valuable product feedback. | Driving strategy and vision for products by translating research, customer insights, and data discovery into innovative solutions for customers. | Actively collaborating with global IT, Architectures, Infrastructure, and Sales teams to deploy products. | Develop End-to-End Data Science, Machine Learning Project using: MySQL, Scikit-Learn, NumPy, Pandas, PySpark, TensorFlow 2, Keras | LightGBM, XGBoost, SpaCy, NLTK | . | . | Regional System Engineer Singapore, May/2016 – Jun/2017 . Response to business development, including leads generating, roadshow/seminar conducting, solution designing, quoting/bidding, solution deploying, and after sales servicing. | Promote Allied Telesis SDN, Service Cloud solution; initiative in the region, managing the technical team to provide high reliability services. | Managing key account, provide advisory of the option for new technology adoption. | . | . NETGEAR Inc. . Regional System Engineer Singapore, Jun/2013 – May/2016 Act as a constant performer, teaming with Regional Sales Director, perform annually business growth. | Provide consultation service to downstream partner &amp; end user, including basic infra network design and deployment, integration with virtualization, customized solution for individual customers. | . | Regional System Engineer Guangzhou, China, Nov/2012 – Oct/2013 Design, develop and carry out technical solutions | Establish and develop technical marketing objectives and goals | Analyze and interpret marketing trends concerning technical products or services | . | . Professional Certification . Machine Learning from Stanford University Online. Oct/2018 | Mathematics for Machine Learning: Linear Algebra, from Imperial College London. Feb/2019 | Mathematics for Machine Learning: Multivariate Calculus, from from Imperial College London. Mar/2019 | Neural Network and Deep Learning, from deeplearning.ai. Mar/2019 | Improving Deep Neural Networks: Hyperparameter tuning, Regularization and Optimization, from deeplearning.ai. Apr/2019 | Convolutional Neural Networks, from deeplearning.ai. May/2019 | Sequance Models, from deeplearning.ai. Jul/2019 | . Education Background . National University of Singapore . Master of Science - MS, Technology &amp; Intelligent System . Machine Reasoning | Reasoning Systems | Cognitive Systems | Problem Solving using Pattern Recognition | Intelligent Sensing and Sense Making | Pattern Recognition and Machine Learning Systems | Text Analytics | New Media and Sentiment Mining | Text Processing using Machine Learning | Conversational UIs | Vision Systems | Spatial Reasoning from Sensor Data | Real Time Audio-Visual Sensing and Sense Making | . Royal Holloway, University of London . Bachelor of Science (B.S.), Marketing/Marketing Management, General . MN2041K Managerial Accounting | MN2061K Marketing Management | MN2155K Asia Pacific Businesses | MN2165K The Global Economy | MN22201K Strategic Management | MN3215K Asia Pacific Multinationals in Europe | MN3455K Advertising And Promotion in Brand Marketing | MN3495K Clusters, Small Business and International Competition | MN3555K E-Commerce | MN3035K Marketing Research | MN3055K Consumer Behaviour | MN3301K Modern Business in Comparative Perspective | . Guangzhou Civil Aviation College . Associate’s degree, Electrical and Electronics Engineering . Further Mathematics | College English | Circuit Analysis | Analog Electronic Technology | Digital Electronic Technology | Single-chip Microcomputer Design &amp; Develop | The C Programming Language | Computer Network | Digital Communication Theory | Stored Program Control &amp; Mobile Communication Theory | .",
          "url": "https://jl1829.github.io/turbo-funicular/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jl1829.github.io/turbo-funicular/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}